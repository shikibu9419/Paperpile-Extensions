@ARTICLE{noauthor_undated-tu,
  title = "{chi02.pdf}"
}

@MISC{noauthor_undated-ui,
  title    = "{{IPSJ-EIP06033016.pdf}}",
  keywords = "eye contact;telepresence"
}

@MISC{noauthor_undated-jo,
  title    = "{{IPSJ-JNL5801020.pdf}}",
  keywords = "eye contact;telepresence"
}

@ARTICLE{noauthor_undated-ot,
  title    = "{vol26\_4\_006jp.pdf}",
  keywords = "eye contact;telepresence"
}

@ARTICLE{noauthor_undated-vq,
  title = "{isomoto\_thesis.pdf}"
}

@ARTICLE{noauthor_undated-jw,
  title    = "{考えてみるに As We May Think}",
  language = "ja"
}

@MISC{noauthor_undated-gp,
  title = "{IPSJ-JNL5801020.pdf}"
}

@ARTICLE{noauthor_undated-gm,
  title    = "{自己像追加による視線理解の変化}",
  keywords = "telepresence"
}

@ARTICLE{noauthor_undated-sy,
  title = "{多人数ビデオ会議における 話者交替のための視線提示手法}"
}

@MISC{noauthor_undated-rw,
  title       = "{Build software better, together}",
  institution = "Github",
  abstract    = "GitHub is where people build software. More than 73 million
                 people use GitHub to discover, fork, and contribute to over 200
                 million projects.",
  language    = "en"
}

@ARTICLE{noauthor_undated-sb,
  title = "{多人数ビデオ会議における 話者交替のための視線提示手法}"
}

@ARTICLE{noauthor_undated-vy,
  title = "{vol26\_4\_006jp.pdf}"
}

@MISC{noauthor_undated-yk,
  title = "{IPSJ-EIP06033016.pdf}"
}

@MISC{noauthor_undated-te,
  title     = "{Build software better, together}",
  publisher = "Github",
  abstract  = "GitHub is where people build software. More than 73 million
               people use GitHub to discover, fork, and contribute to over 200
               million projects.",
  language  = "en"
}

@ARTICLE{Carbune2019-pw,
  title         = "{Fast Multi-language {LSTM-based} Online Handwriting
                   Recognition}",
  author        = "Carbune, Victor and Gonnet, Pedro and Deselaers, Thomas and
                   Rowley, Henry A and Daryin, Alexander and Calvo, Marcos and
                   Wang, Li-Lun and Keysers, Daniel and Feuz, Sandro and
                   Gervais, Philippe",
  journal       = "arXiv [cs.CL]",
  abstract      = "We describe an online handwriting system that is able to
                   support 102 languages using a deep neural network
                   architecture. This new system has completely replaced our
                   previous Segment-and-Decode-based system and reduced the
                   error rate by 20\%-40\% relative for most languages. Further,
                   we report new state-of-the-art results on IAM-OnDB for both
                   the open and closed dataset setting. The system combines
                   methods from sequence recognition with a new input encoding
                   using Bézier curves. This leads to up to 10x faster
                   recognition times compared to our previous system. Through a
                   series of experiments we determine the optimal configuration
                   of our models and report the results of our setup on a number
                   of additional public datasets.",
  month         =  feb,
  year          =  2019,
  url           = "http://arxiv.org/abs/1902.10525",
  file          = "All Papers/Other/Carbune et al. 2019 - Fast Multi-language LSTM-based Online Handwriting Recognition.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1902.10525"
}

@ARTICLE{Wu2019-es,
  title         = "{Imagine That! Leveraging Emergent Affordances for {3D} Tool
                   Synthesis}",
  author        = "Wu, Yizhe and Kasewa, Sudhanshu and Groth, Oliver and Salter,
                   Sasha and Sun, Li and Jones, Oiwi Parker and Posner, Ingmar",
  journal       = "arXiv [cs.LG]",
  abstract      = "In this paper we explore the richness of information captured
                   by the latent space of a vision-based generative model. The
                   model combines unsupervised generative learning with a
                   task-based performance predictor to learn and to exploit
                   task-relevant object affordances given visual observations
                   from a reaching task, involving a scenario and a stick-like
                   tool. While the learned embedding of the generative model
                   captures factors of variation in 3D tool geometry (e.g.
                   length, width, and shape), the performance predictor
                   identifies sub-manifolds of the embedding that correlate with
                   task success. Within a variety of scenarios, we demonstrate
                   that traversing the latent space via backpropagation from the
                   performance predictor allows us to imagine tools appropriate
                   for the task at hand. Our results indicate that
                   affordances-like the utility for reaching-are encoded along
                   smooth trajectories in latent space. Accessing these emergent
                   affordances by considering only high-level performance
                   criteria (such as task success) enables an agent to
                   manipulate tool geometries in a targeted and deliberate way.",
  month         =  sep,
  year          =  2019,
  url           = "http://arxiv.org/abs/1909.13561",
  file          = "All Papers/Other/Wu et al. 2019 - Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.13561",
  keywords      = "world model"
}

@ARTICLE{He2019-cy,
  title         = "{Exploring the Effectiveness of Face-to-face Mixed Reality
                   for Teaching with Chalktalk}",
  author        = "He, Zhenyi and Perlin, Ken",
  journal       = "arXiv [cs.HC]",
  abstract      = "Teaching that uses projected presentation media such as
                   slide-shows lacks support for dynamic content whose form and
                   behaviors require live changes during a lecture. Recent
                   software alternatives such as the Chalktalk software platform
                   allow the creation of interactive simulations in arbitrary
                   sequences and combinations within presentations. These more
                   dynamic solutions, however, do not optimize for face-to-face
                   interactions: eye-contact, gaze direction, and concurrent
                   awareness of another person's movements together with the
                   presented content. To explore the extent to which these
                   face-to-face interactions may improve learning and engagement
                   during a lecture, we propose a Mixed Reality (MR) platform
                   that places Chalktalk's behaviors and simulations within a
                   mirrored virtual world environment designed for face-to-face,
                   one-on-one interactions. We compare our system with projected
                   Chalktalk to evaluate its relative effectiveness for
                   learning, retention, and level of engagement.",
  month         =  dec,
  year          =  2019,
  url           = "http://arxiv.org/abs/1912.03863",
  file          = "All Papers/Other/He and Perlin 2019 - Exploring the Effectiveness of Face-to-face Mixed Reality for Teaching with Chalktalk.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "1912.03863"
}

@ARTICLE{Sharma2020-nk,
  title         = "{Digital Twins: State of the Art Theory and Practice,
                   Challenges, and Open Research Questions}",
  author        = "Sharma, Angira and Kosasih, Edward and Zhang, Jie and
                   Brintrup, Alexandra and Calinescu, Anisoara",
  journal       = "arXiv [cs.LG]",
  abstract      = "Digital Twin was introduced over a decade ago, as an
                   innovative all-encompassing tool, with perceived benefits
                   including real-time monitoring, simulation and forecasting.
                   However, the theoretical framework and practical
                   implementations of digital twins (DT) are still far from this
                   vision. Although successful implementations exist, sufficient
                   implementation details are not publicly available, therefore
                   it is difficult to assess their effectiveness, draw
                   comparisons and jointly advance the DT methodology. This work
                   explores the various DT features and current approaches, the
                   shortcomings and reasons behind the delay in the
                   implementation and adoption of digital twin. Advancements in
                   machine learning, internet of things and big data have
                   contributed hugely to the improvements in DT with regards to
                   its real-time monitoring and forecasting properties. Despite
                   this progress and individual company-based efforts, certain
                   research gaps exist in the field, which have caused delay in
                   the widespread adoption of this concept. We reviewed relevant
                   works and identified that the major reasons for this delay
                   are the lack of a universal reference framework, domain
                   dependence, security concerns of shared data, reliance of
                   digital twin on other technologies, and lack of quantitative
                   metrics. We define the necessary components of a digital twin
                   required for a universal reference framework, which also
                   validate its uniqueness as a concept compared to similar
                   concepts like simulation, autonomous systems, etc. This work
                   further assesses the digital twin applications in different
                   domains and the current state of machine learning and big
                   data in it. It thus answers and identifies novel research
                   questions, both of which will help to better understand and
                   advance the theory and practice of digital twins.",
  month         =  nov,
  year          =  2020,
  url           = "http://arxiv.org/abs/2011.02833",
  file          = "All Papers/Other/Sharma et al. 2020 - Digital Twins - State of the Art Theory and Practice, Challenges, and Open Research Questions.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2011.02833"
}

@ARTICLE{Pumarola2020-ie,
  title         = "{{D-NeRF}: Neural radiance fields for dynamic scenes}",
  author        = "Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and
                   Moreno-Noguer, Francesc",
  journal       = "arXiv [cs.CV]",
  abstract      = "Neural rendering techniques combining machine learning with
                   geometric reasoning have arisen as one of the most promising
                   approaches for synthesizing novel views of a scene from a
                   sparse set of images. Among these, stands out the Neural
                   radiance fields (NeRF), which trains a deep network to map 5D
                   input coordinates (representing spatial location and viewing
                   direction) into a volume density and view-dependent emitted
                   radiance. However, despite achieving an unprecedented level
                   of photorealism on the generated images, NeRF is only
                   applicable to static scenes, where the same spatial location
                   can be queried from different images. In this paper we
                   introduce D-NeRF, a method that extends neural radiance
                   fields to a dynamic domain, allowing to reconstruct and
                   render novel images of objects under rigid and non-rigid
                   motions from a \textbackslashemph\{single\} camera moving
                   around the scene. For this purpose we consider time as an
                   additional input to the system, and split the learning
                   process in two main stages: one that encodes the scene into a
                   canonical space and another that maps this canonical
                   representation into the deformed scene at a particular time.
                   Both mappings are simultaneously learned using
                   fully-connected networks. Once the networks are trained,
                   D-NeRF can render novel images, controlling both the camera
                   view and the time variable, and thus, the object movement. We
                   demonstrate the effectiveness of our approach on scenes with
                   objects under rigid, articulated and non-rigid motions. Code,
                   model weights and the dynamic scenes dataset will be
                   released.",
  month         =  nov,
  year          =  2020,
  url           = "http://arxiv.org/abs/2011.13961",
  file          = "All Papers/Other/Pumarola et al. 2020 - D-NeRF - Neural radiance fields for dynamic scenes.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2011.13961"
}

@ARTICLE{Wang2020-nj,
  title         = "{{One-Shot} {Free-View} Neural {Talking-Head} Synthesis for
                   Video Conferencing}",
  author        = "Wang, Ting-Chun and Mallya, Arun and Liu, Ming-Yu",
  journal       = "arXiv [cs.CV]",
  abstract      = "We propose a neural talking-head video synthesis model and
                   demonstrate its application to video conferencing. Our model
                   learns to synthesize a talking-head video using a source
                   image containing the target person's appearance and a driving
                   video that dictates the motion in the output. Our motion is
                   encoded based on a novel keypoint representation, where the
                   identity-specific and motion-related information is
                   decomposed unsupervisedly. Extensive experimental validation
                   shows that our model outperforms competing methods on
                   benchmark datasets. Moreover, our compact keypoint
                   representation enables a video conferencing system that
                   achieves the same visual quality as the commercial H.264
                   standard while only using one-tenth of the bandwidth.
                   Besides, we show our keypoint representation allows the user
                   to rotate the head during synthesis, which is useful for
                   simulating face-to-face video conferencing experiences.",
  month         =  nov,
  year          =  2020,
  url           = "http://arxiv.org/abs/2011.15126",
  file          = "All Papers/Other/Wang et al. 2020 - One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2011.15126",
  keywords      = "eye contact;telepresence"
}

@ARTICLE{Wang2020-ow,
  title         = "{{One-Shot} {Free-View} Neural {Talking-Head} Synthesis for
                   Video Conferencing}",
  author        = "Wang, Ting-Chun and Mallya, Arun and Liu, Ming-Yu",
  journal       = "arXiv [cs.CV]",
  abstract      = "We propose a neural talking-head video synthesis model and
                   demonstrate its application to video conferencing. Our model
                   learns to synthesize a talking-head video using a source
                   image containing the target person's appearance and a driving
                   video that dictates the motion in the output. Our motion is
                   encoded based on a novel keypoint representation, where the
                   identity-specific and motion-related information is
                   decomposed unsupervisedly. Extensive experimental validation
                   shows that our model outperforms competing methods on
                   benchmark datasets. Moreover, our compact keypoint
                   representation enables a video conferencing system that
                   achieves the same visual quality as the commercial H.264
                   standard while only using one-tenth of the bandwidth.
                   Besides, we show our keypoint representation allows the user
                   to rotate the head during synthesis, which is useful for
                   simulating face-to-face video conferencing experiences.",
  month         =  nov,
  year          =  2020,
  url           = "http://arxiv.org/abs/2011.15126",
  file          = "All Papers/Other/Wang et al. 2020 - One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2011.15126",
  keywords      = "eye contact;telepresence"
}

@ARTICLE{Zhang2021-nm,
  title         = "{Onfocus Detection: Identifying {Individual-Camera} Eye
                   Contact from Unconstrained Images}",
  author        = "Zhang, Dingwen and Wang, Bo and Wang, Gerong and Zhang, Qiang
                   and Zhang, Jiajia and Han, Jungong and You, Zheng",
  journal       = "arXiv [cs.CV]",
  abstract      = "Onfocus detection aims at identifying whether the focus of
                   the individual captured by a camera is on the camera or not.
                   Based on the behavioral research, the focus of an individual
                   during face-to-camera communication leads to a special type
                   of eye contact, i.e., the individual-camera eye contact,
                   which is a powerful signal in social communication and plays
                   a crucial role in recognizing irregular individual status
                   (e.g., lying or suffering mental disease) and special
                   purposes (e.g., seeking help or attracting fans). Thus,
                   developing effective onfocus detection algorithms is of
                   significance for assisting the criminal investigation,
                   disease discovery, and social behavior analysis. However, the
                   review of the literature shows that very few efforts have
                   been made toward the development of onfocus detector due to
                   the lack of large-scale public available datasets as well as
                   the challenging nature of this task. To this end, this paper
                   engages in the onfocus detection research by addressing the
                   above two issues. Firstly, we build a large-scale onfocus
                   detection dataset, named as the OnFocus Detection In the Wild
                   (OFDIW). It consists of 20,623 images in unconstrained
                   capture conditions (thus called ``in the wild'') and contains
                   individuals with diverse emotions, ages, facial
                   characteristics, and rich interactions with surrounding
                   objects and background scenes. On top of that, we propose a
                   novel end-to-end deep model, i.e., the eye-context
                   interaction inferring network (ECIIN), for onfocus detection,
                   which explores eye-context interaction via dynamic capsule
                   routing. Finally, comprehensive experiments are conducted on
                   the proposed OFDIW dataset to benchmark the existing learning
                   models and demonstrate the effectiveness of the proposed
                   ECIIN. The project (containing both datasets and codes) is at
                   https://github.com/wintercho/focus.",
  month         =  mar,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.15307",
  file          = "All Papers/Other/Zhang et al. 2021 - Onfocus Detection - Identifying Individual-Camera Eye Contact from Unconstrained Images.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2103.15307"
}

@ARTICLE{He2021-wl,
  title         = "{{LookAtChat}: Visualizing Gaze Awareness for Remote
                   {Small-Group} Conversations}",
  author        = "He, Zhenyi and Du, Ruofei and Perlin, Ken",
  journal       = "arXiv [cs.HC]",
  abstract      = "Video conferences play a vital role in our daily lives.
                   However, many nonverbal cues are missing, including gaze and
                   spatial information. We introduce LookAtChat, a web-based
                   video conferencing system, which empowers remote users to
                   identify gaze awareness and spatial relationships in
                   small-group conversations. Leveraging real-time eye-tracking
                   technology available with ordinary webcams, LookAtChat tracks
                   each user's gaze direction, identifies who is looking at
                   whom, and provides corresponding spatial cues. Informed by
                   formative interviews with 5 participants who regularly use
                   videoconferencing software, we explored the design space of
                   gaze visualization in both 2D and 3D layouts. We further
                   conducted an exploratory user study (N=20) to evaluate
                   LookAtChat in three conditions: baseline layout, 2D
                   directional layout, and 3D perspective layout. Our findings
                   demonstrate how LookAtChat engages participants in
                   small-group conversations, how gaze and spatial information
                   improve conversation quality, and the potential benefits and
                   challenges to incorporating gaze awareness visualization into
                   existing videoconferencing systems.",
  month         =  jul,
  year          =  2021,
  url           = "http://arxiv.org/abs/2107.06265",
  file          = "All Papers/Other/He et al. 2021 - LookAtChat - Visualizing Gaze Awareness for Remote Small-Group Conversations.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2107.06265",
  keywords      = "prj-gaze-design;uist2022-gaze-design"
}

@ARTICLE{Wu2021-ho,
  title         = "{{DOVE}: Learning deformable {3D} objects by watching videos}",
  author        = "Wu, Shangzhe and Jakab, Tomas and Rupprecht, Christian and
                   Vedaldi, Andrea",
  journal       = "arXiv [cs.CV]",
  abstract      = "Learning deformable 3D objects from 2D images is an extremely
                   ill-posed problem. Existing methods rely on explicit
                   supervision to establish multi-view correspondences, such as
                   template shape models and keypoint annotations, which
                   restricts their applicability on objects ``in the wild''. In
                   this paper, we propose to use monocular videos, which
                   naturally provide correspondences across time, allowing us to
                   learn 3D shapes of deformable object categories without
                   explicit keypoints or template shapes. Specifically, we
                   present DOVE, which learns to predict 3D canonical shape,
                   deformation, viewpoint and texture from a single 2D image of
                   a bird, given a bird video collection as well as
                   automatically obtained silhouettes and optical flows as
                   training data. Our method reconstructs temporally consistent
                   3D shape and deformation, which allows us to animate and
                   re-render the bird from arbitrary viewpoints from a single
                   image.",
  month         =  jul,
  year          =  2021,
  url           = "http://arxiv.org/abs/2107.10844",
  file          = "All Papers/Other/Wu et al. 2021 - DOVE - Learning deformable 3D objects by watching videos.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2107.10844"
}

@ARTICLE{Zhang2021-ki,
  title         = "{{VirtualCube}: An Immersive {3D} Video Communication System}",
  author        = "Zhang, Yizhong and Yang, Jiaolong and Liu, Zhen and Wang,
                   Ruicheng and Chen, Guojun and Tong, Xin and Guo, Baining",
  journal       = "arXiv [cs.CV]",
  abstract      = "The VirtualCube system is a 3D video conference system that
                   attempts to overcome some limitations of conventional
                   technologies. The key ingredient is VirtualCube, an abstract
                   representation of a real-world cubicle instrumented with RGBD
                   cameras for capturing the 3D geometry and texture of a user.
                   We design VirtualCube so that the task of data capturing is
                   standardized and significantly simplified, and everything can
                   be built using off-the-shelf hardware. We use VirtualCubes as
                   the basic building blocks of a virtual conferencing
                   environment, and we provide each VirtualCube user with a
                   surrounding display showing life-size videos of remote
                   participants. To achieve real-time rendering of remote
                   participants, we develop the V-Cube View algorithm, which
                   uses multi-view stereo for more accurate depth estimation and
                   Lumi-Net rendering for better rendering quality. The
                   VirtualCube system correctly preserves the mutual eye gaze
                   between participants, allowing them to establish eye contact
                   and be aware of who is visually paying attention to them. The
                   system also allows a participant to have side discussions
                   with remote participants as if they were in the same room.
                   Finally, the system sheds lights on how to support the shared
                   space of work items (e.g., documents and applications) and
                   track the visual attention of participants to work items.",
  month         =  dec,
  year          =  2021,
  url           = "http://arxiv.org/abs/2112.06730",
  file          = "All Papers/Other/Zhang et al. 2021 - VirtualCube - An Immersive 3D Video Communication System.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2112.06730",
  keywords      = "prj-gaze-design;uist2022-gaze-design"
}

@ARTICLE{Bhuiyan2022-so,
  title         = "{{OtherTube}: Facilitating Content Discovery and Reflection
                   by Exchanging {YouTube} Recommendations with Strangers}",
  author        = "Bhuiyan, Md Momen and Isaza, Carlos Augusto Bautista and
                   Mitra, Tanushree and Lee, Sang Won",
  journal       = "arXiv [cs.HC]",
  abstract      = "To promote engagement, recommendation algorithms on platforms
                   like YouTube increasingly personalize users' feeds, limiting
                   users' exposure to diverse content and depriving them of
                   opportunities to reflect on their interests compared to
                   others'. In this work, we investigate how exchanging
                   recommendations with strangers can help users discover new
                   content and reflect. We tested this idea by developing
                   OtherTube -- a browser extension for YouTube that displays
                   strangers' personalized YouTube recommendations. OtherTube
                   allows users to (i) create an anonymized profile for social
                   comparison, (ii) share their recommended videos with others,
                   and (iii) browse strangers' YouTube recommendations. We
                   conducted a 10-day-long user study (n=41) followed by a
                   post-study interview (n=11). Our results reveal that users
                   discovered and developed new interests from seeing OtherTube
                   recommendations. We identified user and content
                   characteristics that affect interaction and engagement with
                   exchanged recommendations; for example, younger users
                   interacted more with OtherTube, while the perceived
                   irrelevance of some content discouraged users from watching
                   certain videos. Users reflected on their interests as well as
                   others', recognizing similarities and differences. Our work
                   shows promise for designs leveraging the exchange of
                   personalized recommendations with strangers.",
  month         =  jan,
  year          =  2022,
  url           = "http://arxiv.org/abs/2201.11709",
  file          = "All Papers/Other/Bhuiyan et al. 2022 - OtherTube - Facilitating Content Discovery and Reflection by Exchanging YouTube Recommendations with Strangers.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2201.11709"
}

@ARTICLE{Jiang2022-uf,
  title         = "{Ditto: Building Digital Twins of Articulated Objects from
                   Interaction}",
  author        = "Jiang, Zhenyu and Hsu, Cheng-Chun and Zhu, Yuke",
  journal       = "arXiv [cs.CV]",
  abstract      = "Digitizing physical objects into the virtual world has the
                   potential to unlock new research and applications in embodied
                   AI and mixed reality. This work focuses on recreating
                   interactive digital twins of real-world articulated objects,
                   which can be directly imported into virtual environments. We
                   introduce Ditto to learn articulation model estimation and 3D
                   geometry reconstruction of an articulated object through
                   interactive perception. Given a pair of visual observations
                   of an articulated object before and after interaction, Ditto
                   reconstructs part-level geometry and estimates the
                   articulation model of the object. We employ implicit neural
                   representations for joint geometry and articulation modeling.
                   Our experiments show that Ditto effectively builds digital
                   twins of articulated objects in a category-agnostic way. We
                   also apply Ditto to real-world objects and deploy the
                   recreated digital twins in physical simulation. Code and
                   additional results are available at
                   https://ut-austin-rpl.github.io/Ditto",
  month         =  feb,
  year          =  2022,
  url           = "http://arxiv.org/abs/2202.08227",
  file          = "All Papers/Other/Jiang et al. 2022 - Ditto - Building Digital Twins of Articulated Objects from Interaction.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2202.08227",
  keywords      = "world model"
}

@ARTICLE{Suzuki2022-vn,
  title         = "{Augmented Reality and Robotics: A Survey and Taxonomy for
                   {AR-enhanced} {Human-Robot} Interaction and Robotic
                   Interfaces}",
  author        = "Suzuki, Ryo and Karim, Adnan and Xia, Tian and Hedayati,
                   Hooman and Marquardt, Nicolai",
  journal       = "arXiv [cs.RO]",
  abstract      = "This paper contributes to a taxonomy of augmented reality and
                   robotics based on a survey of 460 research papers. Augmented
                   and mixed reality (AR/MR) have emerged as a new way to
                   enhance human-robot interaction (HRI) and robotic interfaces
                   (e.g., actuated and shape-changing interfaces). Recently, an
                   increasing number of studies in HCI, HRI, and robotics have
                   demonstrated how AR enables better interactions between
                   people and robots. However, often research remains focused on
                   individual explorations and key design strategies, and
                   research questions are rarely analyzed systematically. In
                   this paper, we synthesize and categorize this research field
                   in the following dimensions: 1) approaches to augmenting
                   reality; 2) characteristics of robots; 3) purposes and
                   benefits; 4) classification of presented information; 5)
                   design components and strategies for visual augmentation; 6)
                   interaction techniques and modalities; 7) application
                   domains; and 8) evaluation strategies. We formulate key
                   challenges and opportunities to guide and inform future
                   research in AR and robotics.",
  month         =  mar,
  year          =  2022,
  url           = "http://dx.doi.org/10.1145/3491102.3517719",
  file          = "All Papers/Other/Suzuki et al. 2022 - Augmented Reality and Robotics - A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2203.03254",
  keywords      = "ar/mr",
  doi           = "10.1145/3491102.3517719"
}

@ARTICLE{Moon2024-wc,
  title         = "{VisionTrap: Vision-augmented trajectory prediction guided by
                   textual descriptions}",
  author        = "Moon, Seokha and Woo, Hyun and Park, Hongbeen and Jung, Haeji
                   and Mahjourian, Reza and Chi, Hyung-Gun and Lim, Hyerin and
                   Kim, Sangpil and Kim, Jinkyu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Predicting future trajectories for other road agents is an
                   essential task for autonomous vehicles. Established
                   trajectory prediction methods primarily use agent tracks
                   generated by a detection and tracking system and HD map as
                   inputs. In this work, we propose a novel method that also
                   incorporates visual input from surround-view cameras,
                   allowing the model to utilize visual cues such as human gazes
                   and gestures, road conditions, vehicle turn signals, etc,
                   which are typically hidden from the model in prior methods.
                   Furthermore, we use textual descriptions generated by a
                   Vision-Language Model (VLM) and refined by a Large Language
                   Model (LLM) as supervision during training to guide the model
                   on what to learn from the input data. Despite using these
                   extra inputs, our method achieves a latency of 53 ms, making
                   it feasible for real-time processing, which is significantly
                   faster than that of previous single-agent prediction methods
                   with similar performance. Our experiments show that both the
                   visual inputs and the textual descriptions contribute to
                   improvements in trajectory prediction performance, and our
                   qualitative analysis highlights how the model is able to
                   exploit these additional inputs. Lastly, in this work we
                   create and release the nuScenes-Text dataset, which augments
                   the established nuScenes dataset with rich textual
                   annotations for every scene, demonstrating the positive
                   impact of utilizing VLM on trajectory prediction. Our project
                   page is at https://moonseokha.github.io/VisionTrap/",
  month         =  "17~" # jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.12345",
  file          = "All Papers/Other/Moon et al. 2024 - VisionTrap - Vision-augmented trajectory prediction guided by textual descriptions.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2407.12345"
}

@ARTICLE{Liu2024-nf,
  title         = "{GEM: Context-aware Gaze EstiMation with visual search
                   behavior matching for chest radiograph}",
  author        = "Liu, Shaonan and Chen, Wenting and Liu, Jie and Luo, Xiaoling
                   and Shen, Linlin",
  journal       = "arXiv [cs.CV]",
  abstract      = "Gaze estimation is pivotal in human scene comprehension
                   tasks, particularly in medical diagnostic analysis.
                   Eye-tracking technology facilitates the recording of
                   physicians' ocular movements during image interpretation,
                   thereby elucidating their visual attention patterns and
                   information-processing strategies. In this paper, we
                   initially define the context-aware gaze estimation problem in
                   medical radiology report settings. To understand the
                   attention allocation and cognitive behavior of radiologists
                   during the medical image interpretation process, we propose a
                   context-aware Gaze EstiMation (GEM) network that utilizes eye
                   gaze data collected from radiologists to simulate their
                   visual search behavior patterns throughout the image
                   interpretation process. It consists of a context-awareness
                   module, visual behavior graph construction, and visual
                   behavior matching. Within the context-awareness module, we
                   achieve intricate multimodal registration by establishing
                   connections between medical reports and images. Subsequently,
                   for a more accurate simulation of genuine visual search
                   behavior patterns, we introduce a visual behavior graph
                   structure, capturing such behavior through high-order
                   relationships (edges) between gaze points (nodes). To
                   maintain the authenticity of visual behavior, we devise a
                   visual behavior-matching approach, adjusting the high-order
                   relationships between them by matching the graph constructed
                   from real and estimated gaze points. Extensive experiments on
                   four publicly available datasets demonstrate the superiority
                   of GEM over existing methods and its strong generalizability,
                   which also provides a new direction for the effective
                   utilization of diverse modalities in medical image
                   interpretation and enhances the interpretability of models in
                   the field of medical imaging. https://github.com/Tiger-SN/GEM",
  month         =  "10~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.05502",
  file          = "All Papers/Other/Liu et al. 2024 - GEM - Context-aware Gaze EstiMation with visual search behavior matching for chest radiograph.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2408.05502"
}

@ARTICLE{Cohn2024-od,
  title         = "{Multimodal methods for analyzing learning and training
                   environments: A systematic literature review}",
  author        = "Cohn, Clayton and Davalos, Eduardo and Vatral, Caleb and
                   Fonteles, Joyce Horn and Wang, Hanchen David and Ma, Meiyi
                   and Biswas, Gautam",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent technological advancements have enhanced our ability
                   to collect and analyze rich multimodal data (e.g., speech,
                   video, and eye gaze) to better inform learning and training
                   experiences. While previous reviews have focused on parts of
                   the multimodal pipeline (e.g., conceptual models and data
                   fusion), a comprehensive literature review on the methods
                   informing multimodal learning and training environments has
                   not been conducted. This literature review provides an
                   in-depth analysis of research methods in these environments,
                   proposing a taxonomy and framework that encapsulates recent
                   methodological advances in this field and characterizes the
                   multimodal domain in terms of five modality groups: Natural
                   Language, Video, Sensors, Human-Centered, and Environment
                   Logs. We introduce a novel data fusion category -- mid fusion
                   -- and a graph-based technique for refining literature
                   reviews, termed citation graph pruning. Our analysis reveals
                   that leveraging multiple modalities offers a more holistic
                   understanding of the behaviors and outcomes of learners and
                   trainees. Even when multimodality does not enhance predictive
                   accuracy, it often uncovers patterns that contextualize and
                   elucidate unimodal data, revealing subtleties that a single
                   modality may miss. However, there remains a need for further
                   research to bridge the divide between multimodal learning and
                   training studies and foundational AI research.",
  month         =  "22~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.14491",
  file          = "All Papers/Other/Cohn et al. 2024 - Multimodal methods for analyzing learning and training environments - A systematic literature review.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2408.14491"
}

@ARTICLE{Yu2024-kw,
  title         = "{Video-based analysis reveals atypical social gaze in people
                   with autism spectrum disorder}",
  author        = "Yu, Xiangxu and Ruan, Mindi and Hu, Chuanbo and Li, Wenqi and
                   Paul, Lynn K and Li, Xin and Wang, Shuo",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "In this study, we present a quantitative and comprehensive
                   analysis of social gaze in people with autism spectrum
                   disorder (ASD). Diverging from traditional first-person
                   camera perspectives based on eye-tracking technologies, this
                   study utilizes a third-person perspective database from the
                   Autism Diagnostic Observation Schedule, 2nd Edition (ADOS-2)
                   interview videos, encompassing ASD participants and
                   neurotypical individuals as a reference group. Employing
                   computational models, we extracted and processed gaze-related
                   features from the videos of both participants and examiners.
                   The experimental samples were divided into three groups based
                   on the presence of social gaze abnormalities and ASD
                   diagnosis. This study quantitatively analyzed four gaze
                   features: gaze engagement, gaze variance, gaze density map,
                   and gaze diversion frequency. Furthermore, we developed a
                   classifier trained on these features to identify gaze
                   abnormalities in ASD participants. Together, we demonstrated
                   the effectiveness of analyzing social gaze in people with ASD
                   in naturalistic settings, showcasing the potential of
                   third-person video perspectives in enhancing ASD diagnosis
                   through gaze analysis.",
  month         =  "1~" # sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.00664",
  file          = "All Papers/Other/Yu et al. 2024 - Video-based analysis reveals atypical social gaze in people with autism spectrum disorder.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2409.00664"
}

@ARTICLE{Jayawardena2024-ee,
  title         = "{Advanced Gaze Analytics Dashboard}",
  author        = "Jayawardena, Gavindya and Ashok, Vikas and Jayarathna,
                   Sampath",
  journal       = "arXiv [cs.HC]",
  abstract      = "Eye movements can provide informative cues to understand
                   human visual scan/search behavior and cognitive load during
                   varying tasks. Visualizations of real-time gaze measures
                   during tasks, provide an understanding of human behavior as
                   the experiment is being conducted. Even though existing eye
                   tracking analysis tools provide calculation and visualization
                   of eye-tracking data, none of them support real-time
                   visualizations of advanced gaze measures, such as ambient or
                   focal processing, or eye-tracked measures of cognitive load.
                   In this paper, we present an eye movements analytics
                   dashboard that enables visualizations of various gaze
                   measures, fixations, saccades, cognitive load, ambient-focal
                   attention, and gaze transitions analysis by extracting eye
                   movements from participants utilizing common off-the-shelf
                   eye trackers. We validate the proposed eye movement
                   visualizations by using two publicly available eye-tracking
                   datasets. We showcase that, the proposed dashboard could be
                   utilized to visualize advanced eye movement measures
                   generated using multiple data sources.",
  month         =  "10~" # sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.06628",
  file          = "All Papers/Other/Jayawardena et al. 2024 - Advanced Gaze Analytics Dashboard.pdf",
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2409.06628"
}

@ARTICLE{Harrison2018-if,
  title     = "{An experimental investigation into the use of eye-contact in
               social interactions in women in the acute and recovered stages of
               anorexia nervosa}",
  author    = "Harrison, Amy and Watterson, Stephanie V and Bennett, Samuel D",
  journal   = "The International journal of eating disorders",
  publisher = "Wiley",
  volume    =  52,
  number    =  1,
  pages     = "61--70",
  abstract  = "OBJECTIVE: People with anorexia nervosa (AN) report significant
               difficulties in social functioning and a growing literature is
               beginning to explain some of the differences in social skills
               that might underlie the social challenges experienced by
               patients. One vital area of social functioning that has been
               largely neglected to date is how eye-contact is used in the
               context of social stimuli and in social situations. METHODS: This
               cross-sectional, experimental study used eye-tracking to measure
               the frequency and duration of eye-contact made with the eye
               region of interest (ROI) of (1) static social stimuli (man and
               woman Ekman faces displaying basic emotions); (2) moving social
               stimuli (a video of two actors conversing); and (3) during a
               real-life social interaction in 75 women (25 with AN, 25
               recovered from AN, and 25 non-AN controls; mean age = 27.18, SD =
               6.19). RESULTS: Participants showed greater eye-contact during a
               real-life social interaction than when viewing static social
               stimuli. Those with AN made contact with the eye ROI of the
               static and moving social stimuli and during a real-life social
               interaction significantly less often and for significantly less
               time than non-AN controls. Those recovered from AN showed greater
               eye-contact than the acute group but significantly less
               eye-contact with the eye ROI across the static and moving social
               stimuli and during the real-life social interaction than non-AN
               controls. DISCUSSION: These findings contribute new knowledge
               regarding the types of social skills that people with AN may need
               additional support with to allow them to make greater use of
               social support in their recovery.",
  month     =  dec,
  year      =  2018,
  url       = "http://dx.doi.org/10.1002/eat.22993",
  file      = "All Papers/Other/Harrison et al. 2018 - An experimental investigation into the use of eye ... ctions in women in the acute and recovered stages of anorexia nervosa.pdf",
  keywords  = "anorexia nervosa; eating disorders; eye tracking; eye-contact;
               eye-gaze; social skills",
  doi       = "10.1002/eat.22993",
  pmid      =  30578634,
  issn      = "0276-3478,1098-108X",
  language  = "en"
}

@ARTICLE{Harrison2018-lo,
  title    = "{An experimental investigation into the use of eye-contact in
              social interactions in women in the acute and recovered stages of
              anorexia nervosa}",
  author   = "Harrison, Amy and Watterson, Stephanie V and Bennett, Samuel D",
  journal  = "The International journal of eating disorders",
  volume   =  52,
  number   =  1,
  pages    = "61–70",
  abstract = "OBJECTIVE: People with anorexia nervosa (AN) report significant
              difficulties in social functioning and a growing literature is
              beginning to explain some of the differences in social skills that
              might underlie the social challenges experienced by patients. One
              vital area of social functioning that has been largely neglected
              to date is how eye-contact is used in the context of social
              stimuli and in social situations. METHODS: This cross-sectional,
              experimental study used eye-tracking to measure the frequency and
              duration of eye-contact made with the eye region of interest (ROI)
              of (1) static social stimuli (man and woman Ekman faces displaying
              basic emotions); (2) moving social stimuli (a video of two actors
              conversing); and (3) during a real-life social interaction in 75
              women (25 with AN, 25 recovered from AN, and 25 non-AN controls;
              mean age = 27.18, SD = 6.19). RESULTS: Participants showed greater
              eye-contact during a real-life social interaction than when
              viewing static social stimuli. Those with AN made contact with the
              eye ROI of the static and moving social stimuli and during a
              real-life social interaction significantly less often and for
              significantly less time than non-AN controls. Those recovered from
              AN showed greater eye-contact than the acute group but
              significantly less eye-contact with the eye ROI across the static
              and moving social stimuli and during the real-life social
              interaction than non-AN controls. DISCUSSION: These findings
              contribute new knowledge regarding the types of social skills that
              people with AN may need additional support with to allow them to
              make greater use of social support in their recovery.",
  month    =  dec,
  year     =  2018,
  url      = "http://dx.doi.org/10.1002/eat.22993",
  file     = "All Papers/My Library/Harrison et al. 2018 - An experimental investigation into the use of eye ... ctions in women in the acute and recovered stages of anorexia nervosa.pdf",
  doi      = "10.1002/eat.22993",
  issn     = "0276-3478,1098-108X",
  language = "en"
}

@ARTICLE{Kim2020-dh,
  title     = "{Gaze window: A new gaze interface showing relevant content close
               to the gaze point}",
  author    = "Kim, Seungwon and Billinghurst, Mark and Lee, Gun and Huang,
               Weidong",
  journal   = "Journal of the Society for Information Display",
  publisher = "Wiley",
  volume    =  28,
  number    =  12,
  pages     = "979--996",
  abstract  = "Abstract In this paper, we propose a novel concept of the gaze
               window, which follows the user's gaze to show relevant
               information close to their gaze point on a screen space. We
               explore the use of a gaze window, especially for two use cases:
               (1) a single-user data entry task by displaying data-source image
               close to the gaze point where the data are typed in and (2) a
               teleconferencing setup showing a remote partner close to the gaze
               point. To investigate the performance of gaze window in these
               cases, we conducted two experiments. For the single-user task, we
               compared the gaze window with a baseline non-gaze-window
               condition and found that the gaze window significantly reduced
               the user's gaze movement and mental effort and increased
               usability. In the teleconferencing task, we compared displaying
               partner close to gaze point in the gaze window to displaying
               partner at the corner of screen, when two remote users watch a
               shared and synchronized content together. We found that
               displaying the partner's face nearby the user's gaze helped the
               user look at the partner more frequently, and for a longer time,
               significantly increasing co-presence and emotional
               interdependence. As a result of this research, we also presented
               design guidelines for gaze-window systems.",
  month     =  dec,
  year      =  2020,
  url       = "http://dx.doi.org/10.1002/jsid.954",
  file      = "All Papers/Other/Kim et al. 2020 - Gaze window - A new gaze interface showing relevant content close to the gaze point.pdf",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1002/jsid.954",
  issn      = "1071-0922,1938-3657",
  language  = "en"
}

@ARTICLE{Kim2020-qs,
  title    = "{Gaze window: A new gaze interface showing relevant content close
              to the gaze point}",
  author   = "Kim, Seungwon and Billinghurst, Mark and Lee, Gun and Huang,
              Weidong",
  journal  = "Journal of the Society for Information Display",
  volume   =  28,
  number   =  12,
  pages    = "979–996",
  abstract = "Abstract In this paper, we propose a novel concept of the gaze
              window, which follows the user's gaze to show relevant information
              close to their gaze point on a screen space. We explore the use of
              a gaze window, especially for two use cases: (1) a single-user
              data entry task by displaying data-source image close to the gaze
              point where the data are typed in and (2) a teleconferencing setup
              showing a remote partner close to the gaze point. To investigate
              the performance of gaze window in these cases, we conducted two
              experiments. For the single-user task, we compared the gaze window
              with a baseline non-gaze-window condition and found that the gaze
              window significantly reduced the user's gaze movement and mental
              effort and increased usability. In the teleconferencing task, we
              compared displaying partner close to gaze point in the gaze window
              to displaying partner at the corner of screen, when two remote
              users watch a shared and synchronized content together. We found
              that displaying the partner's face nearby the user's gaze helped
              the user look at the partner more frequently, and for a longer
              time, significantly increasing co-presence and emotional
              interdependence. As a result of this research, we also presented
              design guidelines for gaze-window systems.",
  month    =  dec,
  year     =  2020,
  url      = "http://dx.doi.org/10.1002/jsid.954",
  file     = "All Papers/My Library/Kim et al. 2020 - Gaze window - A new gaze interface showing relevant content close to the gaze point.pdf",
  doi      = "10.1002/jsid.954",
  issn     = "1071-0922,1938-3657",
  language = "en"
}

@ARTICLE{Pizzi2023-aa,
  title    = "{I, chatbot! the impact of anthropomorphism and gaze direction on
              willingness to disclose personal information and behavioral
              intentions}",
  author   = "Pizzi, Gabriele and Vannucci, Virginia and Mazzoli, Valentina and
              Donvito, Raffaele",
  journal  = "Psychology \& Marketing",
  volume   =  40,
  number   =  7,
  pages    = "1372--1387",
  abstract = "The present research focuses on the interplay between two common
              features of the customer service chatbot experience: gaze
              direction and anthropomorphism. Although the dominant approach in
              marketing theory and practice is to make chatbots as human-like as
              possible, the current study, built on the humanness-value-loyalty
              model, addresses the chain of effects through which chatbots'
              nonverbal behaviors affect customers' willingness to disclose
              personal information and purchase intentions. By means of two
              experiments that adopt a real chatbot in a simulated shopping
              environment (i.e., car rental and travel insurance), the present
              work allows us to understand how to reduce individuals' tendency
              to see conversational agents as less knowledgeable and empathetic
              compared with humans. The results show that warmth perceptions are
              affected by gaze direction, whereas competence perceptions are
              affected by anthropomorphism. Warmth and competence perceptions
              are found to be key drivers of consumers’ skepticism toward the
              chatbot, which, in turn, affects consumers’ trust toward the
              service provider hosting the chatbot, ultimately leading consumers
              to be more willing to disclose their personal information and to
              repatronize the e-tailer in the future. Building on the Theory of
              Mind, our results show that perceiving competence from a chatbot
              makes individuals less skeptical as long as they feel they are
              good at detecting others’ ultimate intentions.",
  year     =  2023,
  url      = "https://onlinelibrary.wiley.com/doi/abs/10.1002/mar.21813",
  file     = "All Papers/My Library/Pizzi et al. 2023 - I, chatbot! the impact of anthropomorphism and gaze ... illingness to disclose personal information and behavioral intentions.pdf",
  doi      = "10.1002/mar.21813",
  issn     = "1520-6793",
  language = "en"
}

@INCOLLECTION{Velichkovsky1997-qv,
  title     = "{Towards gaze-mediated interaction: Collecting solutions of the
               “Midas touch problem”}",
  author    = "Velichkovsky, Boris and Sprenger, Andreas and Unema, Pieter",
  editor    = "Howard, Steve and Hammond, Judy and Lindgaard, Gitte",
  booktitle = "{Human-Computer Interaction INTERACT ’97: IFIP TC13 International
               Conference on Human-Computer Interaction, 14th–18th July 1997,
               Sydney, Australia}",
  publisher = "Springer US",
  address   = "Boston, MA",
  pages     = "509--516",
  abstract  = "For a development of truly user-centered interfaces we need to
               take into account not only generic characteristics of human
               beings but also actual dynamics of attention and intentions of
               persons involved in an interaction. Modern eyetracking methods
               are indispensable tools in such a development, as they allow the
               use of eye movement data for control of output devices, for
               gaze-contingent image processing and for desambiguation of verbal
               as well as nonverbal information. The main obstacle on the way to
               these applications is the so-called “Midas touch problem”: how to
               differentiate “attentive” saccades with intended goal of
               communication from the lower level eye movements that are just
               random or provoked by external stimulation? We report results of
               our investigations of the problem and present a solution based on
               a functional classification of fixations correlated with their
               duration. Several additional solutions are also considered
               together with the data on the trainability of the human
               oculomotor system.",
  series    = "IFIP — The International Federation for Information Processing",
  year      =  1997,
  url       = "https://doi.org/10.1007/978-0-387-35175-9_77",
  file      = "All Papers/My Library/Velichkovsky et al. 1997 - Towards gaze-mediated interaction - Collecting solutions of the “Midas touch problem”.pdf",
  doi       = "10.1007/978-0-387-35175-9\_77",
  isbn      =  9780387351759,
  language  = "en"
}

@INCOLLECTION{Majaranta2014-kv,
  title     = "{Eye tracking and eye-based human–computer interaction}",
  author    = "Majaranta, Päivi and Bulling, Andreas",
  booktitle = "{Human–Computer Interaction Series}",
  publisher = "Springer London",
  address   = "London",
  pages     = "39--65",
  year      =  2014,
  url       = "https://consensus.apphttps://consensus.app/papers/tracking-eyebased-human%E2%80%93computer-interaction-majaranta/355b87fda4525457853121567b7da2a2/?extracted-answer=Eye+tracking+technology+has+the+potential+to+revolutionize+mainstream+human-technology+interaction+and+provide+a+means+of+communication+and+control+for+people+with+physical+disabilities.&q=research+that+eye+tracking+for+estimate+eye+contact+occurance+on+videoconferencing&copilot=on",
  file      = "All Papers/Other/Majaranta and Bulling 2014 - Eye tracking and eye-based human–computer interaction.pdf",
  doi       = "10.1007/978-1-4471-6392-3\_3",
  isbn      = "9781447163916,9781447163923",
  issn      = "1571-5035"
}

@INPROCEEDINGS{Brau2018-ue,
  title     = "{Multiple-gaze geometry: Inferring novel 3D locations from gazes
               observed in monocular video}",
  author    = "Brau, Ernesto and Guan, Jinyan and Jeffries, Tanya and Barnard,
               Kobus",
  booktitle = "{Computer vision – ECCV 2018}",
  publisher = "Springer International Publishing",
  pages     = "641–659",
  abstract  = "We develop using person gaze direction for scene understanding.
               In particular, we use intersecting gazes to learn 3D locations
               that people tend to look at, which is analogous to having
               multiple camera views. The 3D locations that we discover need not
               be visible to the camera. Conversely, knowing 3D locations of
               scene elements that draw visual attention, such as other people
               in the scene, can help infer gaze direction. We provide a
               Bayesian generative model for the temporal scene that captures
               the joint probability of camera parameters, locations of people,
               their gaze, what they are looking at, and locations of visual
               attention. Both the number of people in the scene and the number
               of extra objects that draw attention are unknown and need to be
               inferred. To execute this joint inference we use a probabilistic
               data association approach that enables principled comparison of
               model hypotheses. We use MCMC for inference over the discrete
               correspondence variables, and approximate the marginalization
               over continuous parameters using the Metropolis-Laplace
               approximation, using Hamiltonian (Hybrid) Monte Carlo for
               maximization. As existing data sets do not provide the 3D
               locations of what people are looking at, we contribute a small
               data set that does. On this data set, we infer what people are
               looking at with 59\% precision compared with 13\% for a baseline
               approach, and where those objects are within about 0.58 m.",
  year      =  2018,
  url       = "http://dx.doi.org/10.1007/978-3-030-01225-0_38",
  file      = "All Papers/My Library/Brau et al. 2018 - Multiple-gaze geometry - Inferring novel 3D locations from gazes observed in monocular video.pdf",
  doi       = "10.1007/978-3-030-01225-0\_38"
}

@INCOLLECTION{Norouzi2019-lj,
  title     = "{A systematic review of the convergence of augmented reality,
               intelligent virtual agents, and the internet of things}",
  author    = "Norouzi, Nahal and Bruder, Gerd and Belna, Brandon and Mutter,
               Stefanie and Turgut, Damla and Welch, Greg",
  editor    = "Al-Turjman, Fadi",
  booktitle = "{Artificial intelligence in IoT}",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "1–24",
  abstract  = "In recent years we are beginning to see the convergence of three
               distinct research fields: augmented reality (AR), intelligent
               virtual agents (IVAs), and the Internet of things (IoT). Each of
               these has been classified as a disruptive technology for our
               society. Since their emergence, the advancement of knowledge and
               development of technologies and systems in these fields were
               traditionally performed with limited input from each other.
               However, over recent years, we have seen research prototypes and
               commercial products being developed that cross the boundaries
               between these distinct fields to leverage their collective
               strengths. In this paper, we review the body of literature
               published at the intersections between each two of these fields,
               and we discuss a vision for the nexus of all three technologies.",
  year      =  2019,
  url       = "https://doi.org/10.1007/978-3-030-04110-6_1",
  doi       = "10.1007/978-3-030-04110-6\_1",
  isbn      =  9783030041106
}

@INCOLLECTION{Hutton2019-vh,
  title     = "{Eye tracking methodology}",
  author    = "Hutton, S B",
  booktitle = "{Eye Movement Research}",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "277--308",
  year      =  2019,
  url       = "https://consensus.apphttps://consensus.app/papers/tracking-methodology-hutton/6a125f0dd61556b88a3d2f463edf0257/?extracted-answer=Eye+tracking+technology+can+be+used+to+establish+point+of+gaze+and+oculomotor+dynamics%2C+and+improve+accuracy%2C+precision%2C+sampling+rate%2C+and+timing+in+psychology+and+cognitive+neuroscience+research.&q=research+that+eye+tracking+for+estimate+eye+contact+occurance+on+videoconferencing&copilot=on",
  doi       = "10.1007/978-3-030-20085-5\_8",
  isbn      = "9783030200831,9783030200855",
  issn      = "2196-6605,2196-6613"
}

@INPROCEEDINGS{Le2019-xm,
  title     = "{GazeLens: Guiding attention to improve gaze interpretation in
               Hub-Satellite collaboration}",
  author    = "Le, Khanh-Duy and Avellino, Ignacio and Fleury, Cédric and Fjeld,
               Morten and Kunz, Andreas",
  booktitle = "{Human-Computer interaction – INTERACT 2019}",
  publisher = "Springer International Publishing",
  pages     = "282–303",
  abstract  = "In hub-satellite collaboration using video, interpreting gaze
               direction is critical for communication between hub coworkers
               sitting around a table and their remote satellite colleague.
               However, 2D video distorts images and makes this interpretation
               inaccurate. We present GazeLens, a video conferencing system that
               improves hub coworkers' ability to interpret the satellite
               worker's gaze. A 360{}360∘camera captures the hub coworkers and a
               ceiling camera captures artifacts on the hub table. The system
               combines these two video feeds in an interface. Lens widgets
               strategically guide the satellite worker's attention toward
               specific areas of her/his screen allow hub coworkers to clearly
               interpret her/his gaze direction. Our evaluation shows that
               GazeLens (1) increases hub coworkers' overall gaze interpretation
               accuracy by 25.8\%25.8\%in comparison to a conventional video
               conferencing system, (2) especially for physical artifacts on the
               hub table, and (3) improves hub coworkers' ability to distinguish
               between gazes toward people and artifacts. We discuss how screen
               space can be leveraged to improve gaze interpretation.",
  year      =  2019,
  url       = "http://dx.doi.org/10.1007/978-3-030-29384-0_18",
  file      = "All Papers/My Library/Le et al. 2019 - GazeLens - Guiding attention to improve gaze interpretation in Hub-Satellite collaboration.pdf",
  doi       = "10.1007/978-3-030-29384-0\_18"
}

@INPROCEEDINGS{Le2019-ww,
  title     = "{{GazeLens}: Guiding Attention to Improve Gaze Interpretation in
               {Hub-Satellite} Collaboration}",
  author    = "Le, Khanh-Duy and Avellino, Ignacio and Fleury, Cédric and Fjeld,
               Morten and Kunz, Andreas",
  booktitle = "{{Human-Computer} Interaction -- {INTERACT} 2019}",
  publisher = "Springer International Publishing",
  pages     = "282--303",
  abstract  = "In hub-satellite collaboration using video, interpreting gaze
               direction is critical for communication between hub coworkers
               sitting around a table and their remote satellite colleague.
               However, 2D video distorts images and makes this interpretation
               inaccurate. We present GazeLens, a video conferencing system that
               improves hub coworkers' ability to interpret the satellite
               worker's gaze. A $$360\textasciicircum\{\textbackslashcirc
               \}$$360∘camera captures the hub coworkers and a ceiling camera
               captures artifacts on the hub table. The system combines these
               two video feeds in an interface. Lens widgets strategically guide
               the satellite worker's attention toward specific areas of her/his
               screen allow hub coworkers to clearly interpret her/his gaze
               direction. Our evaluation shows that GazeLens (1) increases hub
               coworkers' overall gaze interpretation accuracy by
               $$25.8\%$$25.8\%in comparison to a conventional video
               conferencing system, (2) especially for physical artifacts on the
               hub table, and (3) improves hub coworkers' ability to distinguish
               between gazes toward people and artifacts. We discuss how screen
               space can be leveraged to improve gaze interpretation.",
  year      =  2019,
  url       = "http://dx.doi.org/10.1007/978-3-030-29384-0_18",
  file      = "All Papers/Other/Le et al. 2019 - GazeLens - Guiding Attention to Improve Gaze Interpretation in Hub-Satellite Collaboration.pdf",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1007/978-3-030-29384-0\_18"
}

@INPROCEEDINGS{Sarcar2020-tn,
  title     = "{Usability evaluation of short Dwell-Time activated eye typing
               techniques}",
  author    = "Sarcar, Sayan",
  booktitle = "{Universal access in Human-Computer interaction. Design
               approaches and supporting technologies}",
  publisher = "Springer International Publishing",
  pages     = "188–210",
  abstract  = "Gaze-based interfaces introduce dwell time-based selection method
               to avoid the Midas Touch problem - it is a fixed amount of time
               the users must fixate their gaze upon an object before it is
               selected. In gaze-based text typing, spending such time on each
               character key composition effectively decreases the overall eye
               typing rate. Researchers proposed several interaction mechanisms
               to minimize or diminish the dwelling in the desktop environment,
               however, they lack in understanding the usability of such
               mechanisms for regular users, specifically in a constrained eye
               tracking setup. We conducted a within-subject usability
               evaluation of four such representative short dwell-time activated
               eye typing techniques. The results of the first-time usability
               study, longitudinal study and subjective evaluation conducted
               with 15 participants confirm the superiority of controlled eye
               movement-based advanced eye typing method (Adv-EyeK) compare to
               the other techniques.",
  year      =  2020,
  url       = "http://dx.doi.org/10.1007/978-3-030-49282-3_14",
  file      = "All Papers/My Library/Sarcar 2020 - Usability evaluation of short Dwell-Time activated eye typing techniques.pdf",
  doi       = "10.1007/978-3-030-49282-3\_14"
}

@INPROCEEDINGS{Sarcar2020-xa,
  title     = "{Usability Evaluation of Short {Dwell-Time} Activated Eye Typing
               Techniques}",
  author    = "Sarcar, Sayan",
  booktitle = "{Universal Access in {Human-Computer} Interaction. Design
               Approaches and Supporting Technologies}",
  publisher = "Springer International Publishing",
  pages     = "188--210",
  abstract  = "Gaze-based interfaces introduce dwell time-based selection method
               to avoid the Midas Touch problem - it is a fixed amount of time
               the users must fixate their gaze upon an object before it is
               selected. In gaze-based text typing, spending such time on each
               character key composition effectively decreases the overall eye
               typing rate. Researchers proposed several interaction mechanisms
               to minimize or diminish the dwelling in the desktop environment,
               however, they lack in understanding the usability of such
               mechanisms for regular users, specifically in a constrained eye
               tracking setup. We conducted a within-subject usability
               evaluation of four such representative short dwell-time activated
               eye typing techniques. The results of the first-time usability
               study, longitudinal study and subjective evaluation conducted
               with 15 participants confirm the superiority of controlled eye
               movement-based advanced eye typing method (Adv-EyeK) compare to
               the other techniques.",
  year      =  2020,
  url       = "http://dx.doi.org/10.1007/978-3-030-49282-3_14",
  file      = "All Papers/Other/Sarcar 2020 - Usability Evaluation of Short Dwell-Time Activated Eye Typing Techniques.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1007/978-3-030-49282-3\_14"
}

@INCOLLECTION{Perera2020-oi,
  title     = "{HDGI: A human device gesture interaction ontology for the
               internet of things}",
  author    = "Perera, Madhawa and Haller, Armin and Méndez, Sergio José
               Rodríguez and Adcock, Matt",
  booktitle = "{Lecture notes in computer science}",
  publisher = "Springer International Publishing",
  pages     = "111–126",
  year      =  2020,
  url       = "https://doi.org/10.1007%2F978-3-030-62466-8_8",
  file      = "All Papers/My Library/Perera et al. 2020 - HDGI - A human device gesture interaction ontology for the internet of things.pdf",
  doi       = "10.1007/978-3-030-62466-8\_8"
}

@INPROCEEDINGS{Park2021-eo,
  title     = "{Talking through the eyes: User experience design for eye gaze
               redirection in live video conferencing}",
  author    = "Park, Wooyeong and Heo, Jeongyun and Lee, Jiyoon",
  booktitle = "{Human-Computer interaction. Interaction techniques and novel
               applications}",
  publisher = "Springer International Publishing",
  pages     = "75–88",
  abstract  = "In the post-corona era, more institutions are using
               videoconferencing (VC). However, when we talked in VC, we found
               that people could not easily concentrate on the conversation.
               This is a problem with computer architecture. Because the web
               camera records a person looking at a computer monitor, it appears
               as if people are staring at other places and having a
               conversation. This was a problem caused by overlooking eye
               contact, a non-verbal element of face-to-face conversation. To
               solve this problem, many studies focused on technology in
               literature. However, this study presented ER guidelines in terms
               of user experience: the function of selecting the direction of
               the face, the function of selecting a 3D avatar face divided into
               four stages through morphing, and a guideline on the function of
               intentionally staring at the camera using the teleprompter
               function.",
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/978-3-030-78465-2_7",
  file      = "All Papers/My Library/Park et al. 2021 - Talking through the eyes - User experience design for eye gaze redirection in live video conferencing.pdf",
  doi       = "10.1007/978-3-030-78465-2\_7"
}

@INPROCEEDINGS{Park2021-kt,
  title     = "{Talking Through the Eyes: User Experience Design for Eye Gaze
               Redirection in Live Video Conferencing}",
  author    = "Park, Wooyeong and Heo, Jeongyun and Lee, Jiyoon",
  booktitle = "{{Human-Computer} Interaction. Interaction Techniques and Novel
               Applications}",
  publisher = "Springer International Publishing",
  pages     = "75--88",
  abstract  = "In the post-corona era, more institutions are using
               videoconferencing (VC). However, when we talked in VC, we found
               that people could not easily concentrate on the conversation.
               This is a problem with computer architecture. Because the web
               camera records a person looking at a computer monitor, it appears
               as if people are staring at other places and having a
               conversation. This was a problem caused by overlooking eye
               contact, a non-verbal element of face-to-face conversation. To
               solve this problem, many studies focused on technology in
               literature. However, this study presented ER guidelines in terms
               of user experience: the function of selecting the direction of
               the face, the function of selecting a 3D avatar face divided into
               four stages through morphing, and a guideline on the function of
               intentionally staring at the camera using the teleprompter
               function.",
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/978-3-030-78465-2_7",
  file      = "All Papers/Other/Park et al. 2021 - Talking Through the Eyes - User Experience Design for Eye Gaze Redirection in Live Video Conferencing.pdf",
  doi       = "10.1007/978-3-030-78465-2\_7"
}

@INPROCEEDINGS{Iitsuka2021-mv,
  title     = "{Multi-party video conferencing system with gaze cues
               representation for Turn-Taking}",
  author    = "Iitsuka, Rikuto and Kawaguchi, Ikkaku and Shizuki, Buntarou and
               Takahashi, Shin",
  booktitle = "{Collaboration technologies and social computing}",
  publisher = "Springer International Publishing",
  pages     = "101–108",
  abstract  = "In a multi-party video conference, it is more difficult to
               achieve smooth turn-taking than in face-to-face communication.
               This is probably because gaze cues are not shared. In this paper,
               we propose a system for facilitating turn-taking through the
               sharing of gaze cues in multi-party video conferences. We
               implemented video conferencing systems that use arrows and
               modification of the video window size to share gaze cues the same
               as in face-to-face communication. We also conducted an experiment
               to investigate the effect of the system on turn-taking. The
               results suggested that our system could facilitate turn-taking
               and communication.",
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/978-3-030-85071-5_8",
  file      = "All Papers/My Library/Iitsuka et al. 2021 - Multi-party video conferencing system with gaze cues representation for Turn-Taking.pdf",
  doi       = "10.1007/978-3-030-85071-5\_8"
}

@INPROCEEDINGS{Iitsuka2021-rt,
  title     = "{Multi-party Video Conferencing System with Gaze Cues
               Representation for {Turn-Taking}}",
  author    = "Iitsuka, Rikuto and Kawaguchi, Ikkaku and Shizuki, Buntarou and
               Takahashi, Shin",
  booktitle = "{Collaboration Technologies and Social Computing}",
  publisher = "Springer International Publishing",
  pages     = "101--108",
  abstract  = "In a multi-party video conference, it is more difficult to
               achieve smooth turn-taking than in face-to-face communication.
               This is probably because gaze cues are not shared. In this paper,
               we propose a system for facilitating turn-taking through the
               sharing of gaze cues in multi-party video conferences. We
               implemented video conferencing systems that use arrows and
               modification of the video window size to share gaze cues the same
               as in face-to-face communication. We also conducted an experiment
               to investigate the effect of the system on turn-taking. The
               results suggested that our system could facilitate turn-taking
               and communication.",
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/978-3-030-85071-5_8",
  file      = "All Papers/Other/Iitsuka et al. 2021 - Multi-party Video Conferencing System with Gaze Cues Representation for Turn-Taking.pdf",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1007/978-3-030-85071-5\_8"
}

@INPROCEEDINGS{Yamato2021-ne,
  title     = "{FGFlick: Augmenting single-finger input vocabulary for
               smartphones with simultaneous finger and gaze flicks}",
  author    = "Yamato, Yuki and Suzuki, Yutaro and Takahashi, Shin",
  booktitle = "{Human-computer interaction – INTERACT 2021}",
  publisher = "Springer International Publishing",
  pages     = "421–425",
  abstract  = "FGFlick is an interactive technique featuring simultaneous
               single-finger operation and a gaze. The user flicks a smartphone
               and moves their gaze linearly. FGFlick thus augments the
               single-finger input vocabulary. As a result of the evaluation of
               the FGFlick gestures, we achieved success rates of 84.0\%.",
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/978-3-030-85607-6_50",
  file      = "All Papers/My Library/Yamato et al. 2021 - FGFlick - Augmenting single-finger input vocabulary for smartphones with simultaneous finger and gaze flicks.pdf",
  doi       = "10.1007/978-3-030-85607-6\_50"
}

@INPROCEEDINGS{Niwa2022-gt,
  title     = "{EMS-Supported Throwing: Preliminary Investigation on
               EMS-Supported Training of Movement Form}",
  author    = "Niwa, Ryogo and Izumi, Kazuya and Suzuki, Shieru and Ochiai,
               Yoichi",
  booktitle = "{Universal Access in Human-Computer Interaction. Novel Design
               Approaches and Technologies: 16th International Conference, UAHCI
               2022, Held as Part of the 24th HCI International Conference, HCII
               2022, Virtual Event, June 26 – July 1, 2022, Proceedings, Part I}",
  publisher = "Springer-Verlag",
  address   = "Berlin, Heidelberg",
  pages     = "459–476",
  abstract  = "We propose a learning support system with extremely low latency
               and low cognitive load to correct the user’s motion. In previous
               studies, visual and haptic feedback has been mainly used to
               support motion learning, but there is a delay between the
               presentation of the stimulus and the modification of the action.
               However, this delay is due to reaction time and cognitive load
               and is difficult to shorten. This study proposed a system for
               solving this problem by combining Electrical Muscle Stimulation
               (EMS) and prediction of the user’s motion. In order to improve
               the control ability of the underhand throwing, we used the system
               to tell the subject the release point during the underhand
               throwing motion and verified the learning effect. This experiment
               revealed that EMS tended to be effective in teaching the ball’s
               release point, although it did not improve the control ability of
               the underhand throwing motion. In addition, although the
               effectiveness of EMS for motion learning was not yet fully
               evaluated, this study showed the possibility of applying EMS to
               support learning of motion.",
  year      =  2022,
  url       = "https://doi.org/10.1007/978-3-031-05028-2_31",
  doi       = "10.1007/978-3-031-05028-2\_31",
  isbn      =  9783031050275
}

@INPROCEEDINGS{Izumi2023-et,
  title     = "{A Preliminary Study on Eye Contact Framework Toward Improving
               Gaze Awareness in Video Conferences}",
  author    = "Izumi, Kazuya and Suzuki, Shieru and Niwa, Ryogo and Shinoda,
               Atsushi and Iijima, Ryo and Hyakuta, Ryosuke and Ochiai, Yoichi",
  booktitle = "{Human-Computer Interaction: Thematic Area, HCI 2023, Held as
               Part of the 25th HCI International Conference, HCII 2023,
               Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part I}",
  publisher = "Springer-Verlag",
  address   = "Berlin, Heidelberg",
  pages     = "484–498",
  abstract  = "Gaze information plays an important role as non-verbal
               information in face-to-face conversations. However, in online
               videoconferences, users’ gaze is perceived as misaligned due to
               the different positions of the screen and the camera. This
               problem causes a lack of gaze information, such as gaze
               awareness. To solve this problem, gaze correction methods in
               videoconference have been extensively discussed, and these
               methods allow us to maintain eye contact with other participants
               even in videoconference. However, people rarely make constant eye
               contact with the other person in face-to-face conversations.
               Although a person’s gaze generally reflects their intentions, if
               the system unconditionally corrects gaze, the intention of the
               user’s gaze is incorrectly conveyed. Therefore, we conducted a
               preliminary study to develop an eye contact framework; a system
               that corrects the user’s gaze only when the system detects that
               the user is looking at the face of the videoconferencing
               participant. In this study, participants used this system in a
               online conference and evaluated it qualitatively. As a result,
               this prototype was not significant in the evaluation of gaze
               awareness, but useful feedback was obtained from the
               questionnaire. We will improve this prototype and aim to develop
               a framework to facilitate non-verbal communication in online
               videoconferences.",
  year      =  2023,
  url       = "https://doi.org/10.1007/978-3-031-35596-7_31",
  doi       = "10.1007/978-3-031-35596-7\_31",
  isbn      =  9783031355950
}

@INPROCEEDINGS{Kim2013-nj,
  title     = "{MARIO: Mid-Air Augmented RealityInteraction with Objects}",
  author    = "Kim, Hanyuool and Takahashi, Issei and Yamamoto, Hiroki and Kai,
               Takayuki and Maekawa, Satoshi and Naemura, Takeshi",
  editor    = "Reidsma, Dennis and Katayose, Haruhiro and Nijholt, Anton",
  booktitle = "{Advances in Computer Entertainment}",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "560--563",
  abstract  = "This paper proposes a novel interactive system that supports
               augmented reality interaction between mid-air images and physical
               objects. Our “Mid-air Augmented Reality Interaction with Objects
               (MARIO)” system enables visual images to be displayed at various
               positions and precise depths in mid-air. For entertainment
               purposes, a game character appears in mid-air and runs around and
               over “real” blocks which users have arranged by hands. Users
               thereby enjoy interaction with physical blocks and virtual
               images.",
  series    = "Lecture Notes in Computer Science",
  year      =  2013,
  url       = "http://dx.doi.org/10.1007/978-3-319-03161-3_53",
  file      = "All Papers/My Library/Kim et al. 2013 - MARIO - Mid-Air Augmented RealityInteraction with Objects.pdf",
  doi       = "10.1007/978-3-319-03161-3\_53",
  isbn      =  9783319031613,
  language  = "en"
}

@INPROCEEDINGS{Yeoh2015-gd,
  title     = "{Eyes and Keys: An Evaluation of Click Alternatives Combining
               Gaze and Keyboard}",
  author    = "Yeoh, Ken Neth and Lutteroth, Christof and Weber, Gerald",
  editor    = "Abascal, Julio and Barbosa, Simone and Fetter, Mirko and Gross,
               Tom and Palanque, Philippe and Winckler, Marco",
  booktitle = "{Human-Computer Interaction – INTERACT 2015}",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "367--383",
  abstract  = "With eye gaze tracking technology entering the consumer market,
               there is an increased interest in using it as an input device,
               similar to the mouse. This holds promise for situations where a
               typical desk space is not available. While gaze seems natural for
               pointing, it is inherently inaccurate, which makes the design of
               fast and accurate methods for clicking targets (“click
               alternatives”) difficult. We investigate click alternatives that
               combine gaze with a standard keyboard (“gaze \& key click
               alternatives”) to achieve an experience where the user’s hands
               can remain on the keyboard all the time. We propose three novel
               click alternatives (“Letter Assignment”, “Offset Menu” and “Ray
               Selection”) and present an experiment that compares them with a
               naive gaze pointing approach (“Gaze \& Click”) and the mouse. The
               experiment uses a randomized, realistic click task in a web
               browser to collect data about click times and click accuracy, as
               well as asking users for their preference. Our results indicate
               that eye gaze tracking is currently too inaccurate for the Gaze
               \& Click approach to work reliably. While Letter Assignment and
               Offset Menu were usable and a large improvement, they were still
               significantly slower and less accurate than the mouse.",
  series    = "Lecture Notes in Computer Science",
  year      =  2015,
  url       = "http://dx.doi.org/10.1007/978-3-319-22701-6_28",
  file      = "All Papers/My Library/Yeoh et al. 2015 - Eyes and Keys - An Evaluation of Click Alternatives Combining Gaze and Keyboard.pdf",
  doi       = "10.1007/978-3-319-22701-6\_28",
  isbn      =  9783319227016,
  language  = "en"
}

@INCOLLECTION{Qin2015-nc,
  title     = "{Eye gaze correction with a single webcam based on
               eye-replacement}",
  author    = "Qin, Yalun and Lien, Kuo-Chin and Turk, Matthew and Höllerer,
               Tobias",
  booktitle = "{Advances in Visual Computing}",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "599--609",
  abstract  = "This work implemented a gaze correction system that can
               automatically maintain eye contact by replacing the eyes of the
               user with the direct looking eyes (looking directly into the
               camera) captured in the initialization stage. In traditional
               video conferencing systems, it is impossible for users to have
               eye contact when looking at the conversation partner's face
               displayed on the screen, due to the disparity between the
               locations of the camera and the screen. In this work, we
               implemented a gaze correction system that can automatically
               maintain eye contact by replacing the eyes of the user with the
               direct looking eyes (looking directly into the camera) captured
               in the initialization stage. Our real-time system has good
               robustness against different lighting conditions and head poses,
               and it provides visually convincing and natural results while
               relying only on a single webcam that can be positioned almost
               anywhere around the screen.",
  series    = "Lecture notes in computer science",
  year      =  2015,
  url       = "http://dx.doi.org/10.1007/978-3-319-27857-5_54",
  file      = "All Papers/Other/Qin et al. 2015 - Eye gaze correction with a single webcam based on eye-replacement.pdf",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1007/978-3-319-27857-5\_54",
  isbn      = "9783319278568,9783319278575",
  issn      = "0302-9743,1611-3349",
  language  = "en"
}

@INCOLLECTION{Qin2015-vn,
  title     = "{Eye gaze correction with a single webcam based on
               eye-replacement}",
  author    = "Qin, Yalun and Lien, Kuo-Chin and Turk, Matthew and Höllerer,
               Tobias",
  booktitle = "{Advances in visual computing}",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "599–609",
  abstract  = "This work implemented a gaze correction system that can
               automatically maintain eye contact by replacing the eyes of the
               user with the direct looking eyes (looking directly into the
               camera) captured in the initialization stage. In traditional
               video conferencing systems, it is impossible for users to have
               eye contact when looking at the conversation partner's face
               displayed on the screen, due to the disparity between the
               locations of the camera and the screen. In this work, we
               implemented a gaze correction system that can automatically
               maintain eye contact by replacing the eyes of the user with the
               direct looking eyes (looking directly into the camera) captured
               in the initialization stage. Our real-time system has good
               robustness against different lighting conditions and head poses,
               and it provides visually convincing and natural results while
               relying only on a single webcam that can be positioned almost
               anywhere around the screen.",
  series    = "Lecture notes in computer science",
  year      =  2015,
  url       = "http://dx.doi.org/10.1007/978-3-319-27857-5_54",
  file      = "All Papers/My Library/Qin et al. 2015 - Eye gaze correction with a single webcam based on eye-replacement.pdf",
  doi       = "10.1007/978-3-319-27857-5\_54",
  isbn      = "9783319278568,9783319278575",
  language  = "en"
}

@INPROCEEDINGS{Ganin2016-rv,
  title     = "{DeepWarp: Photorealistic image resynthesis for gaze
               manipulation}",
  author    = "Ganin, Yaroslav and Kononenko, Daniil and Sungatullina, Diana and
               Lempitsky, Victor",
  booktitle = "{Computer vision – ECCV 2016}",
  publisher = "Springer International Publishing",
  pages     = "311–326",
  abstract  = "In this work, we consider the task of generating highly-realistic
               images of a given face with a redirected gaze. We treat this
               problem as a specific instance of conditional image generation
               and suggest a new deep architecture that can handle this task
               very well as revealed by numerical comparison with prior art and
               a user study. Our deep architecture performs coarse-to-fine
               warping with an additional intensity correction of individual
               pixels. All these operations are performed in a feed-forward
               manner, and the parameters associated with different operations
               are learned jointly in the end-to-end fashion. After learning,
               the resulting neural network can synthesize images with
               manipulated gaze, while the redirection angle can be selected
               arbitrarily from a certain range and provided as an input to the
               network.",
  year      =  2016,
  url       = "http://dx.doi.org/10.1007/978-3-319-46475-6_20",
  file      = "All Papers/My Library/Ganin et al. 2016 - DeepWarp - Photorealistic image resynthesis for gaze manipulation.pdf",
  doi       = "10.1007/978-3-319-46475-6\_20"
}

@INPROCEEDINGS{Ganin2016-zr,
  title     = "{{DeepWarp}: Photorealistic Image Resynthesis for Gaze
               Manipulation}",
  author    = "Ganin, Yaroslav and Kononenko, Daniil and Sungatullina, Diana and
               Lempitsky, Victor",
  booktitle = "{Computer Vision -- {ECCV} 2016}",
  publisher = "Springer International Publishing",
  pages     = "311--326",
  abstract  = "In this work, we consider the task of generating highly-realistic
               images of a given face with a redirected gaze. We treat this
               problem as a specific instance of conditional image generation
               and suggest a new deep architecture that can handle this task
               very well as revealed by numerical comparison with prior art and
               a user study. Our deep architecture performs coarse-to-fine
               warping with an additional intensity correction of individual
               pixels. All these operations are performed in a feed-forward
               manner, and the parameters associated with different operations
               are learned jointly in the end-to-end fashion. After learning,
               the resulting neural network can synthesize images with
               manipulated gaze, while the redirection angle can be selected
               arbitrarily from a certain range and provided as an input to the
               network.",
  year      =  2016,
  url       = "http://dx.doi.org/10.1007/978-3-319-46475-6_20",
  file      = "All Papers/Other/Ganin et al. 2016 - DeepWarp - Photorealistic Image Resynthesis for Gaze Manipulation.pdf",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1007/978-3-319-46475-6\_20"
}

@INCOLLECTION{Ducasse2017-es,
  title     = "{Accessible interactive maps for visually impaired users}",
  author    = "Ducasse, Julie and Brock, Anke M and Jouffrais, Christophe",
  booktitle = "{Mobility of visually impaired people}",
  publisher = "Springer International Publishing",
  pages     = "537–584",
  month     =  aug,
  year      =  2017,
  url       = "https://doi.org/10.1007%2F978-3-319-54446-5_17",
  file      = "All Papers/My Library/Ducasse et al. 2017 - Accessible interactive maps for visually impaired users.pdf",
  doi       = "10.1007/978-3-319-54446-5\_17"
}

@INCOLLECTION{Parekh2017-lg,
  title     = "{Eye contact detection via deep neural networks}",
  author    = "Parekh, Viral and Subramanian, Ramanathan and Jawahar, C V",
  booktitle = "{Communications in computer and information science}",
  publisher = "Springer International Publishing",
  pages     = "366–374",
  year      =  2017,
  url       = "https://doi.org/10.1007%2F978-3-319-58750-9_51",
  file      = "All Papers/My Library/Parekh et al. 2017 - Eye contact detection via deep neural networks.pdf",
  doi       = "10.1007/978-3-319-58750-9\_51"
}

@INPROCEEDINGS{Ciechanowski2018-qr,
  title     = "{The Necessity of New Paradigms in Measuring Human-Chatbot
               Interaction}",
  author    = "Ciechanowski, Leon and Przegalinska, Aleksandra and Wegner,
               Krzysztof",
  editor    = "Hoffman, Mark",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "205--214",
  abstract  = "Our research is carried out in the context of the ongoing process
               of introducing artificial intelligence in the area of social
               interaction with people, with a particular emphasis on
               interactions in the professional sphere. In this paper, we
               provide an overview of methods used so far in researching
               human-bot interaction. We describe the methodology behind our
               experiment using electromyography as well as other
               psychophysiological data and a detailed set of questionnaires
               focused on assessing interactions and willingness to collaborate
               with a bot. Our purpose is to thoroughly examine the character of
               the human/non-human interaction process.",
  series    = "Advances in Intelligent Systems and Computing",
  year      =  2018,
  url       = "http://dx.doi.org/10.1007/978-3-319-60747-4_19",
  doi       = "10.1007/978-3-319-60747-4\_19",
  isbn      =  9783319607474,
  language  = "en"
}

@INCOLLECTION{Koesling2009-ok,
  title     = "{With a flick of the eye: Assessing gaze-controlled
               human-computer interaction}",
  author    = "Koesling, Hendrik and Zoellner, Martin and Sichelschmidt, Lorenz
               and Ritter, Helge",
  editor    = "Ritter, Helge and Sagerer, Gerhard and Dillmann, Rüdiger and
               Buss, Martin",
  booktitle = "{Human centered robot systems: Cognition, interaction,
               technology}",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "83–92",
  abstract  = "Gaze-controlled user interfaces appear to be a viable alternative
               to manual mouse control in human-computer interaction. Eye
               movements, however, often occur involuntarily and fixations do
               not necessarily indicate an intention to interact with a
               particular element of a visual display. To address this so-called
               Midas-touch problem, we investigated two methods of object/action
               selection using volitional eye movements, fixating versus
               blinking, and evaluated error rates, response times, response
               accuracy and user satisfaction in a text-typing task. Results
               show significantly less errors for the blinking method while task
               completion times do only vary between methods when practice is
               allowed. In that case, the fixation method is quicker than the
               blinking method. Also, participants rate the fixation method
               higher for its ease of use and regard it as less tiring. In
               general, blinking appears more suited for sparse and
               non-continuous input (e.g., when operating ticket vending
               machines), whereas fixating seems preferable for tasks requiring
               more rapid and continuous selections (e.g., when using virtual
               keyboards). We could demonstrate that the quality of the
               selection method does not rely on efficiency measures (e.g.,
               error rate or task completion time) alone: user satisfaction
               measures must certainly be taken into account as well to ensure
               user-friendly interfaces and, furthermore, gaze-controlled
               interaction methods must be adapted to specific applications.",
  year      =  2009,
  url       = "https://doi.org/10.1007/978-3-642-10403-9_9",
  doi       = "10.1007/978-3-642-10403-9\_9",
  isbn      =  9783642104039
}

@INPROCEEDINGS{Turner2013-wl,
  title     = "{Eye Pull, Eye Push: Moving Objects between Large Screens and
               Personal Devices with Gaze and Touch}",
  author    = "Turner, Jayson and Alexander, Jason and Bulling, Andreas and
               Schmidt, Dominik and Gellersen, Hans",
  editor    = "Kotzé, Paula and Marsden, Gary and Lindgaard, Gitte and Wesson,
               Janet and Winckler, Marco",
  booktitle = "{Human-Computer Interaction – INTERACT 2013}",
  publisher = "Springer",
  address   = "Berlin, Heidelberg",
  pages     = "170--186",
  abstract  = "Previous work has validated the eyes and mobile input as a viable
               approach for pointing at, and selecting out of reach objects.
               This work presents Eye Pull, Eye Push, a novel interaction
               concept for content transfer between public and personal devices
               using gaze and touch. We present three techniques that enable
               this interaction: Eye Cut \& Paste, Eye Drag \& Drop, and Eye
               Summon \& Cast. We outline and discuss several scenarios in which
               these techniques can be used. In a user study we found that
               participants responded well to the visual feedback provided by
               Eye Drag \& Drop during object movement. In contrast, we found
               that although Eye Summon \& Cast significantly improved
               performance, participants had difficulty coordinating their hands
               and eyes during interaction.",
  series    = "Lecture Notes in Computer Science",
  year      =  2013,
  url       = "http://dx.doi.org/10.1007/978-3-642-40480-1_11",
  file      = "All Papers/My Library/Turner et al. 2013 - Eye Pull, Eye Push - Moving Objects between Large Screens and Personal Devices with Gaze and Touch.pdf",
  doi       = "10.1007/978-3-642-40480-1\_11",
  isbn      =  9783642404801,
  language  = "en"
}

@INCOLLECTION{Heikkila2013-it,
  title     = "{Tools for a gaze-controlled drawing application -- comparing
               gaze gestures against dwell buttons}",
  author    = "Heikkilä, Henna",
  booktitle = "{{Human-Computer} Interaction -- {INTERACT} 2013}",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "187--201",
  abstract  = "We designed and implemented a gaze-controlled drawing application
               that utilizes modifiable and movable shapes. Moving and resizing
               tools were implemented with gaze gestures. Our gaze gestures are
               simple one-segment gestures that end outside the screen. Also, we
               use the closure of the eyes to stop actions in the drawing
               application. We carried out an experiment to compare gaze
               gestures with a dwell-based implementation of the tools. Results
               showed that, in terms of performance, gaze gestures were an
               equally good input …",
  series    = "Lecture notes in computer science",
  year      =  2013,
  url       = "http://dx.doi.org/10.1007/978-3-642-40480-1_12",
  file      = "All Papers/Other/Heikkilä 2013 - Tools for a gaze-controlled drawing application - comparing gaze gestures against dwell buttons.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1007/978-3-642-40480-1\_12",
  isbn      = "9783642404795,9783642404801",
  issn      = "0302-9743,1611-3349"
}

@INCOLLECTION{Heikkila2013-qq,
  title     = "{Tools for a gaze-controlled drawing application – comparing gaze
               gestures against dwell buttons}",
  author    = "Heikkilä, Henna",
  booktitle = "{Human-computer interaction – INTERACT 2013}",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "187–201",
  abstract  = "We designed and implemented a gaze-controlled drawing application
               that utilizes modifiable and movable shapes. Moving and resizing
               tools were implemented with gaze gestures. Our gaze gestures are
               simple one-segment gestures that end outside the screen. Also, we
               use the closure of the eyes to stop actions in the drawing
               application. We carried out an experiment to compare gaze
               gestures with a dwell-based implementation of the tools. Results
               showed that, in terms of performance, gaze gestures were an
               equally good input …",
  series    = "Lecture notes in computer science",
  year      =  2013,
  url       = "http://dx.doi.org/10.1007/978-3-642-40480-1_12",
  file      = "All Papers/My Library/Heikkilä 2013 - Tools for a gaze-controlled drawing application – comparing gaze gestures against dwell buttons.pdf",
  doi       = "10.1007/978-3-642-40480-1\_12",
  isbn      = "9783642404795,9783642404801"
}

@ARTICLE{Djajadiningrat2007-do,
  title   = "{Easy doesn't do it: skill and expression in tangible aesthetics}",
  author  = "Djajadiningrat, Tom and Matthews, Ben and Stienstra, Marcelle",
  journal = "Personal and Ubiquitous Computing",
  volume  =  11,
  number  =  8,
  pages   = "657–676",
  month   =  mar,
  year    =  2007,
  url     = "https://doi.org/10.1007%2Fs00779-006-0137-9",
  file    = "All Papers/My Library/Djajadiningrat et al. 2007 - Easy doesn't do it - skill and expression in tangible aesthetics.pdf",
  doi     = "10.1007/s00779-006-0137-9",
  issn    = "0949-2054"
}

@ARTICLE{Basch2021-vm,
  title     = "{It Takes More Than a Good Camera: Which Factors Contribute to
               Differences Between {Face-to-Face} Interviews and Videoconference
               Interviews Regarding Performance Ratings and Interviewee
               Perceptions?}",
  author    = "Basch, Johannes M and Melchers, Klaus G and Kurz, Anja and
               Krieger, Maya and Miller, Linda",
  journal   = "Journal of business and psychology",
  publisher = "Springer",
  volume    =  36,
  number    =  5,
  pages     = "921--940",
  abstract  = "Due to technological progress, videoconference interviews have
               become more and more common in personnel selection. Nevertheless,
               even in recent studies, interviewees received lower performance
               ratings in videoconference interviews than in face-to-face (FTF)
               interviews and interviewees held more negative perceptions of
               these interviews. However, the reasons for these differences are
               unclear. Therefore, we conducted an experiment with 114
               participants to compare FTF and videoconference interviews
               regarding interview performance and fairness perceptions and we
               investigated the role of social presence, eye contact, and
               impression management for these differences. As in other studies,
               ratings of interviewees' performance were lower in the
               videoconference interview. Differences in perceived social
               presence, perceived eye contact, and impression management
               contributed to these effects. Furthermore, live ratings of
               interviewees' performance were higher than ratings based on
               recordings. Additionally, videoconference interviews induced more
               privacy concerns but were perceived as more flexible.
               Organizations should take the present results into account and
               should not use both types of interviews in the same selection
               stage.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/s10869-020-09714-3",
  file      = "All Papers/Other/Basch et al. 2021 - It Takes More Than a Good Camera - Which Factors Con ... Interviews Regarding Performance Ratings and Interviewee Perceptions.pdf",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1007/s10869-020-09714-3",
  issn      = "0889-3268,1573-353X"
}

@ARTICLE{Basch2020-vu,
  title   = "{It takes more than a good camera: Which factors contribute to
             differences between face-to-face interviews and videoconference
             interviews regarding performance ratings and interviewee
             perceptions?}",
  author  = "Basch, Johannes M and Melchers, Klaus G and Kurz, Anja and Krieger,
             Maya and Miller, Linda",
  journal = "Journal of business and psychology",
  volume  =  36,
  number  =  5,
  pages   = "921–940",
  month   =  sep,
  year    =  2020,
  url     = "https://doi.org/10.1007%2Fs10869-020-09714-3",
  file    = "All Papers/My Library/Basch et al. 2020 - It takes more than a good camera - Which factors con ... interviews regarding performance ratings and interviewee perceptions.pdf",
  doi     = "10.1007/s10869-020-09714-3",
  issn    = "0889-3268"
}

@ARTICLE{Jongerius2020-eg,
  title     = "{The Measurement of Eye Contact in Human Interactions: A Scoping
               Review}",
  author    = "Jongerius, Chiara and Hessels, Roy S and Romijn, Johannes A and
               Smets, Ellen M A and Hillen, Marij A",
  journal   = "Journal of nonverbal behavior",
  publisher = "Springer",
  volume    =  44,
  number    =  3,
  pages     = "363--389",
  abstract  = "Eye contact is a fundamental aspect of nonverbal communication
               and therefore important for understanding human interaction. Eye
               contact has been the subject of research in many disciplines,
               including communication sciences, social psychology, and
               psychiatry, and a variety of techniques have been used to measure
               it. The choice of measurement method has consequences for
               research outcomes and their interpretation. To ensure that
               research findings align with study aims and populations, it is
               essential that methodological choices are well substantiated.
               Therefore, to enhance the effective examination of eye contact,
               we performed a literature review of the methods used to study eye
               contact. We searched Medline, PsycINFO and Web of Science for
               empirical peer-reviewed articles published in English that
               described quantitative studies on human eye contact and included
               a methodological description. The identified studies (N = 109)
               used two approaches to assess eye contact: direct, i.e.,
               assessing eye contact while it is occurring, and indirect, i.e.,
               assessing eye contact retrospectively (e.g., from video
               recordings). Within these categories, eight specific techniques
               were distinguished. Variation was found regarding the reciprocity
               of eye contact between two individuals, the involvement of an
               assessor and the behavior of participants while being studied.
               Measures not involving the interactors in assessment of eye
               contact and have a higher spatial and temporal resolution, such
               as eye tracking, have gained popularity. Our results show wide
               methodological diversity regarding the measurement of eye
               contact. Although studies often define eye contact as gaze
               towards an exact location, this may not do justice to the
               subjective character of eye contact. The various methodologies
               have hardly ever been compared, limiting the ability to compare
               findings between studies. Future studies should take notice of
               the controversy surrounding eye contact measures.",
  month     =  sep,
  year      =  2020,
  url       = "http://dx.doi.org/10.1007/s10919-020-00333-3",
  file      = "All Papers/Other/Jongerius et al. 2020 - The Measurement of Eye Contact in Human Interactions - A Scoping Review.pdf",
  doi       = "10.1007/s10919-020-00333-3",
  issn      = "0191-5886,1573-3653"
}

@ARTICLE{Jongerius2020-hg,
  title    = "{The measurement of eye contact in human interactions: A scoping
              review}",
  author   = "Jongerius, Chiara and Hessels, Roy S and Romijn, Johannes A and
              Smets, Ellen M A and Hillen, Marij A",
  journal  = "Journal of nonverbal behavior",
  volume   =  44,
  number   =  3,
  pages    = "363–389",
  abstract = "Eye contact is a fundamental aspect of nonverbal communication and
              therefore important for understanding human interaction. Eye
              contact has been the subject of research in many disciplines,
              including communication sciences, social psychology, and
              psychiatry, and a variety of techniques have been used to measure
              it. The choice of measurement method has consequences for research
              outcomes and their interpretation. To ensure that research
              findings align with study aims and populations, it is essential
              that methodological choices are well substantiated. Therefore, to
              enhance the effective examination of eye contact, we performed a
              literature review of the methods used to study eye contact. We
              searched Medline, PsycINFO and Web of Science for empirical
              peer-reviewed articles published in English that described
              quantitative studies on human eye contact and included a
              methodological description. The identified studies (N = 109) used
              two approaches to assess eye contact: direct, i.e., assessing eye
              contact while it is occurring, and indirect, i.e., assessing eye
              contact retrospectively (e.g., from video recordings). Within
              these categories, eight specific techniques were distinguished.
              Variation was found regarding the reciprocity of eye contact
              between two individuals, the involvement of an assessor and the
              behavior of participants while being studied. Measures not
              involving the interactors in assessment of eye contact and have a
              higher spatial and temporal resolution, such as eye tracking, have
              gained popularity. Our results show wide methodological diversity
              regarding the measurement of eye contact. Although studies often
              define eye contact as gaze towards an exact location, this may not
              do justice to the subjective character of eye contact. The various
              methodologies have hardly ever been compared, limiting the ability
              to compare findings between studies. Future studies should take
              notice of the controversy surrounding eye contact measures.",
  month    =  sep,
  year     =  2020,
  url      = "http://dx.doi.org/10.1007/s10919-020-00333-3",
  file     = "All Papers/My Library/Jongerius et al. 2020 - The measurement of eye contact in human interactions - A scoping review.pdf",
  doi      = "10.1007/s10919-020-00333-3",
  issn     = "0191-5886,1573-3653"
}

@ARTICLE{Kobayashi2021-ja,
  title    = "{Transmission of correct gaze direction in video conferencing
              using screen-embedded cameras}",
  author   = "Kobayashi, Kazuki and Komuro, Takashi and Kagawa, Keiichiro and
              Kawahito, Shoji",
  journal  = "Multimedia tools and applications",
  volume   =  80,
  number   =  21,
  pages    = "31509--31526",
  abstract = "In this paper, we propose a new video conferencing system that
              presents correct gaze directions of a remote user by switching
              among images obtained from multiple cameras embedded in a screen
              according to a local user's position. Our proposed method
              reproduces a situation like that in which the remote user is in
              the same space as the local user. The position of the remote user
              to be displayed on the screen is determined so that the positional
              relationship between the users is reproduced. The system selects
              one of the embedded cameras whose viewing direction towards the
              remote user is the closest to the local user's viewing direction
              to the remote user's image on the screen. As a result of
              quantitative evaluation, we confirmed that, in comparison with the
              case using a single camera, the accuracy of gaze estimation was
              improved by switching among the cameras according to the position
              of the local user.",
  month    =  sep,
  year     =  2021,
  url      = "http://dx.doi.org/10.1007/s11042-020-09758-w",
  file     = "All Papers/Other/Kobayashi et al. 2021 - Transmission of correct gaze direction in video conferencing using screen-embedded cameras.pdf",
  doi      = "10.1007/s11042-020-09758-w",
  issn     = "1380-7501"
}

@ARTICLE{Kobayashi2021-oa,
  title     = "{Transmission of correct gaze direction in video conferencing
               using screen-embedded cameras}",
  author    = "Kobayashi, Kazuki and Komuro, Takashi and Kagawa, Keiichiro and
               Kawahito, Shoji",
  journal   = "Multimedia tools and applications",
  publisher = "Springer Science and Business Media LLC",
  volume    =  80,
  number    = "21-23",
  pages     = "31509--31526",
  abstract  = "A new video conferencing system that presents correct gaze
               directions of a remote user by switching among images obtained
               from multiple cameras embedded in a screen according to a local
               user's position is proposed. In this paper, we propose a new
               video conferencing system that presents correct gaze directions
               of a remote user by switching among images obtained from multiple
               cameras embedded in a screen according to a local user's
               position. Our proposed method reproduces a situation like that in
               which the remote user is in the same space as the local user. The
               position of the remote user to be displayed on the screen is
               determined so that the positional relationship between the users
               is reproduced. The system selects one of the embedded cameras
               whose viewing direction towards the remote user is the closest to
               the local user's viewing direction to the remote user's image on
               the screen. As a result of quantitative evaluation, we confirmed
               that, in comparison with the case using a single camera, the
               accuracy of gaze estimation was improved by switching among the
               cameras according to the position of the local user.",
  month     =  sep,
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/s11042-020-09758-w",
  file      = "All Papers/Other/Kobayashi et al. 2021 - Transmission of correct gaze direction in video conferencing using screen-embedded cameras.pdf",
  doi       = "10.1007/s11042-020-09758-w",
  issn      = "1380-7501,1573-7721",
  language  = "en"
}

@ARTICLE{Kobayashi2021-yj,
  title    = "{Transmission of correct gaze direction in video conferencing
              using screen-embedded cameras}",
  author   = "Kobayashi, Kazuki and Komuro, Takashi and Kagawa, Keiichiro and
              Kawahito, Shoji",
  journal  = "Multimedia tools and applications",
  volume   =  80,
  number   = "21-23",
  pages    = "31509–31526",
  abstract = "A new video conferencing system that presents correct gaze
              directions of a remote user by switching among images obtained
              from multiple cameras embedded in a screen according to a local
              user's position is proposed. In this paper, we propose a new video
              conferencing system that presents correct gaze directions of a
              remote user by switching among images obtained from multiple
              cameras embedded in a screen according to a local user's position.
              Our proposed method reproduces a situation like that in which the
              remote user is in the same space as the local user. The position
              of the remote user to be displayed on the screen is determined so
              that the positional relationship between the users is reproduced.
              The system selects one of the embedded cameras whose viewing
              direction towards the remote user is the closest to the local
              user's viewing direction to the remote user's image on the screen.
              As a result of quantitative evaluation, we confirmed that, in
              comparison with the case using a single camera, the accuracy of
              gaze estimation was improved by switching among the cameras
              according to the position of the local user.",
  month    =  sep,
  year     =  2021,
  url      = "http://dx.doi.org/10.1007/s11042-020-09758-w",
  file     = "All Papers/My Library/Kobayashi et al. 2021 - Transmission of correct gaze direction in video conferencing using screen-embedded cameras.pdf",
  doi      = "10.1007/s11042-020-09758-w",
  issn     = "1380-7501,1573-7721",
  language = "en"
}

@ARTICLE{Lee2011-rg,
  title    = "{Internet Communication Versus Face-to-face Interaction in Quality
              of Life}",
  author   = "Lee, Paul S N and Leung, Louis and Lo, Venhwei and Xiong, Chengyu
              and Wu, Tingjun",
  journal  = "Social indicators research",
  volume   =  100,
  number   =  3,
  pages    = "375--389",
  abstract = "This study seeks to understand the role of the Internet in quality
              of life (QoL). Specifically, it examines the question of whether
              Internet communication serves, like face-to-face interactions, to
              enhance quality of life. It is hypothesized that the use of the
              Internet for interpersonal communication can improve quality of
              life among Internet users, just like face-to-face communication in
              everyday life. Sample survey data were collected in four Chinese
              cities, namely Hong Kong, Taipei, Beijing, and Wuhan, to serve as
              replicates to test the hypothesis. The Satisfaction with Life
              Scale (SWLS) of Diener (1984) was used to measure quality of life
              in the four cities. It was found that contrary to our expectation,
              Internet communication cannot predict quality of life while
              face-to-face communication with friends and family members can.
              The result was the same across the four Chinese cities. Possible
              reasons for this finding are examined and discussed.",
  month    =  feb,
  year     =  2011,
  url      = "http://dx.doi.org/10.1007/s11205-010-9618-3",
  file     = "All Papers/Other/Lee et al. 2011 - Internet Communication Versus Face-to-face Interaction in Quality of Life.pdf",
  keywords = "prj-gaze-design;uist2022-gaze-design",
  doi      = "10.1007/s11205-010-9618-3",
  issn     = "0303-8300,1573-0921"
}

@ARTICLE{Lee2011-iz,
  title    = "{Internet communication versus face-to-face interaction in quality
              of life}",
  author   = "Lee, Paul S N and Leung, Louis and Lo, Venhwei and Xiong, Chengyu
              and Wu, Tingjun",
  journal  = "Social indicators research",
  volume   =  100,
  number   =  3,
  pages    = "375–389",
  abstract = "This study seeks to understand the role of the Internet in quality
              of life (QoL). Specifically, it examines the question of whether
              Internet communication serves, like face-to-face interactions, to
              enhance quality of life. It is hypothesized that the use of the
              Internet for interpersonal communication can improve quality of
              life among Internet users, just like face-to-face communication in
              everyday life. Sample survey data were collected in four Chinese
              cities, namely Hong Kong, Taipei, Beijing, and Wuhan, to serve as
              replicates to test the hypothesis. The Satisfaction with Life
              Scale (SWLS) of Diener (1984) was used to measure quality of life
              in the four cities. It was found that contrary to our expectation,
              Internet communication cannot predict quality of life while
              face-to-face communication with friends and family members can.
              The result was the same across the four Chinese cities. Possible
              reasons for this finding are examined and discussed.",
  month    =  feb,
  year     =  2011,
  url      = "http://dx.doi.org/10.1007/s11205-010-9618-3",
  file     = "All Papers/My Library/Lee et al. 2011 - Internet communication versus face-to-face interaction in quality of life.pdf",
  doi      = "10.1007/s11205-010-9618-3",
  issn     = "0303-8300,1573-0921"
}

@ARTICLE{Benligiray2019-sc,
  title    = "{{SliceType}: fast gaze typing with a merging keyboard}",
  author   = "Benligiray, Burak and Topal, Cihan and Akinlar, Cuneyt",
  journal  = "Journal on Multimodal User Interfaces",
  volume   =  13,
  number   =  4,
  pages    = "321--334",
  abstract = "Jitter is an inevitable by-product of gaze detection. Because of
              this, gaze typing tends to be a slow and frustrating process. In
              this paper, we propose SliceType, a soft keyboard that is
              optimized for gaze input. Our main design objective is to use the
              screen area more efficiently by allocating a larger area to the
              target keys. We achieve this by determining the keys that will not
              be used for the next input, and allocating their space to the
              adjacent keys with a merging animation. Larger keys are faster to
              navigate towards, and easy to dwell on in the presence of eye
              tracking jitter. As a result, the user types faster and more
              comfortably. In addition, we employ a word completion scheme that
              complements gaze typing mechanics. A character and a related
              prediction is displayed at each key. Dwelling at a key enters the
              character, and double-dwelling enters the prediction. While
              dwelling on a key to enter a character, the user reads the related
              prediction effortlessly. The improvements provided by these
              features are quantified using the Fitts' law. The performance of
              the proposed keyboard is compared with two other soft keyboards
              designed for gaze typing, Dasher and GazeTalk. Thirty seven novice
              users gaze-typed a piece of text using all three keyboards. The
              results of the experiment show that the proposed keyboard allows
              faster typing, and is more preferred by the users.",
  month    =  dec,
  year     =  2019,
  url      = "http://dx.doi.org/10.1007/s12193-018-0285-z",
  file     = "All Papers/Other/Benligiray et al. 2019 - SliceType - fast gaze typing with a merging keyboard.pdf",
  keywords = "prj-gaze-shorthand",
  doi      = "10.1007/s12193-018-0285-z",
  issn     = "1783-8738"
}

@ARTICLE{Benligiray2018-lu,
  title   = "{SliceType: fast gaze typing with a merging keyboard}",
  author  = "Benligiray, Burak and Topal, Cihan and Akinlar, Cuneyt",
  journal = "J Multimodal User Interfaces",
  volume  =  13,
  number  =  4,
  pages   = "321–334",
  month   =  dec,
  year    =  2018,
  url     = "https://doi.org/10.1007%2Fs12193-018-0285-z",
  file    = "All Papers/My Library/Benligiray et al. 2018 - SliceType - fast gaze typing with a merging keyboard.pdf",
  doi     = "10.1007/s12193-018-0285-z"
}

@ARTICLE{Guo2021-ff,
  title     = "{Real-time face view correction for front-facing cameras}",
  author    = "Guo, Yudong and Zhang, Juyong and Chen, Yihua and Cai, Hongrui
               and Deng, Bailin",
  journal   = "Computational Visual Media",
  publisher = "Springer-Verlag",
  volume    =  7,
  number    =  4,
  pages     = "1--16",
  abstract  = "Face views are particularly important in person-to-person
               communication. Differenes between the camera location and the
               face orientation can result in undesirable facial appearances of
               the participants during video conferencing. This phenomenon is
               particularly noticeable when using devices where the front-facing
               camera is placed in unconventional locations such as below the
               display or within the keyboard. In this paper, we take a video
               stream from a single RGB camera as input, and generate a video
               stream that emulates the view from a virtual camera at a
               designated location. The most challenging issue in this problem
               is that the corrected view often needs out-of-plane head
               rotations. To address this challenge, we reconstruct the 3D face
               shape and re-render it into synthesized frames according to the
               virtual camera location. To output the corrected video stream
               with natural appearance in real time, we propose several novel
               techniques including accurate eyebrow reconstruction,
               high-quality blending between the corrected face image and
               background, and template-based 3D reconstruction of glasses. Our
               system works well for different lighting conditions and skin
               tones, and can handle users wearing glasses. Extensive
               experiments and user studies demonstrate that our method provides
               high-quality results.",
  month     =  apr,
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/s41095-021-0215-y",
  file      = "All Papers/Other/Guo et al. 2021 - Real-time face view correction for front-facing cameras.pdf",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1007/s41095-021-0215-y",
  issn      = "2096-0433"
}

@ARTICLE{Guo2021-vz,
  title    = "{Real-time face view correction for front-facing cameras}",
  author   = "Guo, Yudong and Zhang, Juyong and Chen, Yihua and Cai, Hongrui and
              Deng, Bailin",
  journal  = "Computational Visual Media",
  volume   =  7,
  number   =  4,
  pages    = "1–16",
  abstract = "Face views are particularly important in person-to-person
              communication. Differenes between the camera location and the face
              orientation can result in undesirable facial appearances of the
              participants during video conferencing. This phenomenon is
              particularly noticeable when using devices where the front-facing
              camera is placed in unconventional locations such as below the
              display or within the keyboard. In this paper, we take a video
              stream from a single RGB camera as input, and generate a video
              stream that emulates the view from a virtual camera at a
              designated location. The most challenging issue in this problem is
              that the corrected view often needs out-of-plane head rotations.
              To address this challenge, we reconstruct the 3D face shape and
              re-render it into synthesized frames according to the virtual
              camera location. To output the corrected video stream with natural
              appearance in real time, we propose several novel techniques
              including accurate eyebrow reconstruction, high-quality blending
              between the corrected face image and background, and
              template-based 3D reconstruction of glasses. Our system works well
              for different lighting conditions and skin tones, and can handle
              users wearing glasses. Extensive experiments and user studies
              demonstrate that our method provides high-quality results.",
  month    =  apr,
  year     =  2021,
  url      = "http://dx.doi.org/10.1007/s41095-021-0215-y",
  file     = "All Papers/My Library/Guo et al. 2021 - Real-time face view correction for front-facing cameras.pdf",
  doi      = "10.1007/s41095-021-0215-y",
  issn     = "2096-0433"
}

@ARTICLE{OMalley1996-zx,
  title     = "{Comparison of face-to-face and video-mediated interaction}",
  author    = "O'Malley, Claire and Langton, Steve and Anderson, Anne and
               Doherty-Sneddon, Gwyneth and Bruce, Vicki",
  journal   = "Interacting with computers",
  publisher = "Oxford University Press (OUP)",
  volume    =  8,
  number    =  2,
  pages     = "177--192",
  abstract  = "Abstract. A series of experiments are reported in which pairs of
               subjects performed a collaborative task remotely and communicated
               either via video and audio li",
  month     =  jun,
  year      =  1996,
  url       = "http://dx.doi.org/10.1016/0953-5438(96)01027-2",
  doi       = "10.1016/0953-5438(96)01027-2",
  issn      = "0953-5438,1873-7951",
  language  = "en"
}

@ARTICLE{OMalley1996-rz,
  title    = "{Comparison of face-to-face and video-mediated interaction}",
  author   = "O'Malley, Claire and Langton, Steve and Anderson, Anne and
              Doherty-Sneddon, Gwyneth and Bruce, Vicki",
  journal  = "Interacting with computers",
  volume   =  8,
  number   =  2,
  pages    = "177–192",
  abstract = "Abstract. A series of experiments are reported in which pairs of
              subjects performed a collaborative task remotely and communicated
              either via video and audio li",
  month    =  jun,
  year     =  1996,
  url      = "http://dx.doi.org/10.1016/0953-5438(96)01027-2",
  file     = "All Papers/My Library/O'Malley et al. 1996 - Comparison of face-to-face and video-mediated interaction.pdf",
  doi      = "10.1016/0953-5438(96)01027-2",
  issn     = "0953-5438,1873-7951",
  language = "en"
}

@ARTICLE{Vickers2009-zc,
  title    = "{Advances in coupling perception and action: the quiet eye as a
              bidirectional link between gaze, attention, and action}",
  author   = "Vickers, Joan N",
  journal  = "Progress in brain research",
  volume   =  174,
  pages    = "279–288",
  abstract = "One of the most elusive mysteries in psychology is
              perception-action coupling and the extent vision for perception is
              distinct from vision for action. In this chapter, I explore
              research on the control of the gaze during well-known sport tasks
              (vision for action) and the bidirectional link between perceptual
              and cognitive processes and optimal/nonoptimal motor performance.
              Considerable evidence now exists showing that specific gaze
              characteristics underlie higher levels of sport performance. The
              quiet eye has emerged as a characteristic of higher levels of
              performance and is the final fixation or tracking gaze that occurs
              prior to the final movement. Cognitive and ecological accounts of
              the quiet eye are presented and current controversies and future
              directions explored.",
  year     =  2009,
  url      = "http://dx.doi.org/10.1016/S0079-6123(09)01322-3",
  doi      = "10.1016/S0079-6123(09)01322-3",
  issn     = "0079-6123,1875-7855",
  language = "en"
}

@ARTICLE{Sato2022-or,
  title    = "{Development of an Eye-Tracking Image Manipulation System for
              Angiography: A Comparative Study}",
  author   = "Sato, Mitsuru and Takahashi, Minoru and Hoshino, Hiromitsu and
              Terashita, Takayoshi and Hayashi, Norio and Watanabe, Haruyuki and
              Ogura, Toshihiro",
  journal  = "Academic radiology",
  volume   =  29,
  number   =  8,
  pages    = "1196--1205",
  abstract = "Rationale and Objectives Appropriate image manipulation of
              angiographic image display systems during interventional radiology
              is performed by radiological technologists and/or nurses given
              instructions from radiologists. However, appropriate images might
              not be displayed because of communication errors. Therefore, we
              developed a manipulation system that uses an eye tracker. The
              study aimed to determine if an angiographic image display system
              can be manipulated as well by using an eye tracker as by using a
              mouse. Materials and Methods An angiographic image display system
              using an eye tracker to calculate the gaze position on the screen
              and state of fixation was developed. Fourteen radiological
              technologists participated in an observer study by manipulating 10
              images for each of 5 typical cases frequently performed in
              angiography, such as renal tumor, cerebral aneurysm, liver tumor,
              uterine bleeding, and hypersplenism. We measured the time from the
              start to the end of manipulating a series of images required when
              using the eye tracker and the conventional mouse. In this study,
              the statistical processing was done using Excel and R and R
              studio. Results The average time required for all observers for
              completing all cases was significantly shorter when using the eye
              tracker than when using the mouse (10.4 ± 2.1 s and 16.9 ± 2.6 s,
              respectively; p< 0.001 by paired t test). Conclusion Radiologists
              were able to manipulate an angiographic image display system
              directly by using the newly developed eye tracker system without
              touching contact devices, such as a mouse or angiography console.
              Therefore, communication error could be avoided.",
  month    =  "1~" # aug,
  year     =  2022,
  url      = "https://www.sciencedirect.com/science/article/pii/S1076633220305882",
  doi      = "10.1016/j.acra.2020.09.027",
  issn     = "1076-6332"
}

@ARTICLE{Singh2020-kj,
  title    = "{Combining gaze and AI planning for online human intention
              recognition}",
  author   = "Singh, Ronal and Miller, Tim and Newn, Joshua and Velloso, Eduardo
              and Vetere, Frank and Sonenberg, Liz",
  journal  = "Artificial intelligence",
  volume   =  284,
  pages    =  103275,
  abstract = "Intention recognition is the process of using behavioural cues,
              such as deliberative actions, eye gaze, and gestures, to infer an
              agent's goals or future behaviour. In artificial intelligence, one
              approach for intention recognition is to use a model of possible
              behaviour to rate intentions as more likely if they are a better
              ‘fit’ to actions observed so far. In this paper, we draw from
              literature linking gaze and visual attention, and we propose a
              novel model of online human intention recognition that combines
              gaze and model-based AI planning to build probability
              distributions over a set of possible intentions. In
              human-behavioural experiments (n=40) involving a multi-player
              board game, we demonstrate that adding gaze-based priors to
              model-based intention recognition improved the accuracy of
              intention recognition by 22\% (p<0.05), determined those
              intentions ≈90 seconds earlier (p<0.05), and at no additional
              computational cost. We also demonstrate that, when evaluated in
              the presence of semi-rational or deceptive gaze behaviours, the
              proposed model is significantly more accurate (9\% improvement)
              (p<0.05) compared to a model-based or gaze only approaches. Our
              results indicate that the proposed model could be used to design
              novel human-agent interactions in cases when we are unsure whether
              a person is honest, deceitful, or semi-rational.",
  month    =  "1~" # jul,
  year     =  2020,
  url      = "https://www.sciencedirect.com/science/article/pii/S0004370218307628",
  doi      = "10.1016/j.artint.2020.103275",
  issn     = "0004-3702"
}

@ARTICLE{Kiilavuori2021-yw,
  title     = "{Making eye contact with a robot: Psychophysiological responses
               to eye contact with a human and with a humanoid robot}",
  author    = "Kiilavuori, Helena and Sariola, Veikko and Peltola, Mikko J and
               Hietanen, Jari K",
  journal   = "Biological psychology",
  publisher = "Elsevier BV",
  volume    =  158,
  number    =  107989,
  pages     =  107989,
  abstract  = "Previous research has shown that eye contact, in human-human
               interaction, elicits increased affective and attention related
               psychophysiological responses. In the present study, we
               investigated whether eye contact with a humanoid robot would
               elicit these responses. Participants were facing a humanoid robot
               (NAO) or a human partner, both physically present and looking at
               or away from the participant. The results showed that both in
               human-robot and human-human condition, eye contact versus averted
               gaze elicited greater skin conductance responses indexing
               autonomic arousal, greater facial zygomatic muscle responses (and
               smaller corrugator responses) associated with positive affect,
               and greater heart deceleration responses indexing attention
               allocation. With regard to the skin conductance and zygomatic
               responses, the human model's gaze direction had a greater effect
               on the responses as compared to the robot's gaze direction. In
               conclusion, eye contact elicits automatic affective and
               attentional reactions both when shared with a humanoid robot and
               with another human.",
  month     =  jan,
  year      =  2021,
  url       = "https://consensus.apphttps://consensus.app/papers/making-psychophysiological-responses-contact-humanoid-kiilavuori/7779f539891659bfbd7c2ff2aacc4d5f/?q=eye+contact+with+AI&copilot=on&lang=en",
  keywords  = "Electromyography; Heart rate; Mutual gaze; Skin conductance;
               Social robot",
  doi       = "10.1016/j.biopsycho.2020.107989",
  pmid      =  33217486,
  issn      = "0301-0511,1873-6246",
  language  = "en"
}

@ARTICLE{Vriends2017-wg,
  title     = "{How do {I} look? Self-focused attention during a video chat of
               women with social anxiety (disorder)}",
  author    = "Vriends, Noortje and Meral, Yasemin and Bargas-Avila, Javier A
               and Stadler, Christina and Bögels, Susan M",
  journal   = "Behaviour research and therapy",
  publisher = "Elsevier",
  volume    =  92,
  pages     = "77--86",
  abstract  = "We investigated the role of self-focused attention (SFA) in
               social anxiety (disorder) in an ecologically valid way. In
               Experiment 1 high (n = 26) versus low (n = 25) socially anxious
               single women between 18 and 30 years had a video (``Skype'')
               conversation with an attractive male confederate, while seeing
               themselves and the confederate on-screen. The conversation was
               divided in four phases: (I) warm-up, (II) positive (confederate
               was friendly to the participant), (III) critical (confederate was
               critical to the participant), and (IV) active (participant was
               instructed to ask questions to the confederate). Participant's
               SFA was measured by eye-tracked gaze duration at their own image
               relative to the confederates' video image and other places at the
               computer screen. Results show that high socially anxious
               participants were more self-focused in the critical phase, but
               less self-focused in the active phase than low socially anxious
               participants. In Experiment 2 women diagnosed with SAD (n = 32)
               and controls (n = 30) between 18 and 30 years conducted the same
               experiment. Compared to controls participants with SAD showed
               increased SFA across all four phases of the conversation, and SFA
               predicted increased self-rated anxiety during the conversation.
               In conclusion, in subclinical social anxiety SFA is high only
               when the interaction partner is critical, whereas instructions to
               ask questions to the confederate reduces subclinical socially
               anxious' SFA, while clinical SAD is characterized by heightened
               self-focused attention throughout the interaction. Results
               support theories that social anxiety disorder is maintained by
               SFA, and imply that interventions that lower SFA may help prevent
               and treat social anxiety disorder, but that SFA can also be
               adaptive in certain types of interaction, such as when receiving
               compliments.",
  month     =  may,
  year      =  2017,
  url       = "http://dx.doi.org/10.1016/j.brat.2017.02.008",
  keywords  = "Arousal; Cognitive model; Eye-tracking; Self-focused attention;
               Social anxiety disorder",
  doi       = "10.1016/j.brat.2017.02.008",
  pmid      =  28284146,
  issn      = "0005-7967,1873-622X",
  language  = "en"
}

@ARTICLE{Vriends2017-no,
  title    = "{How do I look? Self-focused attention during a video chat of
              women with social anxiety (disorder)}",
  author   = "Vriends, Noortje and Meral, Yasemin and Bargas-Avila, Javier A and
              Stadler, Christina and Bögels, Susan M",
  journal  = "Behaviour research and therapy",
  volume   =  92,
  pages    = "77–86",
  abstract = "We investigated the role of self-focused attention (SFA) in social
              anxiety (disorder) in an ecologically valid way. In Experiment 1
              high (n = 26) versus low (n = 25) socially anxious single women
              between 18 and 30 years had a video (“Skype”) conversation with an
              attractive male confederate, while seeing themselves and the
              confederate on-screen. The conversation was divided in four
              phases: (I) warm-up, (II) positive (confederate was friendly to
              the participant), (III) critical (confederate was critical to the
              participant), and (IV) active (participant was instructed to ask
              questions to the confederate). Participant's SFA was measured by
              eye-tracked gaze duration at their own image relative to the
              confederates' video image and other places at the computer screen.
              Results show that high socially anxious participants were more
              self-focused in the critical phase, but less self-focused in the
              active phase than low socially anxious participants. In Experiment
              2 women diagnosed with SAD (n = 32) and controls (n = 30) between
              18 and 30 years conducted the same experiment. Compared to
              controls participants with SAD showed increased SFA across all
              four phases of the conversation, and SFA predicted increased
              self-rated anxiety during the conversation. In conclusion, in
              subclinical social anxiety SFA is high only when the interaction
              partner is critical, whereas instructions to ask questions to the
              confederate reduces subclinical socially anxious' SFA, while
              clinical SAD is characterized by heightened self-focused attention
              throughout the interaction. Results support theories that social
              anxiety disorder is maintained by SFA, and imply that
              interventions that lower SFA may help prevent and treat social
              anxiety disorder, but that SFA can also be adaptive in certain
              types of interaction, such as when receiving compliments.",
  month    =  may,
  year     =  2017,
  url      = "http://dx.doi.org/10.1016/j.brat.2017.02.008",
  file     = "All Papers/My Library/Vriends et al. 2017 - How do I look - Self-focused attention during a video chat of women with social anxiety (disorder).pdf",
  doi      = "10.1016/j.brat.2017.02.008",
  issn     = "0005-7967,1873-622X",
  language = "en"
}

@ARTICLE{Duchowski2018-lr,
  title    = "{Gaze-based interaction: A 30 year retrospective}",
  author   = "Duchowski, Andrew T",
  journal  = "Computers \& graphics",
  volume   =  73,
  pages    = "59–69",
  abstract = "Gaze-based interaction is reviewed, categorized within a taxonomy
              that splits interaction into four forms, namely diagnostic
              (off-line measurement), active (selection, look to shoot), passive
              (foveated rendering, a.k.a. gaze-contingent displays), and
              expressive (gaze synthesis). Diagnostic interaction is the
              mainstay of eye-tracked applications, including training or
              assessment of expertise, and is possibly the longest standing use
              of gaze due to its mainly offline requirements. Diagnostic
              analysis of gaze is still very much in demand, especially in
              training situations such as flight or surgery training. Active
              interaction is rooted in the desire to use the eyes to point and
              click, with gaze gestures growing in popularity. Passive
              interaction is the manipulation of scene elements in response to
              gaze direction, e.g., to improve frame rate. Expressive eye
              movement is drawn from its synthesis, which can make use of a
              procedural (stochastic) model of eye motion driven by
              goal-oriented tasks such as reading. In discussing each form of
              interaction, seminal results and recent advancements are reviewed,
              highlighting outstanding research problems. The survey paper
              extends an invited proceedings contribution to VS-Games 2017.",
  month    =  "6~" # jan,
  year     =  2018,
  url      = "https://www.sciencedirect.com/science/article/pii/S0097849318300487",
  doi      = "10.1016/j.cag.2018.04.002",
  issn     = "0097-8493"
}

@ARTICLE{Pfeuffer2021-xx,
  title    = "{ARtention: A design space for gaze-adaptive user interfaces in
              augmented reality}",
  author   = "Pfeuffer, Ken and Abdrabou, Yasmeen and Esteves, Augusto and Rivu,
              Radiah and Abdelrahman, Yomna and Meitner, Stefanie and Saadi, Amr
              and Alt, Florian",
  journal  = "Computers \& graphics",
  volume   =  95,
  pages    = "1--12",
  abstract = "Augmented Reality (AR) headsets extended with eye-tracking, a
              promising input technology for its natural and implicit nature,
              open a wide range of new interaction capabilities for everyday
              use. In this paper we present ARtention, a design space for gaze
              interaction specifically tailored for in-situ AR information
              interfaces. It highlights three important dimensions to consider
              in the UI design of such gaze-enabled applications: transitions
              from reality to the virtual interface, from single- to multi-layer
              content, and from information consumption to selection tasks. Such
              transitional aspects bring previously isolated gaze interaction
              concepts together to form a unified AR space, enabling more
              advanced application control seamlessly mediated by gaze. We
              describe these factors in detail. To illustrate how the design
              space can be used, we present three prototype applications and
              report informal user feedback obtained from different scenarios: a
              conversational UI, viewing a 3D visualization, and browsing items
              for shopping. We conclude with design considerations derived from
              our development and evaluation of the prototypes. We expect these
              to be valuable for researchers and designers investigating the use
              of gaze input in AR systems and applications.",
  month    =  "1~" # apr,
  year     =  2021,
  url      = "https://www.sciencedirect.com/science/article/pii/S0097849321000017",
  doi      = "10.1016/j.cag.2021.01.001",
  issn     = "0097-8493"
}

@ARTICLE{Schleich2017-gw,
  title    = "{Shaping the digital twin for design and production engineering}",
  author   = "Schleich, Benjamin and Anwer, Nabil and Mathieu, Luc and Wartzack,
              Sandro",
  journal  = "CIRP Annals",
  volume   =  66,
  number   =  1,
  pages    = "141--144",
  abstract = "The digitalization of manufacturing fuels the application of
              sophisticated virtual product models, which are referred to as
              digital twins, throughout all stages of product realization.
              Particularly, more realistic virtual models of manufactured
              products are essential to bridge the gap between design and
              manufacturing and to mirror the real and virtual worlds. In this
              paper, we propose a comprehensive reference model based on the
              concept of Skin Model Shapes, which serves as a digital twin of
              the physical product in design and manufacturing. In this regard,
              model conceptualization, representation, and implementation as
              well as applications along the product life-cycle are addressed.",
  month    =  jan,
  year     =  2017,
  url      = "http://dx.doi.org/10.1016/j.cirp.2017.04.040",
  file     = "All Papers/Other/Schleich et al. 2017 - Shaping the digital twin for design and production engineering.pdf",
  keywords = "Design; Tolerancing; Digital twin",
  doi      = "10.1016/j.cirp.2017.04.040",
  issn     = "0007-8506"
}

@ARTICLE{Schleich2017-op,
  title    = "{Shaping the digital twin for design and production engineering}",
  author   = "Schleich, Benjamin and Anwer, Nabil and Mathieu, Luc and Wartzack,
              Sandro",
  journal  = "CIRP Annals",
  volume   =  66,
  number   =  1,
  pages    = "141–144",
  abstract = "The digitalization of manufacturing fuels the application of
              sophisticated virtual product models, which are referred to as
              digital twins, throughout all stages of product realization.
              Particularly, more realistic virtual models of manufactured
              products are essential to bridge the gap between design and
              manufacturing and to mirror the real and virtual worlds. In this
              paper, we propose a comprehensive reference model based on the
              concept of Skin Model Shapes, which serves as a digital twin of
              the physical product in design and manufacturing. In this regard,
              model conceptualization, representation, and implementation as
              well as applications along the product life-cycle are addressed.",
  month    =  jan,
  year     =  2017,
  url      = "http://dx.doi.org/10.1016/j.cirp.2017.04.040",
  file     = "All Papers/My Library/Schleich et al. 2017 - Shaping the digital twin for design and production engineering.pdf",
  doi      = "10.1016/j.cirp.2017.04.040",
  issn     = "0007-8506"
}

@ARTICLE{Jones2020-up,
  title    = "{Characterising the Digital Twin: A systematic literature review}",
  author   = "Jones, David and Snider, Chris and Nassehi, Aydin and Yon, Jason
              and Hicks, Ben",
  journal  = "CIRP Journal of Manufacturing Science and Technology",
  volume   =  29,
  pages    = "36--52",
  abstract = "While there has been a recent growth of interest in the Digital
              Twin, a variety of definitions employed across industry and
              academia remain. There is a need to consolidate research such to
              maintain a common understanding of the topic and ensure future
              research efforts are to be based on solid foundations. Through a
              systematic literature review and a thematic analysis of 92 Digital
              Twin publications from the last ten years, this paper provides a
              characterisation of the Digital Twin, identification of gaps in
              knowledge, and required areas of future research. In
              characterising the Digital Twin, the state of the concept, key
              terminology, and associated processes are identified, discussed,
              and consolidated to produce 13 characteristics (Physical
              Entity/Twin; Virtual Entity/Twin; Physical Environment; Virtual
              Environment; State; Realisation; Metrology; Twinning; Twinning
              Rate; Physical-to-Virtual Connection/Twinning; Virtual-to-Physical
              Connection/Twinning; Physical Processes; and Virtual Processes)
              and a complete framework of the Digital Twin and its process of
              operation. Following this characterisation, seven knowledge gaps
              and topics for future research focus are identified: Perceived
              Benefits; Digital Twin across the Product Life-Cycle; Use-Cases;
              Technical Implementations; Levels of Fidelity; Data Ownership; and
              Integration between Virtual Entities; each of which are required
              to realise the Digital Twin.",
  month    =  may,
  year     =  2020,
  url      = "http://dx.doi.org/10.1016/j.cirpj.2020.02.002",
  file     = "All Papers/Other/Jones et al. 2020 - Characterising the Digital Twin - A systematic literature review.pdf",
  keywords = "Digital Twin; Virtual Twin",
  doi      = "10.1016/j.cirpj.2020.02.002",
  issn     = "1755-5817"
}

@ARTICLE{Jones2020-qw,
  title    = "{Characterising the digital twin: A systematic literature review}",
  author   = "Jones, David and Snider, Chris and Nassehi, Aydin and Yon, Jason
              and Hicks, Ben",
  journal  = "CIRP Journal of Manufacturing Science and Technology",
  volume   =  29,
  pages    = "36–52",
  abstract = "While there has been a recent growth of interest in the Digital
              Twin, a variety of definitions employed across industry and
              academia remain. There is a need to consolidate research such to
              maintain a common understanding of the topic and ensure future
              research efforts are to be based on solid foundations. Through a
              systematic literature review and a thematic analysis of 92 Digital
              Twin publications from the last ten years, this paper provides a
              characterisation of the Digital Twin, identification of gaps in
              knowledge, and required areas of future research. In
              characterising the Digital Twin, the state of the concept, key
              terminology, and associated processes are identified, discussed,
              and consolidated to produce 13 characteristics (Physical
              Entity/Twin; Virtual Entity/Twin; Physical Environment; Virtual
              Environment; State; Realisation; Metrology; Twinning; Twinning
              Rate; Physical-to-Virtual Connection/Twinning; Virtual-to-Physical
              Connection/Twinning; Physical Processes; and Virtual Processes)
              and a complete framework of the Digital Twin and its process of
              operation. Following this characterisation, seven knowledge gaps
              and topics for future research focus are identified: Perceived
              Benefits; Digital Twin across the Product Life-Cycle; Use-Cases;
              Technical Implementations; Levels of Fidelity; Data Ownership; and
              Integration between Virtual Entities; each of which are required
              to realise the Digital Twin.",
  month    =  may,
  year     =  2020,
  url      = "http://dx.doi.org/10.1016/j.cirpj.2020.02.002",
  file     = "All Papers/My Library/Jones et al. 2020 - Characterising the digital twin - A systematic literature review.pdf",
  doi      = "10.1016/j.cirpj.2020.02.002",
  issn     = "1755-5817"
}

@ARTICLE{Bohannon2013-xb,
  title     = "{Eye contact and video-mediated communication: A review}",
  author    = "Bohannon, Leanne S and Herbert, Andrew M and Pelz, Jeff B and
               Rantanen, Esa M",
  journal   = "Displays",
  publisher = "Elsevier",
  volume    =  34,
  number    =  2,
  pages     = "177--185",
  abstract  = "A relatively new form of human communication, video-conferencing
               has become more popular as video technology improves and with
               increasing demands for real-time communication across greater
               distances. The full effects of video-conferencing on human
               communication are still being explored. Video-conferencing is
               presumed to be a somewhat richer form of communication than email
               and telephone, but not quite as informative as face-to-face
               communication. This review explores research into the influence
               of eye contact on communication and how video-conferencing
               mediates both verbal and non-verbal interactions. Facilitation of
               eye contact is a challenge that must be addressed so that
               video-conferencing can approach the rich interactions of
               face-to-face communication.",
  month     =  apr,
  year      =  2013,
  url       = "http://dx.doi.org/10.1016/j.displa.2012.10.009",
  keywords  = "Video-conferencing; Eye contact; Communication; Eye-tracking;
               Display; Laptop",
  doi       = "10.1016/j.displa.2012.10.009",
  issn      = "0141-9382"
}

@ARTICLE{Bohannon2013-tt,
  title   = "{Eye contact and video-mediated communication: A review}",
  author  = "Bohannon, Leanne S and Herbert, Andrew M and Pelz, Jeff B and
             Rantanen, Esa M",
  journal = "Displays",
  volume  =  34,
  number  =  2,
  pages   = "177–185",
  month   =  apr,
  year    =  2013,
  url     = "https://doi.org/10.1016%2Fj.displa.2012.10.009",
  doi     = "10.1016/j.displa.2012.10.009",
  issn    = "0141-9382"
}

@ARTICLE{Jaklic2017-jw,
  title    = "{User interface for a better eye contact in videoconferencing}",
  author   = "Jaklič, Aleš and Solina, Franc and Šajn, Luka",
  journal  = "Displays",
  volume   =  46,
  pages    = "25--36",
  abstract = "When people talk to each other, eye contact is very important for
              a trustful and efficient communication. Video-conferencing systems
              were invented to enable such communication over large distances,
              recently using mostly Internet and personal computers. Despite low
              cost of such solutions, a broader acceptance and use of these
              communication means has not happened yet. One of the most
              important reasons for this situation is that it is almost
              impossible to establish eye contact between distant parties on the
              most common hardware configurations of such videoconferencing
              systems, where the camera for face capture is usually mounted
              above the computer monitor, where the face of the correspondent is
              observed. Different hardware and software solutions to this
              problem of missing eye contact have been proposed over the years.
              In this article we propose a simple solution that can improve the
              subjective feeling of eye contact, which is based on how people
              perceive 3D scenes displayed on slanted surfaces, and offer some
              experiments in support of the hypothesis.",
  month    =  jan,
  year     =  2017,
  url      = "http://dx.doi.org/10.1016/j.displa.2016.12.002",
  keywords = "Videoconferencing; Eye contact; Human-computer interface;eye
              contact;telepresence;prj-gaze-design;uist2022-gaze-design",
  doi      = "10.1016/j.displa.2016.12.002",
  issn     = "0141-9382"
}

@ARTICLE{Jaklic2017-dm,
  title    = "{User interface for a better eye contact in videoconferencing}",
  author   = "Jaklič, Aleš and Solina, Franc and Šajn, Luka",
  journal  = "Displays",
  volume   =  46,
  pages    = "25–36",
  abstract = "When people talk to each other, eye contact is very important for
              a trustful and efficient communication. Video-conferencing systems
              were invented to enable such communication over large distances,
              recently using mostly Internet and personal computers. Despite low
              cost of such solutions, a broader acceptance and use of these
              communication means has not happened yet. One of the most
              important reasons for this situation is that it is almost
              impossible to establish eye contact between distant parties on the
              most common hardware configurations of such videoconferencing
              systems, where the camera for face capture is usually mounted
              above the computer monitor, where the face of the correspondent is
              observed. Different hardware and software solutions to this
              problem of missing eye contact have been proposed over the years.
              In this article we propose a simple solution that can improve the
              subjective feeling of eye contact, which is based on how people
              perceive 3D scenes displayed on slanted surfaces, and offer some
              experiments in support of the hypothesis.",
  month    =  "1~" # jan,
  year     =  2017,
  url      = "https://www.sciencedirect.com/science/article/pii/S0141938216300944",
  doi      = "10.1016/j.displa.2016.12.002",
  issn     = "0141-9382"
}

@ARTICLE{Pathirana2022-qy,
  title    = "{Eye gaze estimation: A survey on deep learning-based approaches}",
  author   = "Pathirana, Primesh and Senarath, Shashimal and Meedeniya, Dulani
              and Jayarathna, Sampath",
  journal  = "Expert systems with applications",
  volume   =  199,
  pages    =  116894,
  abstract = "Human gaze estimation plays a major role in many applications in
              human–computer interaction and computer vision by identifying the
              users' point-of-interest. Revolutionary developments of deep
              learning have captured significant attention in gaze estimation
              literature. Gaze estimation techniques have progressed from
              single-user constrained environments to multi-user unconstrained
              environments with the applicability of deep learning techniques in
              complex unconstrained environments with extensive variations. This
              paper presents a comprehensive survey of the single-user and
              multi-user gaze estimation approaches with deep learning.
              State-of-the-art approaches are analyzed based on deep learning
              model architectures, coordinate systems, environmental
              constraints, datasets and performance evaluation metrics. A key
              outcome from this survey realizes the limitations, challenges and
              future directions of multi-user gaze estimation techniques.
              Furthermore, this paper serves as a reference point and a
              guideline for future multi-user gaze estimation research.",
  month    =  "8~" # jan,
  year     =  2022,
  url      = "https://www.sciencedirect.com/science/article/pii/S0957417422003347",
  doi      = "10.1016/j.eswa.2022.116894",
  issn     = "0957-4174"
}

@ARTICLE{Ens2019-bi,
  title    = "{Revisiting collaboration through mixed reality: The evolution of
              groupware}",
  author   = "Ens, Barrett and Lanir, Joel and Tang, Anthony and Bateman, Scott
              and Lee, Gun and Piumsomboon, Thammathip and Billinghurst, Mark",
  journal  = "International journal of human-computer studies",
  volume   =  131,
  pages    = "81--98",
  abstract = "Collaborative Mixed Reality (MR) systems are at a critical point
              in time as they are soon to become more commonplace. However, MR
              technology has only recently matured to the point where
              researchers can focus deeply on the nuances of supporting
              collaboration, rather than needing to focus on creating the
              enabling technology. In parallel, but largely independently, the
              field of Computer Supported Cooperative Work (CSCW) has focused on
              the fundamental concerns that underlie human communication and
              collaboration over the past 30-plus years. Since MR research is
              now on the brink of moving into the real world, we reflect on
              three decades of collaborative MR research and try to reconcile it
              with existing theory from CSCW, to help position MR researchers to
              pursue fruitful directions for their work. To do this, we review
              the history of collaborative MR systems, investigating how the
              common taxonomies and frameworks in CSCW and MR research can be
              applied to existing work on collaborative MR systems, exploring
              where they have fallen behind, and look for new ways to describe
              current trends. Through identifying emergent trends, we suggest
              future directions for MR, and also find where CSCW researchers can
              explore new theory that more fully represents the future of
              working, playing and being with others.",
  series   = "50 years of the International Journal of Human-Computer Studies.
              Reflections on the past, present and future of human-centred
              technologies",
  month    =  "1~" # nov,
  year     =  2019,
  url      = "https://www.sciencedirect.com/science/article/pii/S1071581919300606",
  file     = "All Papers/My Library/Ens et al. 2019 - Revisiting collaboration through mixed reality - The evolution of groupware.pdf",
  doi      = "10.1016/j.ijhcs.2019.05.011",
  issn     = "1071-5819"
}

@ARTICLE{Rajanna2022-at,
  title    = "{{PressTapFlick}: Exploring a gaze and foot-based multimodal
              approach to gaze typing}",
  author   = "Rajanna, Vijay and Russel, Murat and Zhao, Jeffrey and Hammond,
              Tracy",
  journal  = "International journal of human-computer studies",
  volume   =  161,
  pages    =  102787,
  abstract = "Text entry is extremely difficult or sometimes impossible in the
              scenarios of situationally-induced impairments and disabilities,
              and for individuals with motor impairments (physical impairments
              and disabilities) by birth or due to an injury. As a remedy, many
              rely on gaze typing with dwell-based selection as it allows for
              hands-free text entry. However, dwell-based gaze typing could be
              limited by usability issues, reduced typing speed, high error
              rate, steep learning curve, and visual fatigue with prolonged
              usage. Addressing these issues is crucial for improving the
              usability and performance of gaze typing. In our work, we present
              a dwell-free, multimodal approach to gaze typing where the gaze
              input is supplemented with a foot input modality. Our combined
              gaze and foot-based typing system comprises of an enhanced virtual
              QWERTY keyboard (VKB), and a footwear augmenting wearable device
              that provides the foot input. In this multi-modal setup, the user
              points her gaze at the desired character, and selects it with the
              foot input. We further investigated two approaches to foot-based
              selection, a foot gesture-based selection and a foot press-based
              selection, which are compared against the standard dwell-based
              selection. We evaluated our gaze typing system through a
              comparative study involving three experiments (51 participants),
              where each experiment used one of the three target selection
              methods, and had 17 participants in it. In the first experiment
              the participants used dwell-based selection, second, foot
              gesture-based selection, and third, foot press-based selection for
              gaze typing. We found that with dwell-based selection the highest
              mean typing speed of 11.65 WPM (max 14.83 WPM) was achieved when
              using a dwell time of 400 ms. Similarly, among foot-based
              selection methods the highest mean typing speed of 14.98 WPM (max
              18.18 WPM) was achieved with foot press-based selection.
              Furthermore, ANOVA tests revealed that the difference in the
              typing speeds between the three selection methods is significant,
              however, no significant difference was found in the error rate.
              Overall, based on both typing performance and qualitative feedback
              the results suggest that gaze and foot-based typing is convenient,
              easy to learn, and addresses the usability issues associated with
              dwell-based typing. Furthermore, toe tapping is the most preferred
              foot gesture of all the four gestures (toe tapping, heel tapping,
              right flick and left flick) we used in the study. Also, we found
              that when using foot-based selection users quickly develop a
              rhythm in focusing at a character with gaze and selecting it with
              the foot, and this familiarity reduces the errors significantly.
              We believe, our findings would encourage further research in
              leveraging a supplemental foot input in gaze typing, or in
              general, would assist in the development of rich foot-based
              interactions.",
  month    =  "1~" # may,
  year     =  2022,
  url      = "https://www.sciencedirect.com/science/article/pii/S1071581922000167",
  keywords = "Gaze typing; Multimodal interaction; Foot-based interaction;
              Virtual keyboard; Optikey; 3D printing; Microcontroller",
  doi      = "10.1016/j.ijhcs.2022.102787",
  issn     = "1071-5819"
}

@ARTICLE{Rajanna2022-op,
  title    = "{PressTapFlick: Exploring a gaze and foot-based multimodal
              approach to gaze typing}",
  author   = "Rajanna, Vijay and Russel, Murat and Zhao, Jeffrey and Hammond,
              Tracy",
  journal  = "International journal of human-computer studies",
  volume   =  161,
  pages    =  102787,
  abstract = "Text entry is extremely difficult or sometimes impossible in the
              scenarios of situationally-induced impairments and disabilities,
              and for individuals with motor impairments (physical impairments
              and disabilities) by birth or due to an injury. As a remedy, many
              rely on gaze typing with dwell-based selection as it allows for
              hands-free text entry. However, dwell-based gaze typing could be
              limited by usability issues, reduced typing speed, high error
              rate, steep learning curve, and visual fatigue with prolonged
              usage. Addressing these issues is crucial for improving the
              usability and performance of gaze typing. In our work, we present
              a dwell-free, multimodal approach to gaze typing where the gaze
              input is supplemented with a foot input modality. Our combined
              gaze and foot-based typing system comprises of an enhanced virtual
              QWERTY keyboard (VKB), and a footwear augmenting wearable device
              that provides the foot input. In this multi-modal setup, the user
              points her gaze at the desired character, and selects it with the
              foot input. We further investigated two approaches to foot-based
              selection, a foot gesture-based selection and a foot press-based
              selection, which are compared against the standard dwell-based
              selection. We evaluated our gaze typing system through a
              comparative study involving three experiments (51 participants),
              where each experiment used one of the three target selection
              methods, and had 17 participants in it. In the first experiment
              the participants used dwell-based selection, second, foot
              gesture-based selection, and third, foot press-based selection for
              gaze typing. We found that with dwell-based selection the highest
              mean typing speed of 11.65 WPM (max 14.83 WPM) was achieved when
              using a dwell time of 400 ms. Similarly, among foot-based
              selection methods the highest mean typing speed of 14.98 WPM (max
              18.18 WPM) was achieved with foot press-based selection.
              Furthermore, ANOVA tests revealed that the difference in the
              typing speeds between the three selection methods is significant,
              however, no significant difference was found in the error rate.
              Overall, based on both typing performance and qualitative feedback
              the results suggest that gaze and foot-based typing is convenient,
              easy to learn, and addresses the usability issues associated with
              dwell-based typing. Furthermore, toe tapping is the most preferred
              foot gesture of all the four gestures (toe tapping, heel tapping,
              right flick and left flick) we used in the study. Also, we found
              that when using foot-based selection users quickly develop a
              rhythm in focusing at a character with gaze and selecting it with
              the foot, and this familiarity reduces the errors significantly.
              We believe, our findings would encourage further research in
              leveraging a supplemental foot input in gaze typing, or in
              general, would assist in the development of rich foot-based
              interactions.",
  month    =  "5~" # jan,
  year     =  2022,
  url      = "https://www.sciencedirect.com/science/article/pii/S1071581922000167",
  doi      = "10.1016/j.ijhcs.2022.102787",
  issn     = "1071-5819"
}

@ARTICLE{Sharafi2015-ug,
  title     = "{A systematic literature review on the usage of eye-tracking in
               software engineering}",
  author    = "Sharafi, Zohreh and Soh, Zéphyrin and Guéhéneuc, Yann-Gaël",
  journal   = "Information and software technology",
  publisher = "Elsevier BV",
  volume    =  67,
  pages     = "79--107",
  abstract  = "Eye-tracking is a mean to collect evidence regarding some
               participants’ cognitive processes. Eye-trackers monitor
               participants’ visual attention by collecting eye-movement data.
               These data are useful to get insights into participants’
               cognitive processes during reasoning tasks.The Evidence-based
               Software Engineering (EBSE) paradigm has been proposed in 2004
               and, since then, has been used to provide detailed insights
               regarding different topics in software engineering research and
               practice. Systematic Literature Reviews (SLR) are also useful in
               the context of EBSE by bringing together all existing evidence of
               research and results about a particular topic. This SLR evaluates
               the current state of the art of using eye-trackers in software
               engineering and provides evidence on the uses and contributions
               of eye-trackers to empirical studies in software engineering.We
               perform a SLR covering eye-tracking studies in software
               engineering published from 1990 up to the end of 2014. To search
               all recognised resources, instead of applying manual search, we
               perform an extensive automated search using Engineering Village.
               We identify 36 relevant publications, including nine journal
               papers, two workshop papers, and 25 conference papers.The
               software engineering community started using eye-trackers in the
               1990s and they have become increasingly recognised as useful
               tools to conduct empirical studies from 2006. We observe that
               researchers use eye-trackers to study model comprehension, code
               comprehension, debugging, collaborative interaction, and
               traceability. Moreover, we find that studies use different
               metrics based on eye-movement data to obtain quantitative
               measures. We also report the limitations of current eye-tracking
               technology, which threaten the validity of previous studies,
               along with suggestions to mitigate these limitations.However, not
               withstanding these limitations and threats, we conclude that the
               advent of new eye-trackers makes the use of these tools easier
               and less obtrusive and that the software engineering community
               could benefit more from this technology.",
  month     =  nov,
  year      =  2015,
  url       = "https://consensus.apphttps://consensus.app/papers/literature-review-usage-eyetracking-software-sharafi/dff7e742394b50f9b7970449eb21d6ff/?extracted-answer=Eye-tracking+is+used+in+software+engineering+to+study+model+comprehension%2C+code+comprehension%2C+debugging%2C+collaborative+interaction%2C+and+traceability.&q=research+that+eye+tracking+for+estimate+eye+contact+occurance+on+videoconferencing&copilot=on",
  doi       = "10.1016/j.infsof.2015.06.008",
  issn      = "0950-5849,1873-6025",
  language  = "en"
}

@ARTICLE{Van_Reijmersdal2020-hp,
  title    = "{Effects of disclosing influencer marketing in videos: An eye
              tracking study among children in early adolescence}",
  author   = "Van Reijmersdal, Eva A and Rozendaal, Esther and Hudders, Liselot
              and Vanwesenbeeck, Ini and Cauberghe, Veroline and Van Berlo, Zeph
              M C",
  journal  = "Journal of Interactive Marketing",
  volume   =  49,
  number   =  1,
  pages    = "94–106",
  abstract = "This study focused on the effects of sponsorship disclosure timing
              on children's ability to understand that social influencer videos
              are sponsored. The study also investigated how sponsorship
              disclosure timing affects children's attitudes toward the
              sponsoring brand, the video, and the influencer. An experiment
              among 272 children in early adolescence (10?13 years of age) was
              conducted using eye tracking. Results show that a disclosure shown
              prior to the start of the videos leads to more visual attention
              than a disclosure shown concurrently with the start of videos.
              Consequently, disclosure prior to the start of videos is better
              processed, as indicated by disclosure memory, which then leads to
              a better understanding that the content is sponsored. This
              understanding evokes a more critical attitude toward the sponsored
              content in the video, and results in less positive attitudes
              toward the brands, the videos, and the influencers. Theoretically,
              this study provides insights into the mechanisms that explain
              disclosure timing effects among children in early adolescence.
              Practically, this study offers recommendations to policy makers to
              develop sponsorship disclosures that can increase transparency of
              online embedded advertising to minors.",
  month    =  "2~" # jan,
  year     =  2020,
  url      = "https://doi.org/10.1016/j.intmar.2019.09.001",
  file     = "All Papers/My Library/Van Reijmersdal et al. 2020 - Effects of disclosing influencer marketing in videos - An eye tracking study among children in early adolescence.pdf",
  doi      = "10.1016/j.intmar.2019.09.001",
  issn     = "1094-9968"
}

@ARTICLE{Kahn2008-qb,
  title    = "{A plasma display window?—The shifting baseline problem in a
              technologically mediated natural world}",
  author   = "Kahn, Peter H and Friedman, Batya and Gill, Brian and Hagman,
              Jennifer and Severson, Rachel L and Freier, Nathan G and Feldman,
              Erika N and Carrère, Sybil and Stolyar, Anna",
  journal  = "Journal of environmental psychology",
  volume   =  28,
  number   =  2,
  pages    = "192--199",
  abstract = "Humans will continue to adapt to an increasingly technological
              world. But are there costs to such adaptations in terms of human
              well being? Toward broaching this question, we investigated
              physiological effects of experiencing a HDTV quality real-time
              view of nature through a plasma display “window.” In an office
              setting, 90 participants (30 per group) were exposed either to (a)
              a glass window that afforded a view of a nature scene, (b) a
              plasma window that afforded a real-time HDTV view of essentially
              the same scene, or (c) a blank wall. Results showed that in terms
              of heart rate recovery from low-level stress the glass window was
              more restorative than a blank wall; in turn, a plasma window was
              no more restorative than a blank wall. Moreover, when participants
              spent more time looking at the glass window, their heart rate
              tended to decrease more rapidly; that was not the case with the
              plasma window. Discussion focuses on how the purported benefits of
              viewing nature may be attenuated by a digital medium.",
  month    =  "1~" # jun,
  year     =  2008,
  url      = "https://www.sciencedirect.com/science/article/pii/S027249440700093X",
  doi      = "10.1016/j.jenvp.2007.10.008",
  issn     = "0272-4944"
}

@ARTICLE{Qi2021-ns,
  title    = "{Enabling technologies and tools for digital twin}",
  author   = "Qi, Qinglin and Tao, Fei and Hu, Tianliang and Anwer, Nabil and
              Liu, Ang and Wei, Yongli and Wang, Lihui and Nee, A Y C",
  journal  = "Journal of Manufacturing Systems",
  volume   =  58,
  pages    = "3–21",
  abstract = "Digital twin is revolutionizing industry. Fired by sensor updates
              and history data, the sophisticated models can mirror almost every
              facet of a product, process or service. In the future, everything
              in the physical world would be replicated in the digital space
              through digital twin technology. As a cutting-edge technology,
              digital twin has received a lot of attention. However, digital
              twin is far from realizing their potential, which is a complex
              system and long-drawn process. Researchers must model all the
              different parts of the objects or systems. Varied types of data
              needed to be collected and merged. Many researchers and
              participators in engineering are not clear which technologies and
              tools should be used. 5-dimension digital twin model provides
              reference guidance for understanding and implementing digital
              twin. From the perspective of 5-dimension digital twin model, this
              paper tries to investigate and summarize the frequently-used
              enabling technologies and tools for digital twin to provide
              technologies and tools references for the applications of digital
              twin in the future.",
  month    =  "1~" # jan,
  year     =  2021,
  url      = "https://www.sciencedirect.com/science/article/pii/S027861251930086X",
  doi      = "10.1016/j.jmsy.2019.10.001",
  issn     = "0278-6125"
}

@ARTICLE{van-Gog2010-wd,
  title     = "{Eye tracking as a tool to study and enhance multimedia learning}",
  author    = "van Gog, Tamara and Scheiter, Katharina",
  journal   = "Learning and instruction",
  publisher = "Elsevier BV",
  volume    =  20,
  number    =  2,
  pages     = "95--99",
  abstract  = "This special issue comprises a set of six papers, in which
               studies are presented that use eye tracking to analyse multimedia
               learning processes in detail. Most of the papers focus on the
               effects on visual attention of animations with different design
               features such as spoken vs. written text, different kinds of
               cues, or different presentation speeds. Two contributions concern
               effects of learner characteristics (prior knowledge) on visual
               attention when learning with video and complex graphics. In
               addition, in some papers eye tracking is not only used as a
               process measure in itself, but also as input for verbal reports
               (i.e., cued retrospective reporting). In the two commentaries,
               the contributions are discussed from a multimedia learning
               perspective and an eye tracking perspective, by prominent
               researchers in those fields. Together, the contributions to this
               issue give an overview of the various possibilities eye tracking
               opens up for research on multimedia learning and instruction.",
  month     =  apr,
  year      =  2010,
  url       = "https://consensus.apphttps://consensus.app/papers/tracking-tool-study-enhance-multimedia-learning-gog/5103595e735950d6ab6567d7ba0df9eb/?extracted-answer=Eye+tracking+is+used+to+analyze+multimedia+learning+processes%2C+including+the+effects+on+visual+attention+of+animations+with+different+design+features+and+learner+characteristics.&q=research+that+eye+tracking+for+estimate+eye+contact+occurance+on+videoconferencing&copilot=on",
  doi       = "10.1016/j.learninstruc.2009.02.009",
  issn      = "0959-4752,1873-3263",
  language  = "en"
}

@ARTICLE{Kishore_Kumar2021-ab,
  title    = "{FPGA implementation of eye movement detection algorithm}",
  author   = "Kishore Kumar, Gundugonti and Narayanam, Balaji",
  journal  = "Microprocessors and microsystems",
  pages    =  104364,
  abstract = "In this paper, we propose an simple and efficient VLSI hardware
              architecture is implemented for eye movement detection. For Eye
              movement detection reading activity Electrooculography (EOG)
              signal is considered. Here, for denoising the noisy EOG signal
              efficient FIR filter and for decomposition of denoised EOG signal
              an efficient Haar wavelet transform architecture is used
              respectively. The modified VLSI hardware architecture method
              detected the saccade (left movement of eye and right movement of
              eye) and blink efficiently. The hardware architecture of the eye
              movement detection algorithm functionality is verified by using
              Xilinx System Generator hardware co-simulation tool. The eye
              movement detection algorithm is implemented on the ZedBoard FPGA
              using Xilinx Vivado design suite.",
  month    =  "11~" # nov,
  year     =  2021,
  url      = "https://www.sciencedirect.com/science/article/pii/S0141933121005172",
  file     = "All Papers/My Library/Kishore Kumar and Narayanam 2021 - FPGA implementation of eye movement detection algorithm.pdf",
  doi      = "10.1016/j.micpro.2021.104364",
  issn     = "0141-9331"
}

@ARTICLE{Despres2005-vt,
  title    = "{Spatial auditory compensation in early-blind humans: Involvement
              of eye movements and/or attention orienting?}",
  author   = "Després, O and Candas, V and Dufour, A",
  journal  = "Neuropsychologia",
  volume   =  43,
  number   =  13,
  pages    = "1955--1962",
  abstract = "Several studies have reported that the early-blind displays higher
              auditory spatial abilities than the sighted. Although many studies
              have attempted to delineate the cortical structures that undergo
              functional reorganization in blind people, few have tried to
              determine which auditory or non-auditory processes mediate these
              increased auditory spatial abilities. The aim of this paper is to
              investigate the role of eye movements and orientation of attention
              in auditory localization in blind humans. Although we found, in a
              first experiment, that the influence of eye movements on auditory
              spatial localization is preserved in spite of congenital visual
              deprivation, the saccade influence on spatial hearing is not more
              pronounced in the blind than in the sighted. In a second
              experiment, early-blind and sighted subjects undertook a task
              involving discrimination of sound elevation in which auditory
              targets followed uninformative auditory cues on either side with
              an intermediate elevation. When sounds were emitted from the
              frontal hemifield, both groups showed similar auditory
              localization performance. Although the auditory cue did not affect
              discrimination accuracy in both groups, early-blind subjects
              exhibited shorter reaction times than sighted subjects when sound
              sources were placed at far-lateral locations. Attentional cues,
              however, had similar effects on both groups of subjects,
              suggesting that improved auditory spatial abilities are not
              mediated by attention orienting mechanisms.",
  month    =  "1~" # jan,
  year     =  2005,
  url      = "https://www.sciencedirect.com/science/article/pii/S0028393205001259",
  doi      = "10.1016/j.neuropsychologia.2005.03.002",
  issn     = "0028-3932"
}

@ARTICLE{Liu2022-fk,
  title    = "{In the eye of the beholder: A survey of gaze tracking techniques}",
  author   = "Liu, Jiahui and Chi, Jiannan and Yang, Huijie and Yin, Xucheng",
  journal  = "Pattern recognition",
  volume   =  132,
  pages    =  108944,
  abstract = "Gaze tracking estimates and tracks the user's gaze by analyzing
              facial or eye features, it is an important way to realize
              automated vision-based interaction. This paper introduces the
              visual information used in gaze tracking, and discusses the
              commonly used gaze estimation methods and their research dynamics,
              including: 2D mapping-based methods, 3D model-based methods, and
              appearance-based methods. In this way, some key issues that need
              to be solved in these methods are considered, and their research
              trends are discussed. Their characteristics in system
              configuration, personal calibration, head motion, gaze accuracy
              and robustness are also compared. Finally, the applications of
              gaze tracking techniques are analyzed from various application
              factors and fields. This paper reviews the latest development of
              gaze tracking, focuses more on various gaze tracking algorithms
              and their existing challenges. The development trends of gaze
              tracking are prospected, which provides ideas for future
              theoretical research and practical applications.",
  month    =  "12~" # jan,
  year     =  2022,
  url      = "https://www.sciencedirect.com/science/article/pii/S0031320322004241",
  doi      = "10.1016/j.patcog.2022.108944",
  issn     = "0031-3203"
}

@ARTICLE{Negri2017-eu,
  title    = "{A review of the roles of digital twin in CPS-based production
              systems}",
  author   = "Negri, Elisa and Fumagalli, Luca and Macchi, Marco",
  journal  = "Procedia Manufacturing",
  volume   =  11,
  pages    = "939–948",
  abstract = "The Digital Twin (DT) is one of the main concepts associated to
              the Industry 4.0 wave. This term is more and more used in industry
              and research initiatives; however, the scientific literature does
              not provide a unique definition of this concept. The paper aims at
              analyzing the definitions of the DT concept in scientific
              literature, retracing it from the initial conceptualization in the
              aerospace field, to the most recent interpretations in the
              manufacturing domain and more specifically in Industry 4.0 and
              smart manufacturing research. DT provides virtual representations
              of systems along their lifecycle. Optimizations and decisions
              making would then rely on the same data that are updated in
              real-time with the physical system, through synchronization
              enabled by sensors. The paper also proposes the definition of DT
              for Industry 4.0 manufacturing, elaborated by the European H2020
              project MAYA, as a contribution to the research discussion about
              DT concept.",
  month    =  "1~" # jan,
  year     =  2017,
  url      = "https://www.sciencedirect.com/science/article/pii/S2351978917304067",
  file     = "All Papers/My Library/Negri et al. 2017 - A review of the roles of digital twin in CPS-based production systems.pdf",
  doi      = "10.1016/j.promfg.2017.07.198",
  issn     = "2351-9789"
}

@ARTICLE{Hayward2017-dg,
  title    = "{Staring reality in the face: A comparison of social attention
              across laboratory and real world measures suggests little common
              ground}",
  author   = "Hayward, Dana A and Voorhies, Willa and Morris, Jenna L and
              Capozzi, Francesca and Ristic, Jelena",
  journal  = "Canadian journal of experimental psychology = Revue canadienne de
              psychologie experimentale",
  volume   =  71,
  number   =  3,
  pages    = "212–225",
  abstract = "The ability to attend to someone else's gaze is thought to
              represent one of the essential building blocks of the human
              sociocognitive system. This behavior, termed social attention, has
              traditionally been assessed using laboratory procedures in which
              participants' response time and/or accuracy performance indexes
              attentional function. Recently, a parallel body of emerging
              research has started to examine social attention during real life
              social interactions using naturalistic and observational
              methodologies. The main goal of the present work was to begin
              connecting these two lines of inquiry. To do so, here we
              operationalized, indexed, and measured the engagement and shifting
              components of social attention using covert and overt measures.
              These measures were obtained during an unconstrained real-world
              social interaction and during a typical laboratory social cuing
              task. Our results indicated reliable and overall similar indices
              of social attention engagement and shifting within each task.
              However, these measures did not relate across the two tasks. We
              discuss these results as potentially reflecting the differences in
              social attention mechanisms, the specificity of the cuing task's
              measurement, as well as possible general dissimilarities with
              respect to context, task goals, and/or social presence. (PsycINFO
              Database Record",
  month    =  sep,
  year     =  2017,
  url      = "http://dx.doi.org/10.1037/cep0000117",
  file     = "All Papers/My Library/Hayward et al. 2017 - Staring reality in the face - A comparison of soci ... ross laboratory and real world measures suggests little common ground.pdf",
  doi      = "10.1037/cep0000117",
  issn     = "1196-1961,1878-7290",
  language = "en"
}

@ARTICLE{Duncan1972-rb,
  title     = "{Some signals and rules for taking speaking turns in
               conversations}",
  author    = "Duncan, Starkey",
  journal   = "Journal of personality and social psychology",
  publisher = "psycnet.apa.org",
  volume    =  23,
  number    =  2,
  pages     = "283--292",
  abstract  = "Studied the turn-taking mechanism, whereby participants manage
               the smooth and appropriate exchange of speaking turns in
               face-to-face interaction in 2 videotapes showing a
               therapist-patient interview and a discussion between 2
               therapists. 3 basic signals were noted: (a) turn-yielding signals
               by the speaker, (b) attempt-suppressing signals by the speaker,
               and (c) back-channel signals by the auditor. These signals were
               used and responded to in a relatively structured manner,
               describable in terms of a set of rules. Results indicate that
               behaviors in every communication modality examined content,
               syntax, intonation, paralanguage, and body motion were active as
               elements of the turn-taking signals. (22 ref.) (PsycINFO Database
               Record (c) 2016 APA, all rights reserved)",
  month     =  aug,
  year      =  1972,
  url       = "http://dx.doi.org/10.1037/h0033031",
  doi       = "10.1037/h0033031",
  issn      = "0022-3514,1939-1315"
}

@ARTICLE{Duncan1972-ia,
  title    = "{Some signals and rules for taking speaking turns in
              conversations}",
  author   = "Duncan, Starkey",
  journal  = "Journal of personality and social psychology",
  volume   =  23,
  number   =  2,
  pages    = "283–292",
  abstract = "Studied the turn-taking mechanism, whereby participants manage the
              smooth and appropriate exchange of speaking turns in face-to-face
              interaction in 2 videotapes showing a therapist-patient interview
              and a discussion between 2 therapists. 3 basic signals were noted:
              (a) turn-yielding signals by the speaker, (b) attempt-suppressing
              signals by the speaker, and (c) back-channel signals by the
              auditor. These signals were used and responded to in a relatively
              structured manner, describable in terms of a set of rules. Results
              indicate that behaviors in every communication modality examined
              content, syntax, intonation, paralanguage, and body motion were
              active as elements of the turn-taking signals. (22 ref.) (PsycINFO
              Database Record (c) 2016 APA, all rights reserved)",
  month    =  aug,
  year     =  1972,
  url      = "https://psycnet.apa.org/fulltext/1973-00754-001.pdf",
  doi      = "10.1037/h0033031",
  issn     = "0022-3514,1939-1315"
}

@ARTICLE{Bailenson2021-sc,
  title     = "{Nonverbal overload: A theoretical argument for the causes of
               Zoom fatigue}",
  author    = "Bailenson, Jeremy N",
  journal   = "Technology, Mind, and Behavior",
  publisher = "American Psychological Association (APA)",
  volume    =  2,
  number    =  1,
  month     =  feb,
  year      =  2021,
  url       = "http://dx.doi.org/10.1037/tmb0000030",
  file      = "All Papers/Other/Bailenson 2021 - Nonverbal overload - A theoretical argument for the causes of Zoom fatigue.pdf",
  keywords  = "eye contact;telepresence;Zoom
               Fatigue;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1037/tmb0000030",
  issn      = "2689-0208",
  language  = "en"
}

@ARTICLE{Bailenson2021-fq,
  title    = "{Nonverbal overload: A theoretical argument for the causes of Zoom
              fatigue}",
  author   = "Bailenson, Jeremy N",
  journal  = "Technology, Mind, and Behavior",
  volume   =  2,
  number   =  1,
  month    =  feb,
  year     =  2021,
  url      = "http://dx.doi.org/10.1037/tmb0000030",
  file     = "All Papers/My Library/Bailenson 2021 - Nonverbal overload - A theoretical argument for the causes of Zoom fatigue.pdf",
  doi      = "10.1037/tmb0000030",
  issn     = "2689-0208",
  language = "en"
}

@ARTICLE{Francl2022-ke,
  title    = "{Deep neural network models of sound localization reveal how
              perception is adapted to real-world environments}",
  author   = "Francl, Andrew and McDermott, Josh H",
  journal  = "Nature human behaviour",
  volume   =  6,
  number   =  1,
  pages    = "111–133",
  abstract = "Mammals localize sounds using information from their two ears.
              Localization in real-world conditions is challenging, as echoes
              provide erroneous information and noises mask parts of target
              sounds. To better understand real-world localization, we equipped
              a deep neural network with human ears and trained it to localize
              sounds in a virtual environment. The resulting model localized
              accurately in realistic conditions with noise and reverberation.
              In simulated experiments, the model exhibited many features of
              human spatial hearing: sensitivity to monaural spectral cues and
              interaural time and level differences, integration across
              frequency, biases for sound onsets and limits on localization of
              concurrent sources. But when trained in unnatural environments
              without reverberation, noise or natural sounds, these performance
              characteristics deviated from those of humans. The results show
              how biological hearing is adapted to the challenges of real-world
              environments and illustrate how artificial neural networks can
              reveal the real-world constraints that shape perception.",
  month    =  jan,
  year     =  2022,
  url      = "http://dx.doi.org/10.1038/s41562-021-01244-z",
  file     = "All Papers/My Library/Francl and McDermott 2022 - Deep neural network models of sound localization reveal how perception is adapted to real-world environments.pdf",
  doi      = "10.1038/s41562-021-01244-z",
  issn     = "2397-3374",
  language = "en"
}

@ARTICLE{Wright2022-cg,
  title    = "{Deep physical neural networks trained with backpropagation}",
  author   = "Wright, Logan G and Onodera, Tatsuhiro and Stein, Martin M and
              Wang, Tianyu and Schachter, Darren T and Hu, Zoey and McMahon,
              Peter L",
  journal  = "Nature",
  volume   =  601,
  number   =  7894,
  pages    = "549--555",
  abstract = "Deep-learning models have become pervasive tools in science and
              engineering. However, their energy requirements now increasingly
              limit their scalability1. Deep-learning accelerators2-9 aim to
              perform deep learning energy-efficiently, usually targeting the
              inference phase and often by exploiting physical substrates beyond
              conventional electronics. Approaches so far10-22 have been unable
              to apply the backpropagation algorithm to train unconventional
              novel hardware in situ. The advantages of backpropagation have
              made it the de facto training method for large-scale neural
              networks, so this deficiency constitutes a major impediment. Here
              we introduce a hybrid in situ-in silico algorithm, called
              physics-aware training, that applies backpropagation to train
              controllable physical systems. Just as deep learning realizes
              computations with deep neural networks made from layers of
              mathematical functions, our approach allows us to train deep
              physical neural networks made from layers of controllable physical
              systems, even when the physical layers lack any mathematical
              isomorphism to conventional artificial neural network layers. To
              demonstrate the universality of our approach, we train diverse
              physical neural networks based on optics, mechanics and
              electronics to experimentally perform audio and image
              classification tasks. Physics-aware training combines the
              scalability of backpropagation with the automatic mitigation of
              imperfections and noise achievable with in situ algorithms.
              Physical neural networks have the potential to perform machine
              learning faster and more energy-efficiently than conventional
              electronic processors and, more broadly, can endow physical
              systems with automatically designed physical functionalities, for
              example, for robotics23-26, materials27-29 and smart sensors30-32.",
  month    =  jan,
  year     =  2022,
  url      = "http://dx.doi.org/10.1038/s41586-021-04223-6",
  file     = "All Papers/Other/Wright et al. 2022 - Deep physical neural networks trained with backpropagation.pdf",
  keywords = "digitalnature",
  doi      = "10.1038/s41586-021-04223-6",
  pmid     =  35082422,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Wright2022-bt,
  title    = "{Deep physical neural networks trained with backpropagation}",
  author   = "Wright, Logan G and Onodera, Tatsuhiro and Stein, Martin M and
              Wang, Tianyu and Schachter, Darren T and Hu, Zoey and McMahon,
              Peter L",
  journal  = "Nature",
  volume   =  601,
  number   =  7894,
  pages    = "549–555",
  abstract = "Deep-learning models have become pervasive tools in science and
              engineering. However, their energy requirements now increasingly
              limit their scalability1. Deep-learning accelerators2-9 aim to
              perform deep learning energy-efficiently, usually targeting the
              inference phase and often by exploiting physical substrates beyond
              conventional electronics. Approaches so far10-22 have been unable
              to apply the backpropagation algorithm to train unconventional
              novel hardware in situ. The advantages of backpropagation have
              made it the de facto training method for large-scale neural
              networks, so this deficiency constitutes a major impediment. Here
              we introduce a hybrid in situ-in silico algorithm, called
              physics-aware training, that applies backpropagation to train
              controllable physical systems. Just as deep learning realizes
              computations with deep neural networks made from layers of
              mathematical functions, our approach allows us to train deep
              physical neural networks made from layers of controllable physical
              systems, even when the physical layers lack any mathematical
              isomorphism to conventional artificial neural network layers. To
              demonstrate the universality of our approach, we train diverse
              physical neural networks based on optics, mechanics and
              electronics to experimentally perform audio and image
              classification tasks. Physics-aware training combines the
              scalability of backpropagation with the automatic mitigation of
              imperfections and noise achievable with in situ algorithms.
              Physical neural networks have the potential to perform machine
              learning faster and more energy-efficiently than conventional
              electronic processors and, more broadly, can endow physical
              systems with automatically designed physical functionalities, for
              example, for robotics23-26, materials27-29 and smart sensors30-32.",
  month    =  jan,
  year     =  2022,
  url      = "http://dx.doi.org/10.1038/s41586-021-04223-6",
  file     = "All Papers/My Library/Wright et al. 2022 - Deep physical neural networks trained with backpropagation.pdf",
  doi      = "10.1038/s41586-021-04223-6",
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Lake2023-un,
  title     = "{Human-like systematic generalization through a meta-learning
               neural network}",
  author    = "Lake, Brenden M and Baroni, Marco",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  623,
  number    =  7985,
  pages     = "115--121",
  abstract  = "The power of human language and thought arises from systematic
               compositionality-the algebraic ability to understand and produce
               novel combinations from known components. Fodor and Pylyshyn1
               famously argued that artificial neural networks lack this
               capacity and are therefore not viable models of the mind. Neural
               networks have advanced considerably in the years since, yet the
               systematicity challenge persists. Here we successfully address
               Fodor and Pylyshyn's challenge by providing evidence that neural
               networks can achieve human-like systematicity when optimized for
               their compositional skills. To do so, we introduce the
               meta-learning for compositionality (MLC) approach for guiding
               training through a dynamic stream of compositional tasks. To
               compare humans and machines, we conducted human behavioural
               experiments using an instruction learning paradigm. After
               considering seven different models, we found that, in contrast to
               perfectly systematic but rigid probabilistic symbolic models, and
               perfectly flexible but unsystematic neural networks, only MLC
               achieves both the systematicity and flexibility needed for
               human-like generalization. MLC also advances the compositional
               skills of machine learning systems in several systematic
               generalization benchmarks. Our results show how a standard neural
               network architecture, optimized for its compositional skills, can
               mimic human systematic generalization in a head-to-head
               comparison.",
  month     =  nov,
  year      =  2023,
  url       = "http://dx.doi.org/10.1038/s41586-023-06668-3",
  file      = "All Papers/Other/Lake and Baroni 2023 - Human-like systematic generalization through a meta-learning neural network.pdf;All Papers/Other/Lake and Baroni 2023 - al-LakeandBaroni2023-Human-likesystematicgeneralizationthroughameta-learningneuralnetwork.pdf",
  doi       = "10.1038/s41586-023-06668-3",
  pmc       = "PMC10620072",
  pmid      =  37880371,
  issn      = "0028-0836,1476-4687",
  language  = "en"
}

@ARTICLE{Rogers2018-ah,
  title    = "{Using dual eye tracking to uncover personal gaze patterns during
              social interaction}",
  author   = "Rogers, Shane L and Speelman, Craig P and Guidetti, Oliver and
              Longmuir, Melissa",
  journal  = "Scientific reports",
  volume   =  8,
  number   =  1,
  pages    =  4271,
  abstract = "We report the personal eye gaze patterns of people engaged in
              face-to-face getting acquainted conversation. Considerable
              differences between individuals are underscored by a stability of
              eye gaze patterns within individuals. Results suggest the
              existence of an eye-mouth gaze continuum. This continuum includes
              some people showing a strong preference for eye gaze, some with a
              strong preference for mouth gaze, and others distributing their
              gaze between the eyes and mouth to varying extents. Additionally,
              we found evidence of within-participant consistency not just for
              location preference but also for the duration of fixations upon
              the eye and mouth regions. We also estimate that during a 4-minute
              getting acquainted conversation mutual face gaze constitutes about
              60\% of conversation that occurs via typically brief instances of
              2.2 seconds. Mutual eye contact ranged from 0-45\% of
              conversation, via very brief instances. This was despite
              participants subjectively perceiving eye contact occurring for
              about 70\% of conversation. We argue that the subjective
              perception of eye contact is a product of mutual face gaze instead
              of actual mutual eye contact. We also outline the fast activity of
              gaze movements upon various locations both on and off face during
              a typical face-to-face conversation.",
  month    =  "3~" # sep,
  year     =  2018,
  url      = "http://dx.doi.org/10.1038/s41598-018-22726-7",
  file     = "All Papers/My Library/Rogers et al. 2018 - Using dual eye tracking to uncover personal gaze patterns during social interaction.pdf",
  doi      = "10.1038/s41598-018-22726-7",
  issn     = "2045-2322",
  language = "en"
}

@ARTICLE{Khaldi2020-zr,
  title    = "{A laser emitting contact lens for eye tracking}",
  author   = "Khaldi, A and Daniel, E and Massin, L and Kärnfelt, C and
              Ferranti, F and Lahuec, C and Seguin, F and Nourrit, V and de
              Bougrenet de la Tocnaye, J-L",
  journal  = "Scientific reports",
  volume   =  10,
  number   =  1,
  pages    =  14804,
  abstract = "In this paper, we present the first realisation and
              experimentation of a new eye tracking system using an infrared
              (iR) laser pointer embedded into a wireless smart contact lens. We
              denote this contact lens prototype as the cyclops lens, in
              reference to the famous hero of the X-Men comics. The full eye
              tracker device combines the smart contact lens and its eyewear,
              which provides a primary source of energy and the beam detection
              system. We detail the assembling and encapsulation process of the
              main functionalities into the contact lens and present how a gaze
              tracking system is achieved, compared to existing conventional
              eye-tracking ones. Finally, we discuss future technical
              improvements.",
  month    =  "9~" # sep,
  year     =  2020,
  url      = "http://dx.doi.org/10.1038/s41598-020-71233-1",
  file     = "All Papers/My Library/Khaldi et al. 2020 - A laser emitting contact lens for eye tracking.pdf",
  doi      = "10.1038/s41598-020-71233-1",
  issn     = "2045-2322",
  language = "en"
}

@ARTICLE{Vehlen2021-tq,
  title     = "{Evaluation of an eye tracking setup for studying visual
               attention in face-to-face conversations}",
  author    = "Vehlen, Antonia and Spenthof, Ines and Tönsing, Daniel and
               Heinrichs, Markus and Domes, Gregor",
  journal   = "Scientific reports",
  publisher = "Springer Science and Business Media LLC",
  volume    =  11,
  number    =  1,
  pages     =  2661,
  abstract  = "Many eye tracking studies use facial stimuli presented on a
               display to investigate attentional processing of social stimuli.
               To introduce a more realistic approach that allows interaction
               between two real people, we evaluated a new eye tracking setup in
               three independent studies in terms of data quality, short-term
               reliability and feasibility. Study 1 measured the robustness,
               precision and accuracy for calibration stimuli compared to a
               classical display-based setup. Study 2 used the identical
               measures with an independent study sample to compare the data
               quality for a photograph of a face (2D) and the face of the real
               person (3D). Study 3 evaluated data quality over the course of a
               real face-to-face conversation and examined the gaze behavior on
               the facial features of the conversation partner. Study 1 provides
               evidence that quality indices for the scene-based setup were
               comparable to those of a classical display-based setup. Average
               accuracy was better than 0.4° visual angle. Study 2 demonstrates
               that eye tracking quality is sufficient for 3D stimuli and robust
               against short interruptions without re-calibration. Study 3
               confirms the long-term stability of tracking accuracy during a
               face-to-face interaction and demonstrates typical gaze patterns
               for facial features. Thus, the eye tracking setup presented here
               seems feasible for studying gaze behavior in dyadic face-to-face
               interactions. Eye tracking data obtained with this setup achieves
               an accuracy that is sufficient for investigating behavior such as
               eye contact in social interactions in a range of populations
               including clinical conditions, such as autism spectrum and social
               phobia.",
  month     =  "29~" # jan,
  year      =  2021,
  url       = "https://consensus.apphttps://consensus.app/papers/evaluation-tracking-setup-studying-attention-facetoface-vehlen/60d4c11a2dc254cdbd76c0f068975342/?extracted-answer=The+eye+tracking+setup+presented+here+is+feasible+for+studying+gaze+behavior+in+face-to-face+interactions+and+achieves+sufficient+accuracy+for+investigating+eye+contact+in+social+interactions.&q=research+that+eye+tracking+for+estimate+eye+contact+occurance+on+videoconferencing&copilot=on",
  file      = "All Papers/Other/Vehlen et al. 2021 - Evaluation of an eye tracking setup for studying visual attention in face-to-face conversations.pdf",
  doi       = "10.1038/s41598-021-81987-x",
  pmc       = "PMC7846602",
  pmid      =  33514767,
  issn      = "2045-2322",
  language  = "en"
}

@ARTICLE{Vansteensel2016-rv,
  title     = "{Fully Implanted {Brain--Computer} Interface in a {Locked-In}
               Patient with {ALS}}",
  author    = "Vansteensel, Mariska J and Pels, Elmar G M and Bleichner, Martin
               G and Branco, Mariana P and Denison, Timothy and Freudenburg,
               Zachary V and Gosselaar, Peter and Leinders, Sacha and Ottens,
               Thomas H and Van Den Boom, Max A and Van Rijen, Peter C and
               Aarnoutse, Erik J and Ramsey, Nick F",
  journal   = "The New England journal of medicine",
  publisher = "Massachusetts Medical Society",
  volume    =  375,
  number    =  21,
  pages     = "2060--2066",
  abstract  = "Locked-in syndrome is characterized by a loss of voluntary
               muscular control, resulting in quadriplegia and aphonia, with
               retention of normal cognition.1 The syndrome is typically linked
               to brainstem stroke, but degenerative disorders such as ALS,
               which affects approximately 5 persons per 100,000 population,2
               progress to the same state.3 Despite their predicament, people in
               the locked-in state often report a high quality of life,4 which
               is correlated with the ability to communicate.5 Current
               strategies for communication depend mainly on eye movements that
               are followed by a camera, which enables selection of items on a
               computer screen (?eye tracker?); when that fails, . . .",
  month     =  nov,
  year      =  2016,
  url       = "http://dx.doi.org/10.1056/NEJMoa1608085",
  file      = "All Papers/Other/Vansteensel et al. 2016 - Fully Implanted Brain-Computer Interface in a Locked-In Patient with ALS.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1056/NEJMoa1608085",
  issn      = "0028-4793"
}

@ARTICLE{Vansteensel2016-pe,
  title    = "{Fully implanted brain-computer interface in a locked-in patient
              with {ALS}}",
  author   = "Vansteensel, Mariska J and Pels, Elmar G M and Bleichner, Martin G
              and Branco, Mariana P and Denison, Timothy and Freudenburg,
              Zachary V and Gosselaar, Peter and Leinders, Sacha and Ottens,
              Thomas H and Van Den Boom, Max A and Van Rijen, Peter C and
              Aarnoutse, Erik J and Ramsey, Nick F",
  journal  = "The New England journal of medicine",
  volume   =  375,
  number   =  21,
  pages    = "2060–2066",
  abstract = "Options for people with severe paralysis who have lost the ability
              to communicate orally are limited. We describe a method for
              communication in a patient with late-stage amyotrophic lateral
              sclerosis (ALS), involving a fully implanted brain-computer
              interface that consists of subdural electrodes placed over the
              motor cortex and a transmitter placed subcutaneously in the left
              side of the thorax. By attempting to move the hand on the side
              opposite the implanted electrodes, the patient accurately and
              independently controlled a computer typing program 28 weeks after
              electrode placement, at the equivalent of two letters per minute.
              The brain-computer interface offered autonomous communication that
              supplemented and at times supplanted the patient's eye-tracking
              device. (Funded by the Government of the Netherlands and the
              European Union; ClinicalTrials.gov number, NCT02224469 .).",
  month    =  "24~" # nov,
  year     =  2016,
  url      = "http://dx.doi.org/10.1056/NEJMoa1608085",
  file     = "All Papers/My Library/Vansteensel et al. 2016 - Fully implanted brain-computer interface in a locked-in patient with ALS.pdf",
  doi      = "10.1056/NEJMoa1608085",
  issn     = "0028-4793,1533-4406",
  language = "en"
}

@ARTICLE{Gruters2018-xz,
  title    = "{The eardrums move when the eyes move: A multisensory effect on
              the mechanics of hearing}",
  author   = "Gruters, Kurtis G and Murphy, David L K and Jenson, Cole D and
              Smith, David W and Shera, Christopher A and Groh, Jennifer M",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   =  115,
  number   =  6,
  pages    = "E1309--E1318",
  abstract = "Interactions between sensory pathways such as the visual and
              auditory systems are known to occur in the brain, but where they
              first occur is uncertain. Here, we show a multimodal interaction
              evident at the eardrum. Ear canal microphone measurements in
              humans (n = 19 ears in 16 subjects) and monkeys (n = 5 ears in
              three subjects) performing a saccadic eye movement task to visual
              targets indicated that the eardrum moves in conjunction with the
              eye movement. The eardrum motion was oscillatory and began as
              early as 10 ms before saccade onset in humans or with saccade
              onset in monkeys. These eardrum movements, which we dub eye
              movement-related eardrum oscillations (EMREOs), occurred in the
              absence of a sound stimulus. The amplitude and phase of the EMREOs
              depended on the direction and horizontal amplitude of the saccade.
              They lasted throughout the saccade and well into subsequent
              periods of steady fixation. We discuss the possibility that the
              mechanisms underlying EMREOs create eye movement-related binaural
              cues that may aid the brain in evaluating the relationship between
              visual and auditory stimulus locations as the eyes move.",
  month    =  "6~" # feb,
  year     =  2018,
  url      = "https://www.pnas.org/doi/full/10.1073/pnas.1717948115",
  file     = "All Papers/Other/Gruters et al. 2018 - The eardrums move when the eyes move - A multisensory effect on the mechanics of hearing.pdf",
  keywords = "EMREO; middle ear muscles; otoacoustic emissions; reference frame;
              saccade",
  doi      = "10.1073/pnas.1717948115",
  pmc      = "PMC5819440",
  pmid     =  29363603,
  issn     = "0027-8424,1091-6490",
  language = "en"
}

@ARTICLE{Lovich2023-ts,
  title    = "{Parametric information about eye movements is sent to the ears}",
  author   = "Lovich, Stephanie N and King, Cynthia D and Murphy, David L K and
              Landrum, Rachel E and Shera, Christopher A and Groh, Jennifer M",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   =  120,
  number   =  48,
  pages    = "e2303562120",
  abstract = "Eye movements alter the relationship between the visual and
              auditory spatial scenes. Signals related to eye movements affect
              neural pathways from the ear through auditory cortex and beyond,
              but how these signals contribute to computing the locations of
              sounds with respect to the visual scene is poorly understood.
              Here, we evaluated the information contained in eye
              movement-related eardrum oscillations (EMREOs), pressure changes
              recorded in the ear canal that occur in conjunction with
              simultaneous eye movements. We show that EMREOs contain parametric
              information about horizontal and vertical eye displacement as well
              as initial/final eye position with respect to the head. The
              parametric information in the horizontal and vertical directions
              can be modeled as combining linearly, allowing accurate prediction
              of the EMREOs associated with oblique (diagonal) eye movements.
              Target location can also be inferred from the EMREO signals
              recorded during eye movements to those targets. We hypothesize
              that the (currently unknown) mechanism underlying EMREOs could
              impose a two-dimensional eye-movement-related transfer function on
              any incoming sound, permitting subsequent processing stages to
              compute the positions of sounds in relation to the visual scene.",
  month    =  "28~" # nov,
  year     =  2023,
  url      = "https://www.pnas.org/doi/10.1073/pnas.2303562120",
  file     = "All Papers/Other/Lovich et al. 2023 - Parametric information about eye movements is sent to the ears.pdf",
  keywords = "coordinate transformations; otoacoustic emissions; reference
              frames; saccades; sound localization",
  doi      = "10.1073/pnas.2303562120",
  pmc      = "PMC10691342",
  pmid     =  37988462,
  issn     = "0027-8424,1091-6490",
  language = "en"
}

@ARTICLE{Hwang2014-ur,
  title     = "{An eye-tracking assistive device improves the quality of life
               for {ALS} patients and reduces the caregivers' burden}",
  author    = "Hwang, Chi-Shin and Weng, Ho-Hsiu and Wang, Li-Fen and Tsai,
               Chon-Haw and Chang, Hao-Teng",
  journal   = "Journal of motor behavior",
  publisher = "Taylor \& Francis",
  volume    =  46,
  number    =  4,
  pages     = "233--238",
  abstract  = "Amyotrophic lateral sclerosis (ALS) is a devastating
               neurodegenerative disease. In some cases, patients with ALS
               retain a normal level of consciousness but disease progression
               eventually results in generalized paralysis, which first impedes
               and then prevents oral communication. This communication obstacle
               can generate a great deal of stress for the patient, family, and
               caregiver. Here the authors ask whether the use of an
               eye-tracking assistive device can improve quality of life for ALS
               patients and relieves burden of their primary caregivers.
               Subjects were divided into two groups depending on whether they
               used (n = 10) or did not use (n = 10) an eye-tracking assistive
               device. The authors assessed patients' quality of life and
               severity of depression using the ALS Specific Quality of Life
               Instrument-Revised and the Taiwanese Depression Questionnaire,
               respectively. The Caregiver Burden Scale was used to assess the
               burden on caregivers. Our study shows that the eye-tracking
               assistive device significantly improved patients' quality of
               life, as compared with patients in the nonuser group (p <.01).
               The assistive device also reduced the burden on caregivers (p
               <.05). This is likely a result of the improvement of patient's
               autonomy and more effective communication between patient and
               caregiver.",
  month     =  apr,
  year      =  2014,
  url       = "http://dx.doi.org/10.1080/00222895.2014.891970",
  keywords  = "amyotrophic lateral sclerosis; eye-tracking assistive device;
               quality of life;prj-gaze-shorthand",
  doi       = "10.1080/00222895.2014.891970",
  pmid      =  24731126,
  issn      = "0022-2895,1940-1027",
  language  = "en"
}

@ARTICLE{Hwang2014-ct,
  title    = "{An eye-tracking assistive device improves the quality of life for
              ALS patients and reduces the caregivers' burden}",
  author   = "Hwang, Chi-Shin and Weng, Ho-Hsiu and Wang, Li-Fen and Tsai,
              Chon-Haw and Chang, Hao-Teng",
  journal  = "Journal of motor behavior",
  volume   =  46,
  number   =  4,
  pages    = "233–238",
  abstract = "Amyotrophic lateral sclerosis (ALS) is a devastating
              neurodegenerative disease. In some cases, patients with ALS retain
              a normal level of consciousness but disease progression eventually
              results in generalized paralysis, which first impedes and then
              prevents oral communication. This communication obstacle can
              generate a great deal of stress for the patient, family, and
              caregiver. Here the authors ask whether the use of an eye-tracking
              assistive device can improve quality of life for ALS patients and
              relieves burden of their primary caregivers. Subjects were divided
              into two groups depending on whether they used (n = 10) or did not
              use (n = 10) an eye-tracking assistive device. The authors
              assessed patients' quality of life and severity of depression
              using the ALS Specific Quality of Life Instrument-Revised and the
              Taiwanese Depression Questionnaire, respectively. The Caregiver
              Burden Scale was used to assess the burden on caregivers. Our
              study shows that the eye-tracking assistive device significantly
              improved patients' quality of life, as compared with patients in
              the nonuser group (p ¡.01). The assistive device also reduced the
              burden on caregivers (p ¡.05). This is likely a result of the
              improvement of patient's autonomy and more effective communication
              between patient and caregiver.",
  month    =  "14~" # apr,
  year     =  2014,
  url      = "http://dx.doi.org/10.1080/00222895.2014.891970",
  doi      = "10.1080/00222895.2014.891970",
  issn     = "0022-2895,1940-1027",
  language = "en"
}

@ARTICLE{Wenzlaff2015-gl,
  title    = "{Video-based eye tracking in sex research: A systematic literature
              review}",
  author   = "Wenzlaff, Frederike and Briken, Peer and Dekker, Arne",
  journal  = "Journal of sex research",
  volume   =  53,
  number   =  8,
  pages    = "1--12",
  abstract = "Although eye tracking has been used for decades, it has gained
              popularity in the area of sex research only recently. The aim of
              this article is to examine the potential merits of eye tracking
              for this field. We present a systematic review of the current use
              of video-based eye-tracking technology in this area, evaluate the
              findings, and identify future research opportunities. A total of
              34 relevant studies published between 2006 and 2014 were
              identified for inclusion by means of online databases and other
              methods. We grouped them into three main areas of research: body
              perception and attractiveness, forensic research, and sexual
              orientation. Despite the methodological and theoretical
              differences across the studies, eye tracking has been shown to be
              a promising tool for sex research. The article suggests there is
              much potential for further studies to employ this technique
              because it is noninvasive and yet still allows for the assessment
              of both conscious and unconscious perceptional processes.
              Furthermore, eye tracking can be implemented in investigations of
              various theoretical backgrounds, ranging from biology to the
              social sciences.",
  month    =  "21~" # dec,
  year     =  2015,
  url      = "https://consensus.apphttps://consensus.app/papers/videobased-tracking-research-systematic-literature-wenzlaff/64c74583b87e55428b63149152f9fdc2/?extracted-answer=Eye+tracking+has+been+shown+to+be+a+promising+tool+for+sex+research%2C+allowing+for+assessment+of+both+conscious+and+unconscious+perceptional+processes.&q=research+that+eye+tracking+for+estimate+eye+contact+occurance+on+videoconferencing&copilot=on",
  doi      = "10.1080/00224499.2015.1107524",
  pmid     =  26689496,
  issn     = "0022-4499,1559-8519",
  language = "en"
}

@ARTICLE{Brooks1986-pi,
  title     = "{Effects of Duration of Eye Contact on Judgments of Personality
               Characteristics}",
  author    = "Brooks, Charles I and Church, Michael A and Fraser, Lance",
  journal   = "The Journal of social psychology",
  publisher = "Routledge",
  volume    =  126,
  number    =  1,
  pages     = "71--78",
  abstract  = "Abstract American male (n = 60) and female (n = 60) college
               undergraduates were randomly assigned to 12 same-sex groups of 10
               subjects each. The groups individually viewed one of six 60-s
               videotapes. The male or female model in the tape maintained eye
               contact with an alleged interviewer for a total of 5 s, 30 s, or
               50 s. Thus, the design factorially combined gender of subject,
               gender of model, and duration of eye contact, with all
               comparisons between subjects. After viewing the tape, subjects
               rated the model on a series of bipolar adjectives designed to
               assess the perceived potency (e.g., strength, aggression, and
               leadership) of the model. The results consistently showed that as
               eye contact increased, the models were perceived as more potent.
               In addition, the models were judged to have higher grade point
               averages (GPAs) as their eye contact increased. The effects of
               gender (of both model and subject) were mostly nonsignificant,
               following no systematic pattern.",
  month     =  feb,
  year      =  1986,
  url       = "http://dx.doi.org/10.1080/00224545.1986.9713572",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1080/00224545.1986.9713572",
  issn      = "0022-4545"
}

@ARTICLE{Brooks1986-wd,
  title    = "{Effects of duration of eye contact on judgments of personality
              characteristics}",
  author   = "Brooks, Charles I and Church, Michael A and Fraser, Lance",
  journal  = "The Journal of social psychology",
  volume   =  126,
  number   =  1,
  pages    = "71–78",
  abstract = "Abstract American male (n = 60) and female (n = 60) college
              undergraduates were randomly assigned to 12 same-sex groups of 10
              subjects each. The groups individually viewed one of six 60-s
              videotapes. The male or female model in the tape maintained eye
              contact with an alleged interviewer for a total of 5 s, 30 s, or
              50 s. Thus, the design factorially combined gender of subject,
              gender of model, and duration of eye contact, with all comparisons
              between subjects. After viewing the tape, subjects rated the model
              on a series of bipolar adjectives designed to assess the perceived
              potency (e.g., strength, aggression, and leadership) of the model.
              The results consistently showed that as eye contact increased, the
              models were perceived as more potent. In addition, the models were
              judged to have higher grade point averages (GPAs) as their eye
              contact increased. The effects of gender (of both model and
              subject) were mostly nonsignificant, following no systematic
              pattern.",
  month    =  "2~" # jan,
  year     =  1986,
  url      = "https://doi.org/10.1080/00224545.1986.9713572",
  doi      = "10.1080/00224545.1986.9713572",
  issn     = "0022-4545"
}

@ARTICLE{Sumner2022-ba,
  title     = "{Zoom face: Self-surveillance, performance and display}",
  author    = "Sumner, Tyne Daile",
  journal   = "Journal of intercultural studies (Melbourne, Vic.)",
  publisher = "Informa UK Limited",
  volume    =  43,
  number    =  6,
  pages     = "865--879",
  month     =  "2~" # nov,
  year      =  2022,
  url       = "https://consensus.apphttps://consensus.app/papers/zoom-face-selfsurveillance-performance-display-sumner/dbe254815b985f5282e53acbd962e1d8/?q=A+research+study+that+examined+eye+contact+occurrence+and+zoom+fatigue+during+videoconferencing%2C+together+with+eye+tracking.&copilot=on",
  doi       = "10.1080/07256868.2022.2128087",
  issn      = "0725-6868,1469-9540",
  language  = "en"
}

@ARTICLE{Rozado2015-zc,
  title    = "{Controlling a Smartphone Using Gaze Gestures as the Input
              Mechanism}",
  author   = "Rozado, D and Moreno, T and San Agustin, J and Rodriguez, F B and
              Varona, P",
  journal  = "Human–Computer Interaction",
  volume   =  30,
  number   =  1,
  pages    = "34--63",
  abstract = "The emergence of small handheld devices such as tablets and
              smartphones, often with touch sensitive surfaces as their only
              input modality, has spurred a growing interest in the subject of
              gestures for human–computer interaction (HCI). It has been proven
              before that eye movements can be consciously controlled by humans
              to the extent of performing sequences of predefined movement
              patterns, or “gaze gestures” that can be used for HCI purposes in
              desktop computers. Gaze gestures can be tracked noninvasively
              using a video-based eye-tracking system. We propose here that gaze
              gestures can also be an effective input paradigm to interact with
              handheld electronic devices. We show through a pilot user study
              how gaze gestures can be used to interact with a smartphone, how
              they are easily assimilated by potential users, and how the
              Needleman-Wunsch algorithm can effectively discriminate
              intentional gaze gestures from otherwise typical gaze activity
              performed during standard interaction with a small smartphone
              screen. Hence, reliable gaze–smartphone interaction is possible
              with accuracy rates, depending on the modality of gaze gestures
              being used (with or without dwell), higher than 80 to 90\%,
              negligible false positive rates, and completion speeds lower than
              1 to 1.5 s per gesture. These encouraging results and the low-cost
              eye-tracking equipment used suggest the possibilities of this new
              HCI modality for the field of interaction with small-screen
              handheld devices.",
  month    =  "2~" # jan,
  year     =  2015,
  url      = "https://doi.org/10.1080/07370024.2013.870385",
  doi      = "10.1080/07370024.2013.870385",
  issn     = "0737-0024"
}

@ARTICLE{Borgestig2016-im,
  title     = "{Eye gaze performance for children with severe physical
               impairments using gaze-based assistive technology---A
               longitudinal study}",
  author    = "Borgestig, Maria and Sandqvist, Jan and Parsons, Richard and
               Falkmer, Torbjörn and Hemmingsson, Helena",
  journal   = "Assistive technology: the official journal of RESNA",
  publisher = "Taylor \& Francis",
  volume    =  28,
  number    =  2,
  pages     = "93--102",
  abstract  = "ABSTRACTGaze-based assistive technology (gaze-based AT) has the
               potential to provide children affected by severe physical
               impairments with opportunities for communication and activities.
               This study aimed to examine changes in eye gaze performance over
               time (time on task and accuracy) in children with severe physical
               impairments, without speaking ability, using gaze-based AT. A
               longitudinal study with a before and after design was conducted
               on 10 children (aged 1?15 years) with severe physical
               impairments, who were beginners to gaze-based AT at baseline.
               Thereafter, all children used the gaze-based AT in daily
               activities over the course of the study. Compass computer
               software was used to measure time on task and accuracy with eye
               selection of targets on screen, and tests were performed with the
               children at baseline, after 5 months, 9?11 months, and after
               15?20 months. Findings showed that the children improved in time
               on task after 5 months and became more accurate in selecting
               targets after 15?20 months. This study indicates that these
               children with severe physical impairments, who were unable to
               speak, could improve in eye gaze performance. However, the
               children needed time to practice on a long-term basis to acquire
               skills needed to develop fast and accurate eye gaze performance.",
  month     =  apr,
  year      =  2016,
  url       = "http://dx.doi.org/10.1080/10400435.2015.1092182",
  file      = "All Papers/Other/Borgestig et al. 2016 - Eye gaze performance for children with severe ph ... mpairments using gaze-based assistive technology-A longitudinal study.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1080/10400435.2015.1092182",
  issn      = "1040-0435"
}

@ARTICLE{Borgestig2016-qp,
  title    = "{Eye gaze performance for children with severe physical
              impairments using gaze-based assistive technology-A longitudinal
              study}",
  author   = "Borgestig, Maria and Sandqvist, Jan and Parsons, Richard and
              Falkmer, Torbjörn and Hemmingsson, Helena",
  journal  = "Assistive technology: the official journal of RESNA",
  volume   =  28,
  number   =  2,
  pages    = "93–102",
  abstract = "Gaze-based assistive technology (gaze-based AT) has the potential
              to provide children affected by severe physical impairments with
              opportunities for communication and activities. This study aimed
              to examine changes in eye gaze performance over time (time on task
              and accuracy) in children with severe physical impairments,
              without speaking ability, using gaze-based AT. A longitudinal
              study with a before and after design was conducted on 10 children
              (aged 1-15 years) with severe physical impairments, who were
              beginners to gaze-based AT at baseline. Thereafter, all children
              used the gaze-based AT in daily activities over the course of the
              study. Compass computer software was used to measure time on task
              and accuracy with eye selection of targets on screen, and tests
              were performed with the children at baseline, after 5 months, 9-11
              months, and after 15-20 months. Findings showed that the children
              improved in time on task after 5 months and became more accurate
              in selecting targets after 15-20 months. This study indicates that
              these children with severe physical impairments, who were unable
              to speak, could improve in eye gaze performance. However, the
              children needed time to practice on a long-term basis to acquire
              skills needed to develop fast and accurate eye gaze performance.",
  year     =  2016,
  url      = "http://dx.doi.org/10.1080/10400435.2015.1092182",
  file     = "All Papers/My Library/Borgestig et al. 2016 - Eye gaze performance for children with severe ph ... mpairments using gaze-based assistive technology-A longitudinal study.pdf",
  doi      = "10.1080/10400435.2015.1092182",
  issn     = "1040-0435,1949-3614",
  language = "en"
}

@ARTICLE{Kotani2010-wo,
  title    = "{Design of eye-typing interface using saccadic latency of eye
              movement}",
  author   = "Kotani, Kentaro and Yamaguchi, Yuji and Asao, Takafumi and Horii,
              Ken",
  journal  = "International Journal of Human–Computer Interaction",
  volume   =  26,
  number   =  4,
  pages    = "361–376",
  abstract = "The objective of this study was to construct and empirically
              evaluate an improved, online eye-typing interface with respect to
              its practical usability. The system used the concept of saccadic
              latency, a silent period of 200 to 250 msec precedes the
              initiation of a saccade, for identifying the user's intentional
              text entry. Ten individuals participated in the experiment that
              was conducted on 2 consecutive days, with three blocks of trials
              conducted on each day. A block included five trials, each of which
              involved completing the text entry of a short sentence using this
              eye-typing interface. The proposed interface was evaluated by the
              user's performance based on indices including typing speed and an
              error index. For defining the error index, the overproduction
              rates (ORs) were used. The results showed an average OR of 0.032
              and average typing speed of 27.1 characters typed per minute. The
              result revealed that the typing speed changed as an effect of
              participant, day, and block. The characteristics of the proposed
              interface with the related characteristics of an eye-typing
              interface were summarized to discuss a further study for the
              eye-typing interface.",
  month    =  "25~" # mar,
  year     =  2010,
  url      = "https://doi.org/10.1080/10447310903575499",
  doi      = "10.1080/10447310903575499",
  issn     = "1044-7318"
}

@ARTICLE{Howell2016-dg,
  title     = "{Relations among social anxiety, eye contact avoidance, state
               anxiety, and perception of interaction performance during a live
               conversation}",
  author    = "Howell, Ashley N and Zibulsky, Devin A and Srivastav, Akanksha
               and Weeks, Justin W",
  journal   = "Cognitive behaviour therapy",
  publisher = "Informa UK Limited",
  volume    =  45,
  number    =  2,
  pages     = "111--122",
  abstract  = "There is building evidence that highly socially anxious (HSA)
               individuals frequently avoid making eye contact, which may
               contribute to less meaningful social interactions and maintenance
               of social anxiety symptoms. However, research to date is lacking
               in ecological validity due to the usage of either static or
               pre-recorded facial stimuli or subjective coding of eye contact.
               The current study examined the relationships among trait social
               anxiety, eye contact avoidance, state anxiety, and participants'
               self-perceptions of interaction performance during a live,
               four-minute conversation with a confederate via webcam, and while
               being covertly eye-tracked. Participants included undergraduate
               women who conversed with same-sex confederates. Results indicated
               that trait social anxiety was inversely related to eye contact
               duration and frequency averaged across the four minutes, and
               positively related to state social anxiety and negative
               self-ratings. In addition, greater anticipatory state anxiety was
               associated with reduced eye contact throughout the first minute
               of the conversation. Eye contact was not related to post-task
               state anxiety or self-perception of poor performance; although,
               trends emerged in which these relations may be positive for HSA
               individuals. The current findings provide enhanced support for
               the notion that eye contact avoidance is an important feature of
               social anxiety.",
  year      =  2016,
  url       = "http://dx.doi.org/10.1080/16506073.2015.1111932",
  keywords  = "Eye tracking; conversation; eye contact; social anxiety; social
               performance;eye contact",
  doi       = "10.1080/16506073.2015.1111932",
  pmid      =  26677735,
  issn      = "1650-6073,1651-2316",
  language  = "en"
}

@ARTICLE{Howell2016-cr,
  title    = "{Relations among social anxiety, eye contact avoidance, state
              anxiety, and perception of interaction performance during a live
              conversation}",
  author   = "Howell, Ashley N and Zibulsky, Devin A and Srivastav, Akanksha and
              Weeks, Justin W",
  journal  = "Cognitive behaviour therapy",
  volume   =  45,
  number   =  2,
  pages    = "111–122",
  abstract = "There is building evidence that highly socially anxious (HSA)
              individuals frequently avoid making eye contact, which may
              contribute to less meaningful social interactions and maintenance
              of social anxiety symptoms. However, research to date is lacking
              in ecological validity due to the usage of either static or
              pre-recorded facial stimuli or subjective coding of eye contact.
              The current study examined the relationships among trait social
              anxiety, eye contact avoidance, state anxiety, and participants'
              self-perceptions of interaction performance during a live,
              four-minute conversation with a confederate via webcam, and while
              being covertly eye-tracked. Participants included undergraduate
              women who conversed with same-sex confederates. Results indicated
              that trait social anxiety was inversely related to eye contact
              duration and frequency averaged across the four minutes, and
              positively related to state social anxiety and negative
              self-ratings. In addition, greater anticipatory state anxiety was
              associated with reduced eye contact throughout the first minute of
              the conversation. Eye contact was not related to post-task state
              anxiety or self-perception of poor performance; although, trends
              emerged in which these relations may be positive for HSA
              individuals. The current findings provide enhanced support for the
              notion that eye contact avoidance is an important feature of
              social anxiety.",
  year     =  2016,
  url      = "http://dx.doi.org/10.1080/16506073.2015.1111932",
  doi      = "10.1080/16506073.2015.1111932",
  issn     = "1650-6073,1651-2316",
  language = "en"
}

@ARTICLE{Kushner2021-mr,
  title     = "{Eccentric gaze as a possible cause of “zoom fatigue”}",
  author    = "Kushner, Burton J",
  journal   = "Journal of binocular vision and ocular motility",
  publisher = "Informa UK Limited",
  volume    =  71,
  number    =  4,
  pages     = "175--180",
  month     =  "2~" # oct,
  year      =  2021,
  url       = "https://consensus.apphttps://consensus.app/papers/eccentric-gaze-possible-cause-zoom-fatigue-kushner/7c5799ea8fea54c8858492a829313384/?q=research+that+eye+tracking+for+estimate+eye+contact+and++zoom+fatigue+etc.+on+videoconferencing&copilot=on",
  doi       = "10.1080/2576117x.2021.1985899",
  issn      = "2576-117X,2576-1218",
  language  = "en"
}

@ARTICLE{Mainsah2015-ha,
  title     = "{Increasing {BCI} communication rates with dynamic stopping
               towards more practical use: an {ALS} study}",
  author    = "Mainsah, B O and Collins, L M and Colwell, K A and Sellers, E W
               and Ryan, D B and Caves, K and Throckmorton, C S",
  journal   = "Journal of neural engineering",
  publisher = "iopscience.iop.org",
  volume    =  12,
  number    =  1,
  pages     =  016013,
  abstract  = "OBJECTIVE: The P300 speller is a brain-computer interface (BCI)
               that can possibly restore communication abilities to individuals
               with severe neuromuscular disabilities, such as amyotrophic
               lateral sclerosis (ALS), by exploiting elicited brain signals in
               electroencephalography (EEG) data. However, accurate spelling
               with BCIs is slow due to the need to average data over multiple
               trials to increase the signal-to-noise ratio (SNR) of the
               elicited brain signals. Probabilistic approaches to dynamically
               control data collection have shown improved performance in
               non-disabled populations; however, validation of these approaches
               in a target BCI user population has not occurred. APPROACH: We
               have developed a data-driven algorithm for the P300 speller based
               on Bayesian inference that improves spelling time by adaptively
               selecting the number of trials based on the acute SNR of a user's
               EEG data. We further enhanced the algorithm by incorporating
               information about the user's language. In this current study, we
               test and validate the algorithms online in a target BCI user
               population, by comparing the performance of the dynamic stopping
               (DS) (or early stopping) algorithms against the current
               state-of-the-art method, static data collection, where the amount
               of data collected is fixed prior to online operation. MAIN
               RESULTS: Results from online testing of the DS algorithms in
               participants with ALS demonstrate a significant increase in
               communication rate as measured in bits/min (100-300\%), and
               theoretical bit rate (100-550\%), while maintaining selection
               accuracy. Participants also overwhelmingly preferred the DS
               algorithms. SIGNIFICANCE: We have developed a viable BCI
               algorithm that has been tested in a target BCI population which
               has the potential for translation to improve BCI speller
               performance towards more practical use for communication.",
  month     =  feb,
  year      =  2015,
  url       = "http://dx.doi.org/10.1088/1741-2560/12/1/016013",
  file      = "All Papers/Other/Mainsah et al. 2015 - Increasing BCI communication rates with dynamic stopping towards more practical use - an ALS study.pdf",
  doi       = "10.1088/1741-2560/12/1/016013",
  pmc       = "PMC4631027",
  pmid      =  25588137,
  issn      = "1741-2560,1741-2552",
  language  = "en"
}

@ARTICLE{Mainsah2015-ui,
  title    = "{Increasing BCI communication rates with dynamic stopping towards
              more practical use: an ALS study}",
  author   = "Mainsah, B O and Collins, L M and Colwell, K A and Sellers, E W
              and Ryan, D B and Caves, K and Throckmorton, C S",
  journal  = "Journal of neural engineering",
  volume   =  12,
  number   =  1,
  pages    =  016013,
  abstract = "OBJECTIVE: The P300 speller is a brain-computer interface (BCI)
              that can possibly restore communication abilities to individuals
              with severe neuromuscular disabilities, such as amyotrophic
              lateral sclerosis (ALS), by exploiting elicited brain signals in
              electroencephalography (EEG) data. However, accurate spelling with
              BCIs is slow due to the need to average data over multiple trials
              to increase the signal-to-noise ratio (SNR) of the elicited brain
              signals. Probabilistic approaches to dynamically control data
              collection have shown improved performance in non-disabled
              populations; however, validation of these approaches in a target
              BCI user population has not occurred. APPROACH: We have developed
              a data-driven algorithm for the P300 speller based on Bayesian
              inference that improves spelling time by adaptively selecting the
              number of trials based on the acute SNR of a user's EEG data. We
              further enhanced the algorithm by incorporating information about
              the user's language. In this current study, we test and validate
              the algorithms online in a target BCI user population, by
              comparing the performance of the dynamic stopping (DS) (or early
              stopping) algorithms against the current state-of-the-art method,
              static data collection, where the amount of data collected is
              fixed prior to online operation. MAIN RESULTS: Results from online
              testing of the DS algorithms in participants with ALS demonstrate
              a significant increase in communication rate as measured in
              bits/min (100-300\%), and theoretical bit rate (100-550\%), while
              maintaining selection accuracy. Participants also overwhelmingly
              preferred the DS algorithms. SIGNIFICANCE: We have developed a
              viable BCI algorithm that has been tested in a target BCI
              population which has the potential for translation to improve BCI
              speller performance towards more practical use for communication.",
  month    =  feb,
  year     =  2015,
  url      = "http://dx.doi.org/10.1088/1741-2560/12/1/016013",
  file     = "All Papers/My Library/Mainsah et al. 2015 - Increasing BCI communication rates with dynamic stopping towards more practical use - an ALS study.pdf",
  doi      = "10.1088/1741-2560/12/1/016013",
  issn     = "1741-2560",
  language = "en"
}

@ARTICLE{Xiang2021-at,
  title     = "{An intelligent computing and control model of topological
               relation between spatial objects based on fuzzy theory}",
  author    = "Xiang, Jun",
  journal   = "Journal of physics. Conference series",
  publisher = "IOP Publishing",
  volume    =  1948,
  number    =  1,
  pages     =  012012,
  abstract  = "The technological progress in the field of artificial
               intelligence (AI) has brought new opportunities and challenges to
               the intelligent development and innovative research in fuzzy
               spatial relations, fuzzy theory is the basic theory of
               intelligent information processing, and the paper gives a brief
               analysis of fuzzy calculation method in spatial analysis and
               spatial relation calculation. In particular, the spatial
               relations of fuzzy objects are more difficult to express and
               describe accurately. This paper proposes a method to calculate
               the topological relations between fuzzy objects with
               4-Intersection model and 9-Intersection model, in the analysis of
               spatial relations, the extension from deterministic entity to
               uncertain entity is realized with fuzzy set theory, the fuzzy
               space matrix is constructed, then fuzzy topological relations and
               fuzzy membership are calculated and output, the model simplifies
               many complex relations, and it can describe eight topological
               relations including disjoint, meet, overlap, covers, contains,
               covered-by, inside and equal. In practical application, the
               position of spatial objects can be controlled and adjusted
               according to topological relations. Through analysis of algorithm
               and fuzzy membership calculation, the model is effective in
               theory.",
  month     =  jun,
  year      =  2021,
  url       = "http://dx.doi.org/10.1088/1742-6596/1948/1/012012",
  file      = "All Papers/Other/Xiang 2021 - An intelligent computing and control model of topological relation between spatial objects based on fuzzy theory.pdf",
  doi       = "10.1088/1742-6596/1948/1/012012",
  issn      = "1742-6588,1742-6596",
  language  = "en"
}

@ARTICLE{Xiang2021-lc,
  title    = "{An intelligent computing and control model of topological
              relation between spatial objects based on fuzzy theory}",
  author   = "Xiang, Jun",
  journal  = "Journal of physics. Conference series",
  volume   =  1948,
  number   =  1,
  pages    =  012012,
  abstract = "The technological progress in the field of artificial intelligence
              (AI) has brought new opportunities and challenges to the
              intelligent development and innovative research in fuzzy spatial
              relations, fuzzy theory is the basic theory of intelligent
              information processing, and the paper gives a brief analysis of
              fuzzy calculation method in spatial analysis and spatial relation
              calculation. In particular, the spatial relations of fuzzy objects
              are more difficult to express and describe accurately. This paper
              proposes a method to calculate the topological relations between
              fuzzy objects with 4-Intersection model and 9-Intersection model,
              in the analysis of spatial relations, the extension from
              deterministic entity to uncertain entity is realized with fuzzy
              set theory, the fuzzy space matrix is constructed, then fuzzy
              topological relations and fuzzy membership are calculated and
              output, the model simplifies many complex relations, and it can
              describe eight topological relations including disjoint, meet,
              overlap, covers, contains, covered-by, inside and equal. In
              practical application, the position of spatial objects can be
              controlled and adjusted according to topological relations.
              Through analysis of algorithm and fuzzy membership calculation,
              the model is effective in theory.",
  month    =  jun,
  year     =  2021,
  url      = "http://dx.doi.org/10.1088/1742-6596/1948/1/012012",
  file     = "All Papers/My Library/Xiang 2021 - An intelligent computing and control model of topological relation between spatial objects based on fuzzy theory.pdf",
  doi      = "10.1088/1742-6596/1948/1/012012",
  issn     = "1742-6588,1742-6596",
  language = "en"
}

@ARTICLE{De_Greef2001-jl,
  title    = "{Social presence in a home tele-application}",
  author   = "de Greef, P and Ijsselsteijn, W A",
  journal  = "Cyberpsychology \& behavior: the impact of the Internet,
              multimedia and virtual reality on behavior and society",
  volume   =  4,
  number   =  2,
  pages    = "307--315",
  abstract = "The current paper presents a study on the subjective evaluation of
              an advanced telecommunication platform aimed at informal home use,
              called the PhotoShare tele-application. This platform enables
              users to view photos (e.g., family or holiday snapshots) together,
              while the presenter and the viewer are at different, remote
              locations. The platform includes a common viewing space where the
              photos are displayed and selected, as well as an audio connection
              and a large-screen video connection for communication between the
              remote sites. The study investigated the effects of
              videocommunication on social presence. In addition, the ability to
              point at a picture with an electronic pointer was evaluated. In
              the context of presence research, the current study also provided
              information regarding the validity of the IPO Social Presence
              Questionnaire (IPO-SPQ), which was specifically designed to
              investigate social presence with telecommunication applications.
              The results indicated that adding broadband, life-size video
              communication significantly increased social presence. In
              addition, we found a significant effect of sex on social presence:
              women gave substantially higher social presence ratings than men.
              The absence of a significant effect of the pointing function
              indicated that extensive workspace functionality may be of minor
              importance to the user's feeling of social presence.",
  month    =  apr,
  year     =  2001,
  url      = "http://dx.doi.org/10.1089/109493101300117974",
  keywords = "eye contact;telepresence",
  doi      = "10.1089/109493101300117974",
  pmid     =  11710255,
  issn     = "1094-9313",
  language = "en"
}

@ARTICLE{De_Greef2001-ah,
  title    = "{Social presence in a home tele-application}",
  author   = "de Greef, P and Ijsselsteijn, W A",
  journal  = "Cyberpsychology \& behavior: the impact of the Internet,
              multimedia and virtual reality on behavior and society",
  volume   =  4,
  number   =  2,
  pages    = "307–315",
  abstract = "The current paper presents a study on the subjective evaluation of
              an advanced telecommunication platform aimed at informal home use,
              called the PhotoShare tele-application. This platform enables
              users to view photos (e.g., family or holiday snapshots) together,
              while the presenter and the viewer are at different, remote
              locations. The platform includes a common viewing space where the
              photos are displayed and selected, as well as an audio connection
              and a large-screen video connection for communication between the
              remote sites. The study investigated the effects of
              videocommunication on social presence. In addition, the ability to
              point at a picture with an electronic pointer was evaluated. In
              the context of presence research, the current study also provided
              information regarding the validity of the IPO Social Presence
              Questionnaire (IPO-SPQ), which was specifically designed to
              investigate social presence with telecommunication applications.
              The results indicated that adding broadband, life-size video
              communication significantly increased social presence. In
              addition, we found a significant effect of sex on social presence:
              women gave substantially higher social presence ratings than men.
              The absence of a significant effect of the pointing function
              indicated that extensive workspace functionality may be of minor
              importance to the user's feeling of social presence.",
  month    =  apr,
  year     =  2001,
  url      = "http://dx.doi.org/10.1089/109493101300117974",
  doi      = "10.1089/109493101300117974",
  issn     = "1094-9313",
  language = "en"
}

@ARTICLE{Ikeda2024-uv,
  title     = "{Objective evaluation of gaze location patterns using eye
               tracking during cystoscopy and artificial intelligence-assisted
               lesion detection}",
  author    = "Ikeda, Atsushi and Izumi, Kazuya and Katori, Kensuke and Nosato,
               Hirokazu and Kobayashi, Keita and Suzuki, Shuhei and Kandori,
               Shuya and Sanuki, Masaru and Ochiai, Yoichi and Nishiyama,
               Hiroyuki",
  journal   = "Journal of endourology",
  publisher = "Mary Ann Liebert Inc",
  volume    =  38,
  number    =  8,
  pages     = "865--870",
  abstract  = "Background: The diagnostic accuracy of cystoscopy varies
               according to the knowledge and experience of the performing
               physician. In this study, we evaluated the difference in
               cystoscopic gaze location patterns between medical students and
               urologists and assessed the differences in their eye movements
               when simultaneously observing conventional cystoscopic images and
               images with lesions detected by artificial intelligence (AI).
               Methodology: Eye-tracking measurements were performed, and
               observation patterns of participants (24 medical students and 10
               urologists) viewing images from routine cystoscopic videos were
               analyzed. The cystoscopic video was captured preoperatively in a
               case of initial-onset noninvasive bladder cancer with three
               low-lying papillary tumors in the posterior, anterior, and neck
               areas (urothelial carcinoma, high grade, and pTa). The viewpoint
               coordinates and stop times during observation were obtained using
               a noncontact type of gaze tracking and gaze measurement system
               for screen-based gaze tracking. In addition, observation patterns
               of medical students and urologists during parallel observation of
               conventional cystoscopic videos and AI-assisted lesion detection
               videos were compared. Results: Compared with medical students,
               urologists exhibited a significantly higher degree of stationary
               gaze entropy when viewing cystoscopic images (p < 0.05),
               suggesting that urologists with expertise in identifying lesions
               efficiently observed a broader range of bladder mucosal surfaces
               on the screen, presumably with the conscious intent of
               identifying pathologic changes. When the participants observed
               conventional and AI-assisted lesion detection images side by
               side, contrary to urologists, medical students showed a higher
               proportion of attention directed toward AI-detected lesion
               images. Conclusion: Eye-tracking measurements during cystoscopic
               image assessment revealed that experienced specialists
               efficiently observed a wide range of video screens during
               cystoscopy. In addition, this study revealed how lesion images
               detected by AI are viewed. Observation patterns of observers'
               gaze may have implications for assessing and improving
               proficiency and serving educational purposes. To the best of our
               knowledge, this is the first study to utilize eye tracking in
               cystoscopy. University of Tsukuba Hospital, clinical research
               reference number R02-122.",
  month     =  "9~" # aug,
  year      =  2024,
  url       = "https://www.liebertpub.com/doi/10.1089/end.2023.0699",
  keywords  = "artificial intelligence; bladder cancer; cystoscopy; diagnostic
               support; eye tracking; stationary gaze entropy",
  doi       = "10.1089/end.2023.0699",
  pmid      =  38526374,
  issn      = "0892-7790,1557-900X",
  language  = "en"
}

@ARTICLE{Miyauchi2005-ap,
  title     = "{Bidirectional eye contact for human-robot communication}",
  author    = "Miyauchi, D",
  journal   = "IEICE transactions on information and systems",
  publisher = "Institute of Electronics, Information and Communications
               Engineers (IEICE)",
  volume    = "E88-D",
  number    =  11,
  pages     = "2509--2516",
  abstract  = "Eye contact is an effective means of controlling human
               communication, such as in starting communication. It seems that
               we can make eye contact if we simply look at each other. However,
               this alone does not establish eye contact. Both parties also need
               to be aware of being watched by the other. We propose a method of
               bidirectional eye contact satisfying these conditions for
               human-robot communication. When a human wants to start
               communication with a robot, he/she watches the robot. If it finds
               a human looking at it, the robot turns to him/her, changing its
               facial expressions to let him/her know its awareness of his/her
               gaze. When the robot wants to initiate communication with a
               particular person, it moves its body and face toward him/her and
               changes its facial expressions to make the person notice its
               gaze. We show several experimental results to prove the
               effectiveness of this method. Moreover, we present a robot that
               can recognize hand gestures after making eye contact with the
               human to show the usefulness of eye contact as a means of
               controlling communication.",
  month     =  "1~" # nov,
  year      =  2005,
  url       = "https://consensus.apphttps://consensus.app/papers/bidirectional-contact-humanrobot-communication-miyauchi/ad734bd74de25b5cb7774bbd8fe5125a/?q=eye+contact+with+AI&copilot=on&lang=en",
  doi       = "10.1093/ietisy/e88-d.11.2509",
  issn      = "0916-8532,1745-1361",
  language  = "en"
}

@ARTICLE{Bolton2018-iu,
  title    = "{Customer experience challenges: bringing together digital,
              physical and social realms}",
  author   = "Bolton, Ruth N and McColl-Kennedy, Janet R and Cheung, Lilliemay
              and Gallan, Andrew and Orsingher, Chiara and Witell, Lars and
              Zaki, Mohamed",
  journal  = "Journal of Service Management",
  volume   =  29,
  number   =  5,
  pages    = "776–808",
  abstract = "Purpose The purpose of this paper is to explore innovations in
              customer experience at the intersection of the digital, physical
              and social realms. It explicitly considers experiences involving
              new technology-enabled services, such as digital twins and
              automated social presence (i.e. virtual assistants and service
              robots).Design/methodology/approach Future customer experiences
              are conceptualized within a three-dimensional space – low to high
              digital density, low to high physical complexity and low to high
              social presence – yielding eight octants.Findings The conceptual
              framework identifies eight “dualities,” or specific challenges
              connected with integrating digital, physical and social realms
              that challenge organizations to create superior customer
              experiences in both business-to-business and business-to-consumer
              markets. The eight dualities are opposing strategic options that
              organizations must reconcile when co-creating customer experiences
              under different conditions.Research limitations/implications A
              review of theory demonstrates that little research has been
              conducted at the intersection of the digital, physical and social
              realms. Most studies focus on one realm, with occasional reference
              to another. This paper suggests an agenda for future research and
              gives examples of fruitful ways to study connections among the
              three realms rather than in a single realm.Practical implications
              This paper provides guidance for managers in designing and
              managing customer experiences that the authors believe will need
              to be addressed by the year 2050.Social implications This paper
              discusses important societal issues, such as individual and
              societal needs for privacy, security and transparency. It sets out
              potential avenues for service innovation in these
              areas.Originality/value The conceptual framework integrates
              knowledge about customer experiences in digital, physical and
              social realms in a new way, with insights for future service
              research, managers and public policy makers.",
  month    =  "1~" # jan,
  year     =  2018,
  url      = "https://doi.org/10.1108/JOSM-04-2018-0113",
  file     = "All Papers/My Library/Bolton et al. 2018 - Customer experience challenges - bringing together digital, physical and social realms.pdf",
  doi      = "10.1108/JOSM-04-2018-0113",
  issn     = "1757-5818"
}

@ARTICLE{Gemmell2000-tx,
  title     = "{Gaze awareness for video-conferencing: A software approach}",
  author    = "Gemmell, J and Toyama, K and Zitnick, C L and Kang, T and Seitz,
               S",
  journal   = "IEEE Multimedia",
  publisher = "Institute of Electrical and Electronics Engineers",
  volume    =  7,
  number    =  4,
  pages     = "26--35",
  abstract  = "Previous attempts at bringing gaze awareness to desktop
               videoconferencing have relied on hardware solutions. In this
               article, the authors describe their software approach, which
               tracks participants' head and eye movements using vision
               techniques, then uses this information to graphically place the
               head and eyes in a 3D environment",
  month     =  nov,
  year      =  2000,
  url       = "http://dx.doi.org/10.1109/93.895152",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1109/93.895152",
  issn      = "1070-986X"
}

@ARTICLE{Gemmell2000-kg,
  title    = "{Gaze awareness for video-conferencing: a software approach}",
  author   = "Gemmell, J and Toyama, K and Zitnick, C L and Kang, T and Seitz, S",
  journal  = "IEEE Multimedia",
  volume   =  7,
  number   =  4,
  pages    = "26–35",
  abstract = "Previous attempts at bringing gaze awareness to desktop
              videoconferencing have relied on hardware solutions. In this
              article, the authors describe their software approach, which
              tracks participants' head and eye movements using vision
              techniques, then uses this information to graphically place the
              head and eyes in a 3D environment.",
  month    =  oct,
  year     =  2000,
  url      = "http://dx.doi.org/10.1109/93.895152",
  file     = "All Papers/My Library/Gemmell et al. 2000 - Gaze awareness for video-conferencing - a software approach.pdf",
  doi      = "10.1109/93.895152",
  issn     = "1941-0166"
}

@ARTICLE{Eid2016-sz,
  title     = "{A Novel {Eye-Gaze-Controlled} Wheelchair System for Navigating
               Unknown Environments: Case Study With a Person With {ALS}}",
  author    = "Eid, Mohamad A and Giakoumidis, Nikolas and El Saddik,
               Abdulmotaleb",
  journal   = "IEEE Access",
  publisher = "ieeexplore.ieee.org",
  volume    =  4,
  pages     = "558--573",
  abstract  = "Thanks to advances in electric wheelchair design, persons with
               motor impairments due to diseases, such as amyotrophic lateral
               sclerosis (ALS), have tools to become more independent and
               mobile. However, an electric wheelchair generally requires
               considerable skill to learn how to use and operate. Moreover,
               some persons with motor disabilities cannot drive an electric
               wheelchair manually (even with a joystick), because they lack the
               physical ability to control their hand movement (such is the case
               with people with ALS). In this paper, we propose a novel system
               that enables a person with motor disability to control a
               wheelchair via eye-gaze and to provide a continuous, real-time
               navigation in unknown environments. The system comprises a
               Permobile M400 wheelchair, eye tracking glasses, a depth camera
               to capture the geometry of the ambient space, a set of ultrasound
               and infrared sensors to detect obstacles with low proximity that
               are out of the field of view for the depth camera, a laptop
               placed on a flexible mount for maximized comfort, and a safety
               off switch to turn off the system whenever needed. First, a novel
               algorithm is proposed to support continuous, real-time target
               identification, path planning, and navigation in unknown
               environments. Second, the system utilizes a novel N-cell
               grid-based graphical user interface that adapts to input/output
               interfaces specifications. Third, a calibration method for the
               eye tracking system is implemented to minimize the calibration
               overheads. A case study with a person with ALS is presented, and
               interesting findings are discussed. The participant showed
               improved performance in terms of calibration time, task
               completion time, and navigation speed for a navigation trips
               between office, dining room, and bedroom. Furthermore, debriefing
               the caregiver has also shown promising results: the participant
               enjoyed higher level of confidence driving the wheelchair and
               experienced no collisions through all the experiment.",
  year      =  2016,
  url       = "http://dx.doi.org/10.1109/ACCESS.2016.2520093",
  file      = "All Papers/Other/Eid et al. 2016 - A Novel Eye-Gaze-Controlled Wheelchair System for Navigating Unknown Environments - Case Study With a Person With ALS.pdf",
  keywords  = "Wheelchair;Electric wheelchairs;Navigation;Real-time
               systems;Calibration;Gaze tracking;Cameras;Graphical user
               interfaces;Biomedical equipment;Environmental factors;Eye gaze
               tracking;eye gaze calibration;wheelchair control
               system;grid-based graphical user interface;unknown environment
               tracking;Eye gaze tracking;eye gaze calibration;wheelchair
               control system;grid-based graphical user interface;unknown
               environment tracking;prj-gaze-shorthand",
  doi       = "10.1109/ACCESS.2016.2520093",
  issn      = "2169-3536"
}

@ARTICLE{Eid2016-nm,
  title    = "{A novel Eye-Gaze-Controlled wheelchair system for navigating
              unknown environments: Case study with a person with {ALS}}",
  author   = "Eid, Mohamad A and Giakoumidis, Nikolas and El Saddik,
              Abdulmotaleb",
  journal  = "IEEE access : practical innovations, open solutions",
  volume   =  4,
  pages    = "558–573",
  abstract = "Thanks to advances in electric wheelchair design, persons with
              motor impairments due to diseases, such as amyotrophic lateral
              sclerosis (ALS), have tools to become more independent and mobile.
              However, an electric wheelchair generally requires considerable
              skill to learn how to use and operate. Moreover, some persons with
              motor disabilities cannot drive an electric wheelchair manually
              (even with a joystick), because they lack the physical ability to
              control their hand movement (such is the case with people with
              ALS). In this paper, we propose a novel system that enables a
              person with motor disability to control a wheelchair via eye-gaze
              and to provide a continuous, real-time navigation in unknown
              environments. The system comprises a Permobile M400 wheelchair,
              eye tracking glasses, a depth camera to capture the geometry of
              the ambient space, a set of ultrasound and infrared sensors to
              detect obstacles with low proximity that are out of the field of
              view for the depth camera, a laptop placed on a flexible mount for
              maximized comfort, and a safety off switch to turn off the system
              whenever needed. First, a novel algorithm is proposed to support
              continuous, real-time target identification, path planning, and
              navigation in unknown environments. Second, the system utilizes a
              novel N-cell grid-based graphical user interface that adapts to
              input/output interfaces specifications. Third, a calibration
              method for the eye tracking system is implemented to minimize the
              calibration overheads. A case study with a person with ALS is
              presented, and interesting findings are discussed. The participant
              showed improved performance in terms of calibration time, task
              completion time, and navigation speed for a navigation trips
              between office, dining room, and bedroom. Furthermore, debriefing
              the caregiver has also shown promising results: the participant
              enjoyed higher level of confidence driving the wheelchair and
              experienced no collisions through all the experiment.",
  year     =  2016,
  url      = "http://dx.doi.org/10.1109/ACCESS.2016.2520093",
  file     = "All Papers/My Library/Eid et al. 2016 - A novel Eye-Gaze-Controlled wheelchair system for navigating unknown environments - Case study with a person with ALS.pdf",
  doi      = "10.1109/ACCESS.2016.2520093",
  issn     = "2169-3536"
}

@ARTICLE{Liu2020-wp,
  title    = "{3D gaze estimation for head-mounted eye tracking system with
              auto-calibration method}",
  author   = "Liu, Meng and Li, Youfu and Liu, Hai",
  journal  = "IEEE access : practical innovations, open solutions",
  volume   = "PP",
  number   =  99,
  pages    = "1–1",
  abstract = "The general challenges of 3D gaze estimation for head-mounted eye
              tracking systems are inflexible marker-based calibration procedure
              and significant errors of depth estimation. In this paper, we
              propose a 3D gaze estimation with an auto-calibration method. To
              acquire the accurate 3D structure of the environment, an RGBD
              camera is applied as the scene camera of our system. By adopting
              the saliency detection method, saliency maps can be acquired
              through scene images, and 3D salient pixels in the scene are
              considered potential 3D calibration targets. The 3D eye model is
              built on the basis of eye images to determine gaze vectors. By
              combining 3D salient pixels and gaze vectors, the auto-calibration
              can be achieved with our calibration method. Finally, the 3D gaze
              point is obtained through the calibrated gaze vectors, and the
              point cloud is generated from the RGBD camera. The experimental
              result shows that the proposed system can achieve an average
              accuracy of 3.7° in the range of 1 m to 4 m indoors and 4.0°
              outdoors. The proposed system also presents a great improvement in
              depth measurement, which is sufficient for tracking users' visual
              attention in real scenes.",
  month    =  "6~" # mar,
  year     =  2020,
  url      = "https://www.researchgate.net/publication/341891783_3D_Gaze_Estimation_for_Head-Mounted_Eye_Tracking_System_With_Auto-Calibration_Method",
  file     = "All Papers/My Library/Liu et al. 2020 - 3D gaze estimation for head-mounted eye tracking system with auto-calibration method.pdf",
  doi      = "10.1109/ACCESS.2020.2999633",
  issn     = "2169-3536"
}

@ARTICLE{Adilkhanov2022-xp,
  title    = "{Haptic devices: Wearability-based taxonomy and literature review}",
  author   = "Adilkhanov, Adilzhan and Rubagotti, Matteo and Kappassov, Zhanat",
  journal  = "IEEE access : practical innovations, open solutions",
  pages    = "1–1",
  abstract = "In the last decade, several new haptic devices have been
              developed, contributing to the definition of more realistic
              virtual environments. An overview on this topic requires a
              description of the various technologies employed in building such
              devices, and of their application domains. This survey describes
              the current technology underlying haptic devices, based on the
              concept of “wearability level”. More than 90 devices, newly
              developed and described in scientific papers published in the
              period 2010-2021, are reviewed, which provide either haptic
              illusions or novel haptic feedback for teleoperation,
              entertainment, training, education, guidance and notification. As
              a result, the analyzed systems are divided into grounded,
              hand-held and wearable devices; the latter are further split into
              exoskeletons and gloves, finger-worn devices, and arm-worn
              devices. For the systems in each of these categories, descriptions
              and tables are provided that analyze their structure, including
              device mass and employed actuators, their applications, and other
              characteristics such as type of haptic feedback and tactile
              illusions. The paper also provides an overview of devices worn in
              parts of the human body other than arms and hands, and precisely
              haptic vests, jackets and belts, and haptic devices for head, legs
              and feet. Based on this analysis, the survey also provides a
              discussion on research gaps and challenges, and potential future
              directions.",
  year     =  2022,
  url      = "http://dx.doi.org/10.1109/ACCESS.2022.3202986",
  file     = "All Papers/My Library/Adilkhanov et al. 2022 - Haptic devices - Wearability-based taxonomy and literature review.pdf",
  doi      = "10.1109/ACCESS.2022.3202986",
  issn     = "2169-3536"
}

@INPROCEEDINGS{Lu2016-hc,
  title     = "{Chunk-wise face model based gaze correction in conversational
               videos with single camera}",
  author    = "Lu, Jichuan and Tao, Xiaoming and Dong, Linhao and Ge, Ning",
  booktitle = "{2016 international conference on computer, information and
               telecommunication systems (CITS)}",
  publisher = "IEEE",
  address   = "Kunming, China",
  pages     = "1–5",
  abstract  = "Eye contact is one of critical aspects of video conference. In
               current video conference systems, an important problem is the
               shortage of eye contact. This is due to the direction disparity
               between the normal of focus plane of camera and the
               interlocutor's gaze. Currently, the state-of-the-method performs
               well on gaze correction with only single webcam. However, this
               method that applied to the sequences with face obscured has shown
               unsatisfying results. In this paper, we propose a novel
               model-based gaze correction scheme to independently build the eye
               models, the nose model and the mouth model independently so that
               the occlusion problem can be solved effectively. To be more
               specific, by rotating the aforementioned four models at a
               pre-detected displacement, the degree of gaze correction can be
               obtained. The facial appearance that is occluded will not be
               rotated, which will prevent the occluding object from being
               distorted. Experiment results show that, under different parts of
               occluding objects, our method can achieve better performance on
               gaze correction than the current method in terms of perceived
               quality in two scenes usually appearing in conversational videos.",
  month     =  jul,
  year      =  2016,
  url       = "http://dx.doi.org/10.1109/CITS.2016.7546437",
  doi       = "10.1109/CITS.2016.7546437",
  isbn      =  9781509006908,
  language  = "en"
}

@INPROCEEDINGS{Kazemi2014-ln,
  title     = "{One millisecond face alignment with an ensemble of regression
               trees}",
  author    = "Kazemi, Vahid and Sullivan, Josephine",
  booktitle = "{2014 {IEEE} Conference on Computer Vision and Pattern
               Recognition}",
  pages     = "1867--1874",
  abstract  = "This paper addresses the problem of Face Alignment for a single
               image. We show how an ensemble of regression trees can be used to
               estimate the face's landmark positions directly from a sparse
               subset of pixel intensities, achieving super-realtime performance
               with high quality predictions. We present a general framework
               based on gradient boosting for learning an ensemble of regression
               trees that optimizes the sum of square error loss and naturally
               handles missing or partially labelled data. We show how using
               appropriate priors exploiting the structure of image data helps
               with efficient feature selection. Different regularization
               strategies and its importance to combat overfitting are also
               investigated. In addition, we analyse the effect of the quantity
               of training data on the accuracy of the predictions and explore
               the effect of data augmentation using synthesized data.",
  month     =  jun,
  year      =  2014,
  url       = "http://dx.doi.org/10.1109/CVPR.2014.241",
  file      = "All Papers/Other/Kazemi and Sullivan 2014 - One millisecond face alignment with an ensemble of regression trees.pdf",
  keywords  = "Shape;Regression tree analysis;Face;Training;Boosting;Training
               data;Vectors;Face Alignment;Real-Time;Gradient Boosting;Decision
               Trees",
  doi       = "10.1109/CVPR.2014.241",
  issn      = "1063-6919"
}

@INPROCEEDINGS{Kazemi2014-tf,
  title     = "{One millisecond face alignment with an ensemble of regression
               trees}",
  author    = "Kazemi, Vahid and Sullivan, Josephine",
  booktitle = "{2014 IEEE conference on computer vision and pattern recognition}",
  pages     = "1867–1874",
  abstract  = "This paper addresses the problem of Face Alignment for a single
               image. We show how an ensemble of regression trees can be used to
               estimate the face's landmark positions directly from a sparse
               subset of pixel intensities, achieving super-realtime performance
               with high quality predictions. We present a general framework
               based on gradient boosting for learning an ensemble of regression
               trees that optimizes the sum of square error loss and naturally
               handles missing or partially labelled data. We show how using
               appropriate priors exploiting the structure of image data helps
               with efficient feature selection. Different regularization
               strategies and its importance to combat overfitting are also
               investigated. In addition, we analyse the effect of the quantity
               of training data on the accuracy of the predictions and explore
               the effect of data augmentation using synthesized data.",
  month     =  jun,
  year      =  2014,
  url       = "http://dx.doi.org/10.1109/CVPR.2014.241",
  file      = "All Papers/My Library/Kazemi and Sullivan 2014 - One millisecond face alignment with an ensemble of regression trees.pdf",
  doi       = "10.1109/CVPR.2014.241"
}

@INPROCEEDINGS{Kononenko2015-dc,
  title     = "{Learning to look up: Realtime monocular gaze correction using
               machine learning}",
  author    = "Kononenko, Daniil and Lempitsky, Victor",
  booktitle = "{2015 IEEE conference on computer vision and pattern recognition
               (CVPR)}",
  publisher = "IEEE",
  address   = "Boston, MA, USA",
  pages     = "4667–4675",
  abstract  = "We revisit the well-known problem of gaze correction and present
               a solution based on supervised machine learning. At training
               time, our system observes pairs of images, where each pair
               contains the face of the same person with a fixed angular
               difference in gaze direction. It then learns to synthesize the
               second image of a pair from the first one. After learning, the
               system gets the ability to redirect the gaze of a previously
               unseen person by the same angular difference as in the training
               set. Unlike many previous solutions to gaze problem in
               videoconferencing, ours is purely monocular, i.e. it does not
               require any hardware apart from an in-built web-camera of a
               laptop. Being based on efficient machine learning predictors such
               as decision forests, the system is fast (runs in real-time on a
               single core of a modern laptop). In the paper, we demonstrate
               results on a variety of videoconferencing frames and evaluate the
               method quantitatively on the hold-out set of registered images.
               The supplementary video shows example sessions of our system at
               work.",
  month     =  jun,
  year      =  2015,
  url       = "http://dx.doi.org/10.1109/CVPR.2015.7299098",
  file      = "All Papers/My Library/Kononenko and Lempitsky 2015 - Learning to look up - Realtime monocular gaze correction using machine learning.pdf",
  doi       = "10.1109/CVPR.2015.7299098",
  isbn      =  9781467369640,
  language  = "en"
}

@INPROCEEDINGS{Murrugarra-Llerena2019-gq,
  title     = "{Cross-Modality Personalization for Retrieval}",
  author    = "Murrugarra-Llerena, Nils and Kovashka, Adriana",
  booktitle = "{2019 IEEE/CVF Conference on Computer Vision and Pattern
               Recognition (CVPR)}",
  publisher = "IEEE",
  address   = "Long Beach, CA, USA",
  pages     = "6422--6431",
  abstract  = "Existing captioning and gaze prediction approaches do not
               consider the multiple facets of personality that affect how a
               viewer extracts meaning from an image. While there are methods
               that consider personalized captioning, they do not consider
               personalized perception across modalities, i.e. how a person’s
               way of looking at an image (gaze) affects the way they describe
               it (captioning). In this work, we propose a model for modeling
               cross-modality personalized retrieval. In addition to modeling
               gaze and captions, we also explicitly model the personality of
               the users providing these samples. We incorporate constraints
               that encourage gaze and caption samples on the same image to be
               close in a learned space; we refer to this as content modeling.
               We also model style: we encourage samples provided by the same
               user to be close in a separate embedding space, regardless of the
               image on which they were provided. To leverage the complementary
               information that content and style constraints provide, we
               combine the embeddings from both networks. We show that our
               combined embeddings achieve better performance than existing
               approaches for cross-modal retrieval.",
  year      =  2019,
  url       = "https://ieeexplore.ieee.org/document/8953895/",
  file      = "All Papers/My Library/Murrugarra-Llerena and Kovashka 2019 - Cross-Modality Personalization for Retrieval.pdf",
  doi       = "10.1109/CVPR.2019.00659",
  isbn      =  9781728132938,
  language  = "en"
}

@INPROCEEDINGS{Deliege2021-ix,
  title     = "{SoccerNet-v2: A Dataset and Benchmarks for Holistic
               Understanding of Broadcast Soccer Videos}",
  author    = "Deliege, Adrien and Cioppa, Anthony and Giancola, Silvio and
               Seikavandi, Meisam J and Dueholm, Jacob V and Nasrollahi, Kamal
               and Ghanem, Bernard and Moeslund, Thomas B and Van Droogenbroeck,
               Marc",
  booktitle = "{2021 IEEE/CVF Conference on Computer Vision and Pattern
               Recognition Workshops (CVPRW)}",
  publisher = "IEEE",
  address   = "Nashville, TN, USA",
  pages     = "4503--4514",
  abstract  = "Understanding broadcast videos is a challenging task in computer
               vision, as it requires generic reasoning capabilities to
               appreciate the content offered by the video editing. In this
               work, we propose SoccerNet-v2, a novel large-scale corpus of
               manual annotations for the SoccerNet [24] video dataset, along
               with open challenges to encourage more research in soccer
               understanding and broadcast production. Speciﬁcally, we release
               around 300k annotations within SoccerNet’s 500 untrimmed
               broadcast soccer videos. We extend current tasks in the realm of
               soccer to include action spotting, camera shot segmentation with
               boundary detection, and we deﬁne a novel replay grounding task.
               For each task, we provide and discuss benchmark results,
               reproducible with our open-source adapted implementations of the
               most relevant works in the ﬁeld. SoccerNet-v2 is presented to the
               broader research community to help push computer vision closer to
               automatic solutions for more general video understanding and
               production purposes.",
  year      =  2021,
  url       = "https://ieeexplore.ieee.org/document/9523091/",
  file      = "All Papers/My Library/Deliege et al. 2021 - SoccerNet-v2 - A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos.pdf",
  doi       = "10.1109/CVPRW53098.2021.00508",
  isbn      =  9781665448994,
  language  = "en"
}

@INPROCEEDINGS{Wang2017-bi,
  title     = "{Using point cloud data to improve three dimensional gaze
               estimation}",
  author    = "Wang, Haofei and Antonelli, Marco and Shi, Bertram E",
  booktitle = "{2017 39th annual international conference of the IEEE
               engineering in medicine and biology society (EMBC)}",
  pages     = "795–798",
  abstract  = "This paper addresses the problem of estimating gaze location in
               the 3D environment using a remote eye tracker. Instead of relying
               only on data provided by the eye tracker, we investigate how to
               integrate gaze direction with the point-cloud-based
               representation of the scene provided by a Kinect sensor. The
               algorithm first combines the gaze vectors for the two eyes
               provided by the eye tracker into a single gaze vector emanating
               from a point in between the two eyes. The gaze target in the
               three dimensional environment is then identified by finding the
               point in the 3D point cloud that is closest to the gaze vector.
               Our experimental results demonstrate that the estimate of the
               gaze target location provided by this method is significantly
               better than that provided when considering gaze information
               alone. It is also better than two other methods for integrating
               point cloud information: (1) finding the 3D point closest to the
               gaze location as estimated by triangulating the gaze vectors from
               the two eyes, and (2) finding the 3D point with smallest average
               distance to the two gaze vectors considered individually. The
               proposed method has an average error of 1.7 cm in a workspace of
               25 × 23 × 24 cm located at a distance of 60 cm from the user.",
  month     =  jul,
  year      =  2017,
  url       = "http://dx.doi.org/10.1109/EMBC.2017.8036944",
  doi       = "10.1109/EMBC.2017.8036944"
}

@INPROCEEDINGS{Dinulescu2022-pf,
  title     = "{A Smart Bracelet Supporting Tactile Communication and
               Interaction}",
  author    = "Dinulescu, Stejara and Tummala, Neeli and Reardon, Gregory and
               Dandu, Bharat and Goetz, Dustin and Topp, Sven and Visell, Yon",
  booktitle = "{2022 {IEEE} Haptics Symposium ({HAPTICS})}",
  publisher = "ieeexplore.ieee.org",
  pages     = "1--7",
  abstract  = "The sense of touch can convey semantic and emotional information
               in social or computer-mediated interactions. Touch plays an
               essential role in communication with individuals affected by
               multiple sensory loss, many of whom use modes of touch
               communication that can be broadly described as tactile sign
               languages. Few technologies exist today to support such
               interactions. Here, we present a smart bracelet for facilitating
               tactile communication and interaction. The smart bracelet
               captures and analyzes vibrations that are elicited in the skin
               via touch gestures performed on the hand. We demonstrate the
               utility of this system for supporting communication via the
               Deafblind Manual alphabet, which is a tactile sign language. This
               smart bracelet can classify signed letters with greater than 90
               \% per-letter accuracy. These results show how existing modes of
               tactile communication can be integrated with information
               technologies. This work may furnish new paradigms for
               human-computer interaction via self- and interpersonal-touch
               contact.",
  month     =  mar,
  year      =  2022,
  url       = "http://dx.doi.org/10.1109/HAPTICS52432.2022.9765616",
  keywords  = "Performance evaluation;Wrist;Vibrations;Tactile
               sensors;Manuals;Gesture recognition;Assistive technologies",
  doi       = "10.1109/HAPTICS52432.2022.9765616",
  issn      = "2324-7355"
}

@INPROCEEDINGS{Dinulescu2022-hd,
  title     = "{A smart bracelet supporting tactile communication and
               interaction}",
  author    = "Dinulescu, Stejara and Tummala, Neeli and Reardon, Gregory and
               Dandu, Bharat and Goetz, Dustin and Topp, Sven and Visell, Yon",
  booktitle = "{2022 IEEE haptics symposium (HAPTICS)}",
  publisher = "ieeexplore.ieee.org",
  pages     = "1–7",
  abstract  = "The sense of touch can convey semantic and emotional information
               in social or computer-mediated interactions. Touch plays an
               essential role in communication with individuals affected by
               multiple sensory loss, many of whom use modes of touch
               communication that can be broadly described as tactile sign
               languages. Few technologies exist today to support such
               interactions. Here, we present a smart bracelet for facilitating
               tactile communication and interaction. The smart bracelet
               captures and analyzes vibrations that are elicited in the skin
               via touch gestures performed on the hand. We demonstrate the
               utility of this system for supporting communication via the
               Deafblind Manual alphabet, which is a tactile sign language. This
               smart bracelet can classify signed letters with greater than 90
               \% per-letter accuracy. These results show how existing modes of
               tactile communication can be integrated with information
               technologies. This work may furnish new paradigms for
               human-computer interaction via self- and interpersonal-touch
               contact.",
  month     =  mar,
  year      =  2022,
  url       = "http://dx.doi.org/10.1109/HAPTICS52432.2022.9765616",
  doi       = "10.1109/HAPTICS52432.2022.9765616"
}

@INPROCEEDINGS{Balakrishnan2021-fb,
  title     = "{Interaction of Spatial Computing In Augmented Reality}",
  author    = "Balakrishnan, S and Hameed, M Syed Shahul and Venkatesan, K and
               Aswin, G",
  booktitle = "{2021 7th International Conference on Advanced Computing and
               Communication Systems ({ICACCS})}",
  volume    =  1,
  pages     = "1900--1904",
  abstract  = "In the above certain years, the technology of spatial computing
               has recognized the environment growth of simulated reality by
               user interaction and extensive knowledge which assist the 3D
               canvas used space for user interfaces. Spatial computing is
               widely compatible with XR which is an Extended Reality. This acts
               itself as an umbrella phase for Augmented Reality (AR), Mixed
               Reality (MR) and Virtual Reality (VR). Moreover, we are looking
               into the deep concepts of AR in spatial computing. Augmented
               reality is a new medium of laminating and merging digital
               contents towards the real world. This medium provides individual
               assets connecting the physical-virtual world with steady control
               and enhances the overall environment reality of user's
               information. These are obtained based on mobile devices,
               projection devices and mounted displays that help in augmenting
               the information over real-life. In advance, AR is part of spatial
               augmented reality which is an assistive technology tool used for
               determining the completion time of task, speed range and quality
               of interactive computation. This paper delivers the effectiveness
               of Augmented Reality (AR) with possible combination of
               technologies and applications from a user's point of view.",
  month     =  mar,
  year      =  2021,
  url       = "http://dx.doi.org/10.1109/ICACCS51430.2021.9442010",
  keywords  = "Three-dimensional displays;Spatial augmented reality;Glass;User
               interfaces;Tools;Internet;X reality;Spatial computing;augmented
               reality;virtual reality;mixed reality",
  doi       = "10.1109/ICACCS51430.2021.9442010",
  issn      = "2575-7288"
}

@INPROCEEDINGS{Balakrishnan2021-zx,
  title     = "{Interaction of spatial computing in augmented reality}",
  author    = "Balakrishnan, S and Hameed, M Syed Shahul and Venkatesan, K and
               Aswin, G",
  booktitle = "{2021 7th international conference on advanced computing and
               communication systems (ICACCS)}",
  volume    =  1,
  pages     = "1900–1904",
  abstract  = "In the above certain years, the technology of spatial computing
               has recognized the environment growth of simulated reality by
               user interaction and extensive knowledge which assist the 3D
               canvas used space for user interfaces. Spatial computing is
               widely compatible with XR which is an Extended Reality. This acts
               itself as an umbrella phase for Augmented Reality (AR), Mixed
               Reality (MR) and Virtual Reality (VR). Moreover, we are looking
               into the deep concepts of AR in spatial computing. Augmented
               reality is a new medium of laminating and merging digital
               contents towards the real world. This medium provides individual
               assets connecting the physical-virtual world with steady control
               and enhances the overall environment reality of user's
               information. These are obtained based on mobile devices,
               projection devices and mounted displays that help in augmenting
               the information over real-life. In advance, AR is part of spatial
               augmented reality which is an assistive technology tool used for
               determining the completion time of task, speed range and quality
               of interactive computation. This paper delivers the effectiveness
               of Augmented Reality (AR) with possible combination of
               technologies and applications from a user's point of view.",
  month     =  mar,
  year      =  2021,
  url       = "http://dx.doi.org/10.1109/ICACCS51430.2021.9442010",
  doi       = "10.1109/ICACCS51430.2021.9442010"
}

@INPROCEEDINGS{Kishi2015-ew,
  title     = "{Effective gazewriting with support of text copy and paste}",
  author    = "Kishi, Reo and Hayashi, Takahiro",
  booktitle = "{2015 IEEE/ACIS 14th International Conference on Computer and
               Information Science (ICIS)}",
  publisher = "IEEE",
  address   = "Las Vegas, NV, USA",
  pages     = "125--130",
  year      =  2015,
  url       = "http://ieeexplore.ieee.org/document/7166581/",
  file      = "All Papers/My Library/Kishi and Hayashi 2015 - Effective gazewriting with support of text copy and paste.pdf",
  doi       = "10.1109/ICIS.2015.7166581",
  isbn      =  9781479986798
}

@INPROCEEDINGS{Giger2014-qm,
  title     = "{Gaze correction with a single webcam}",
  author    = "Giger, Dominik and Bazin, Jean-Charles and Kuster, Claudia and
               Popa, Tiberiu and Gross, Markus",
  booktitle = "{2014 IEEE international conference on multimedia and expo
               (ICME)}",
  publisher = "IEEE",
  address   = "Chengdu",
  pages     = "1–6",
  abstract  = "Eye contact is a critical aspect of human communication. However,
               when talking over a video conferencing system, such as Skype, it
               is not possible for users to have eye contact when looking at the
               conversation partner's face displayed on the screen. This is due
               to the location disparity between the video conferencing window
               and the camera. This issue has been tackled by expensive high-end
               systems or hybrid depth+color cameras, but such equipment is
               still largely unavailable at the consumer level and on platforms
               such as laptops or tablets. In contrast, we propose a gaze
               correction method that needs just a single webcam. We apply
               recent shape deformation techniques to generate a 3D face model
               that matches the user's face. We then render a gaze-corrected
               version of this face model and seamlessly insert it into the
               original image. Experiments on real data and various platforms
               confirm the validity of the approach and demonstrate that the
               visual quality of our results is at least equivalent to those
               obtained by state-of-the-art methods requiring additional
               equipment.",
  month     =  jul,
  year      =  2014,
  url       = "http://dx.doi.org/10.1109/ICME.2014.6890306",
  file      = "All Papers/My Library/Giger et al. 2014 - Gaze correction with a single webcam.pdf",
  doi       = "10.1109/ICME.2014.6890306",
  isbn      =  9781479947614,
  language  = "en"
}

@INPROCEEDINGS{Amirpour2019-ha,
  title     = "{Design and optimization of a multi-DOF hand exoskeleton for
               haptic applications}",
  author    = "Amirpour, E and Savabi, M and Saboukhi, A and Gorji, M Rahimi and
               Ghafarirad, H and Fesharakifard, R and Rezaei, S Mehdi",
  booktitle = "{2019 7th international conference on robotics and mechatronics
               (ICRoM)}",
  publisher = "IEEE",
  address   = "Tehran, Iran",
  pages     = "270–275",
  abstract  = "This paper describes the design and kinematic optimization of a
               novel, underactuated, linkage driven exoskeleton mechanism to
               provide haptic force feedback to the index and thumb fingers.
               Existing exoskeletons are either not compatible with the human
               hand kinematic chain or so heavy that they cannot be compatibly
               installed on a human hand, which in turn affects the natural
               motion of the hand. In order to improve functionalities, the
               design of a novel (HEXON11Hand Exoskeleton of New technologies
               research center) based on a multi-criteria optimization is
               proposed to simultaneously maximize applied force to the finger
               and workspace of the attached exoskeleton. The optimization
               procedure consists of both the Perpendicular Impact Force (PIF)
               and the Global Isotropy Index (GIl), considering worst-case
               collision avoidance. Finally, the exoskeleton mechanism
               functionalities within the achieved link length through the
               optimization procedure are validated, and design is proposed for
               further fabrication.",
  month     =  nov,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/ICRoM48714.2019.9071884",
  doi       = "10.1109/ICRoM48714.2019.9071884",
  isbn      =  9781728166049,
  language  = "en"
}

@INPROCEEDINGS{Cui2005-ze,
  title     = "{Tracking multiple people using laser and vision}",
  author    = "Cui, Jinshi and Zha, Hongbin and Zhao, Huijing and Shibasaki,
               Ryosuke",
  booktitle = "{Intelligent robots and systems, 2005. (IROS 2005). 2005 IEEE/RSJ
               international conference on}",
  publisher = "unknown",
  pages     = "2116–2121",
  abstract  = "We present a novel system that aims at reliably detecting and
               tracking multiple people in an open area. Multiple single-row
               laser scanners and one video camera are utilized. Feet trajectory
               tracking based on registration of distance information from
               multiple laser scanners and visual body region tracking based on
               color histogram are combined in a Bayesian formulation. Results
               from tests in a real environment are reported to demonstrate that
               the system can detect and track multiple people simultaneously
               with reliable and real-time performance.",
  month     =  "9~" # jun,
  year      =  2005,
  url       = "https://www.researchgate.net/publication/224623165_Tracking_multiple_people_using_laser_and_vision",
  doi       = "10.1109/IROS.2005.1545159"
}

@INPROCEEDINGS{Mohan2018-ko,
  title     = "{DualGaze: Addressing the Midas Touch Problem in Gaze Mediated VR
               Interaction}",
  author    = "Mohan, Pallavi and Goh, Wooi Boon and Fu, Chi-Wing and Yeung,
               Sai-Kit",
  booktitle = "{2018 IEEE International Symposium on Mixed and Augmented Reality
               Adjunct (ISMAR-Adjunct)}",
  publisher = "IEEE",
  address   = "Munich, Germany",
  pages     = "79--84",
  abstract  = "With the increasing acceptance of eye tracking as a viable
               interaction method for Virtual Reality (VR) headsets, thoughtful
               gaze interaction methods need to be carefully designed to meet
               common challenges such as the Midas Touch problem, where users
               unintentionally select onscreen objects by gazing upon them. This
               paper presents DualGaze, a novel interaction method in which
               users perform a distinctive two-step gaze gesture for object
               selection. Once users gaze upon an object that they wish to
               select, a confirmation flag pops up next to the object at a
               location where the users’ gaze just passed through. This
               trajectory-adaptive flag placement strategy reduces the chance of
               unintentional confirmation by requiring a returning gaze back to
               the flag. We conducted a user study to compare the accuracy and
               selection speed of DualGaze and the popular gaze fixation method
               on a simple gaze-typing task. Our results show that DualGaze is
               significantly more accurate while maintaining a comparable
               selection speed that was observed to improve with familiarity of
               use.",
  year      =  2018,
  url       = "https://ieeexplore.ieee.org/document/8699248/",
  file      = "All Papers/My Library/Mohan et al. 2018 - DualGaze - Addressing the Midas Touch Problem in Gaze Mediated VR Interaction.pdf",
  doi       = "10.1109/ISMAR-Adjunct.2018.00039",
  isbn      =  9781538675922,
  language  = "en"
}

@INPROCEEDINGS{Takahashi2021-tp,
  title     = "{A japanese character flick-input interface for entering text in
               {VR}}",
  author    = "Takahashi, Ryota and Shirai, Shizuka and Orlosky, Jason and
               Uranishi, Yuki and Takemura, Haruo",
  booktitle = "{2021 IEEE international symposium on mixed and augmented reality
               adjunct (ISMAR-Adjunct)}",
  publisher = "ieeexplore.ieee.org",
  pages     = "251–253",
  abstract  = "This paper presents new flick input interfaces to improve the
               usability of Japanese character input in VR space. We designed
               three different interfaces called TouchFlick, EyeFlick, and
               RoundFlick, which make use of various controller interactions and
               eye gestures. To investigate the effectiveness of these methods,
               we compared them with a conventional VR QWERTY keyboard with
               ray-based selection. We found that TouchFlick was significantly
               faster and RoundFlick had a significantly lower error rate for
               experienced users. On the other hand, the input efficiency was
               the same as that of the conventional method for those who had no
               experience with flick input. Regarding subjective evaluation,
               there was no significant difference in usability and mental
               workload.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1109/ISMAR-Adjunct54149.2021.00058",
  file      = "All Papers/My Library/Takahashi et al. 2021 - A japanese character flick-input interface for entering text in VR.pdf",
  doi       = "10.1109/ISMAR-Adjunct54149.2021.00058"
}

@INPROCEEDINGS{Ader2023-se,
  title     = "{Extended Reality, Augmented Users, and Design Implications for
               Virtual Learning Environments}",
  author    = "Ader, Lilian G Motti and Crowley, Katie and Kuhn, Stefan and
               Caraffini, Fabio and Altındağ, Turgay and Colreavy-Donnelly,
               Simon",
  booktitle = "{2023 IEEE International Symposium on Technology and Society
               (ISTAS)}",
  pages     = "1--8",
  abstract  = "Recently, new technologies have been developed that allow
               physical and virtual space to converge. We reviewed a range of
               innovative technologies that enable immersive and 3D interaction,
               which we believe are of particular interest to apply Universal
               Design for Learning (UDL) principles in teaching and learning
               practices, with a particular interest in higher education. We
               found limitations related to hardware and interactive systems
               that cannot be customised to meet the needs of all different
               students and some users may be marginalised. We draw attention to
               problems that lead to the risk of intentional exclusion while
               highlighting relevant inclusion opportunities. The main
               contribution of this paper is to present a selection of use cases
               to discuss how software applications can be designed to meet the
               guidelines for UDL, and improve the accessibility of 3D
               interaction for innovative Virtual Learning Environments (VLEs).",
  month     =  sep,
  year      =  2023,
  url       = "https://ieeexplore.ieee.org/abstract/document/10305947",
  file      = "All Papers/My Library/Ader et al. 2023 - Extended Reality, Augmented Users, and Design Implications for Virtual Learning Environments.pdf",
  doi       = "10.1109/ISTAS57930.2023.10305947"
}

@INPROCEEDINGS{Kim2019-mb,
  title     = "{Sharing Emotion by Displaying a Partner Near the Gaze Point in a
               Telepresence System}",
  author    = "Kim, Seungwon and Billinghurst, Mark and Lee, Gun and Norman,
               Mitchell and Huang, Weidong and He, Jian",
  booktitle = "{2019 23rd International Conference in Information Visualization
               -- Part {II}}",
  publisher = "ieeexplore.ieee.org",
  pages     = "86--91",
  abstract  = "In this paper, we explore the effect of showing a remote partner
               close to user gaze point in a teleconferencing system. We
               implemented a gaze following function in a teleconferencing
               system and investigate if this improves the user's feeling of
               emotional interdependence. We developed a prototype system that
               shows a remote partner close to the user's current gaze point and
               conducted a user study comparing it to a condition displaying the
               partner fixed in the corner of a screen. Our results showed that
               showing a partner close to their gaze point helped users feel a
               higher level of emotional interdependence. In addition, we
               compared the effect of our method between small and big displays,
               but there was no significant difference in the users' feeling of
               emotional interdependence even though the big display was
               preferred.",
  month     =  jul,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/IV-2.2019.00026",
  keywords  = "
               Face;Prototypes;Visualization;Collaboration;Filtering;Australia;Teleconferencing;sharing
               emotion;gaze interaction;Teleconference
               system;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1109/IV-2.2019.00026"
}

@INPROCEEDINGS{Kim2019-vz,
  title     = "{Sharing emotion by displaying a partner near the gaze point in a
               telepresence system}",
  author    = "Kim, Seungwon and Billinghurst, Mark and Lee, Gun and Norman,
               Mitchell and Huang, Weidong and He, Jian",
  booktitle = "{2019 23rd international conference in information visualization
               – part II}",
  publisher = "ieeexplore.ieee.org",
  pages     = "86–91",
  abstract  = "In this paper, we explore the effect of showing a remote partner
               close to user gaze point in a teleconferencing system. We
               implemented a gaze following function in a teleconferencing
               system and investigate if this improves the user's feeling of
               emotional interdependence. We developed a prototype system that
               shows a remote partner close to the user's current gaze point and
               conducted a user study comparing it to a condition displaying the
               partner fixed in the corner of a screen. Our results showed that
               showing a partner close to their gaze point helped users feel a
               higher level of emotional interdependence. In addition, we
               compared the effect of our method between small and big displays,
               but there was no significant difference in the users' feeling of
               emotional interdependence even though the big display was
               preferred.",
  month     =  jul,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/IV-2.2019.00026",
  file      = "All Papers/My Library/Kim et al. 2019 - Sharing emotion by displaying a partner near the gaze point in a telepresence system.pdf",
  doi       = "10.1109/IV-2.2019.00026"
}

@INPROCEEDINGS{Kim2020-zr,
  title     = "{A User Study of a Gaze Window User Interface}",
  author    = "Kim, Seungwon and Billinghurst, Mark and Lee, Gun and Huang,
               Weidong",
  booktitle = "{2020 24th International Conference Information Visualisation
               ({IV})}",
  pages     = "700--703",
  abstract  = "We have proposed a proof of concept of a gaze window interface. A
               gaze window interface uses the user's gaze point to show the
               relevant content nearby the current gaze point. A system of a
               gaze window interface has been implemented that allows users to
               manage looking at multiple objects of interest on the screen
               while the system is responsive to the user's gaze points with
               relevant information shown in the gaze window. We also conducted
               user studies to investigate the effects of the interface on user
               performance and behaviors. In this paper, we report on a user
               study in which the gaze window was compared with a mouse window
               for supporting single user data entry tasks. We describe details
               of the study design and conduction and present the results.",
  month     =  sep,
  year      =  2020,
  url       = "http://dx.doi.org/10.1109/IV51561.2020.00121",
  keywords  = "Tracking;Data visualization;Mice;Task analysis;Gaze window;mouse
               window;eye tracking;eye movement;user
               interface;uist2022-gaze-design",
  doi       = "10.1109/IV51561.2020.00121",
  issn      = "2375-0138"
}

@INPROCEEDINGS{Kim2020-jt,
  title     = "{A user study of a gaze window user interface}",
  author    = "Kim, Seungwon and Billinghurst, Mark and Lee, Gun and Huang,
               Weidong",
  booktitle = "{2020 24th international conference information visualisation
               (IV)}",
  pages     = "700–703",
  abstract  = "We have proposed a proof of concept of a gaze window interface. A
               gaze window interface uses the user's gaze point to show the
               relevant content nearby the current gaze point. A system of a
               gaze window interface has been implemented that allows users to
               manage looking at multiple objects of interest on the screen
               while the system is responsive to the user's gaze points with
               relevant information shown in the gaze window. We also conducted
               user studies to investigate the effects of the interface on user
               performance and behaviors. In this paper, we report on a user
               study in which the gaze window was compared with a mouse window
               for supporting single user data entry tasks. We describe details
               of the study design and conduction and present the results.",
  month     =  sep,
  year      =  2020,
  url       = "http://dx.doi.org/10.1109/IV51561.2020.00121",
  file      = "All Papers/My Library/Kim et al. 2020 - A user study of a gaze window user interface.pdf",
  doi       = "10.1109/IV51561.2020.00121"
}

@INPROCEEDINGS{Eng2013-xq,
  title     = "{Gaze correction for 3D tele-immersive communication system}",
  author    = "Eng, Wei Yong and Min, Dongbo and Nguyen, Viet-Anh and Lu,
               Jiangbo and Do, Minh N",
  booktitle = "{IVMSP 2013}",
  publisher = "IEEE",
  address   = "Seoul, Korea (South)",
  pages     = "1–4",
  abstract  = "The lack of eye contact between participants in a
               tele-conferencing makes nonverbal communication unnatural and
               ineffective. A lot of research has focused on correcting the user
               gaze for a natural communication. Most of prior solutions require
               expensive and bulky hardware, or incorporate a complicated
               algorithm causing inefficiency and deployment. In this paper, we
               propose an effective and efficient gaze correction solution for a
               3D tele-conferencing system in a single color/depth camera
               set-up. A raw depth map is first refined using the corresponding
               color image. Then, both color and depth data of the participant
               are accurately segmented. A novel view is synthesized in the
               location of the display screen which coincides with the user
               gaze. Stereoscopic views, i.e. virtual left and right images, can
               also be generated for 3D immersive conferencing, and are
               displayed in a 3D monitor with 3D virtual background scenes.
               Finally, to handle large hole regions that often occur in the
               view synthesized with a single color camera, we propose a simple
               yet robust hole filling technique that works in real-time. This
               novel inpainting method can effectively reconstruct missing parts
               of the synthesized image under various challenging situations.
               Our proposed system works in real-time on a single core CPU
               without requiring dedicated hardware, including data acquisition,
               post-processing, rendering, and so on.",
  month     =  jun,
  year      =  2013,
  url       = "http://dx.doi.org/10.1109/IVMSPW.2013.6611942",
  doi       = "10.1109/IVMSPW.2013.6611942",
  isbn      =  9781467358583,
  language  = "en"
}

@INPROCEEDINGS{Takahashi2017-zo,
  title     = "{Video conference environment using representative eye-gaze
               motion of remote participants}",
  author    = "Takahashi, Genki and Takeuchi, Yugo",
  booktitle = "{2017 26th {IEEE} International Symposium on Robot and Human
               Interactive Communication ({RO-MAN})}",
  pages     = "6--11",
  abstract  = "In multi-participant conversation, speaker and listener pay
               attention to all participants to carry out smooth turn-taking.
               However, in video conversation it is difficult to perceive the
               timing of when to take turns talking because participants cannot
               perceive each other's eye-gaze. Consequently, one participant
               often misses the sign of when another person wants to speak, and
               continues to speak longer than appropriate. Considering this
               problem, in this study we propose a video-conference environment
               in which a robot symbolizes and unifies the eye-gaze motion of
               several participants. As a result of an experiment to investigate
               its efficacy and usefulness, it was suggested that the proposed
               video-conference environment suppresses inappropriate speech
               compared with a general video-conversation environment.",
  month     =  aug,
  year      =  2017,
  url       = "http://dx.doi.org/10.1109/ROMAN.2017.8172272",
  keywords  = "Games;Robot sensing systems;Cameras;Proposals;Speech;Microphones",
  doi       = "10.1109/ROMAN.2017.8172272",
  issn      = "1944-9437"
}

@INPROCEEDINGS{Takahashi2017-ss,
  title     = "{Video conference environment using representative eye-gaze
               motion of remote participants}",
  author    = "Takahashi, Genki and Takeuchi, Yugo",
  booktitle = "{2017 26th IEEE international symposium on robot and human
               interactive communication (RO-MAN)}",
  pages     = "6–11",
  abstract  = "In multi-participant conversation, speaker and listener pay
               attention to all participants to carry out smooth turn-taking.
               However, in video conversation it is difficult to perceive the
               timing of when to take turns talking because participants cannot
               perceive each other's eye-gaze. Consequently, one participant
               often misses the sign of when another person wants to speak, and
               continues to speak longer than appropriate. Considering this
               problem, in this study we propose a video-conference environment
               in which a robot symbolizes and unifies the eye-gaze motion of
               several participants. As a result of an experiment to investigate
               its efficacy and usefulness, it was suggested that the proposed
               video-conference environment suppresses inappropriate speech
               compared with a general video-conversation environment.",
  month     =  aug,
  year      =  2017,
  url       = "http://dx.doi.org/10.1109/ROMAN.2017.8172272",
  doi       = "10.1109/ROMAN.2017.8172272"
}

@ARTICLE{Tao2019-cx,
  title    = "{Digital Twin in Industry: {State-of-the-Art}}",
  author   = "Tao, Fei and Zhang, He and Liu, Ang and Nee, A Y C",
  journal  = "IEEE Transactions on Industrial Informatics",
  volume   =  15,
  number   =  4,
  pages    = "2405--2415",
  abstract = "Digital twin (DT) is one of the most promising enabling
              technologies for realizing smart manufacturing and Industry 4.0.
              DTs are characterized by the seamless integration between the
              cyber and physical spaces. The importance of DTs is increasingly
              recognized by both academia and industry. It has been almost 15
              years since the concept of the DT was initially proposed. To date,
              many DT applications have been successfully implemented in
              different industries, including product design, production,
              prognostics and health management, and some other fields. However,
              at present, no paper has focused on the review of DT applications
              in industry. In an effort to understand the development and
              application of DTs in industry, this paper thoroughly reviews the
              state-of-the-art of the DT research concerning the key components
              of DTs, the current development of DTs, and the major DT
              applications in industry. This paper also outlines the current
              challenges and some possible directions for future work.",
  month    =  apr,
  year     =  2019,
  url      = "http://dx.doi.org/10.1109/TII.2018.2873186",
  keywords = "Data models;Industries;Computational modeling;Smart
              manufacturing;Patents;History;Data fusion;digital twin
              (DT);industry application;modeling",
  doi      = "10.1109/TII.2018.2873186",
  issn     = "1941-0050"
}

@ARTICLE{Tao2019-iv,
  title    = "{Digital twin in industry: State-of-the-Art}",
  author   = "Tao, Fei and Zhang, He and Liu, Ang and Nee, A Y C",
  journal  = "IEEE Transactions on Industrial Informatics",
  volume   =  15,
  number   =  4,
  pages    = "2405–2415",
  abstract = "Digital twin (DT) is one of the most promising enabling
              technologies for realizing smart manufacturing and Industry 4.0.
              DTs are characterized by the seamless integration between the
              cyber and physical spaces. The importance of DTs is increasingly
              recognized by both academia and industry. It has been almost 15
              years since the concept of the DT was initially proposed. To date,
              many DT applications have been successfully implemented in
              different industries, including product design, production,
              prognostics and health management, and some other fields. However,
              at present, no paper has focused on the review of DT applications
              in industry. In an effort to understand the development and
              application of DTs in industry, this paper thoroughly reviews the
              state-of-the-art of the DT research concerning the key components
              of DTs, the current development of DTs, and the major DT
              applications in industry. This paper also outlines the current
              challenges and some possible directions for future work.",
  month    =  apr,
  year     =  2019,
  url      = "http://dx.doi.org/10.1109/TII.2018.2873186",
  doi      = "10.1109/TII.2018.2873186",
  issn     = "1941-0050"
}

@ARTICLE{Mukherjee2015-su,
  title     = "{Deep Head Pose: {Gaze-Direction} Estimation in Multimodal Video}",
  author    = "Mukherjee, Sankha S and Robertson, Neil Martin",
  journal   = "IEEE Transactions on Multimedia",
  publisher = "ieeexplore.ieee.org",
  volume    =  17,
  number    =  11,
  pages     = "2094--2107",
  abstract  = "In this paper we present a convolutional neural network
               (CNN)-based model for human head pose estimation in
               low-resolution multi-modal RGB-D data. We pose the problem as one
               of classification of human gazing direction. We further fine-tune
               a regressor based on the learned deep classifier. Next we combine
               the two models (classification and regression) to estimate
               approximate regression confidence. We present state-of-the-art
               results in datasets that span the range of high-resolution human
               robot interaction (close up faces plus depth information) data to
               challenging low resolution outdoor surveillance data. We build
               upon our robust head-pose estimation and further introduce a new
               visual attention model to recover interaction with the
               environment . Using this probabilistic model, we show that many
               higher level scene understanding like human-human/scene
               interaction detection can be achieved. Our solution runs in
               real-time on commercial hardware.",
  month     =  nov,
  year      =  2015,
  url       = "http://dx.doi.org/10.1109/TMM.2015.2482819",
  file      = "All Papers/Other/Mukherjee and Robertson 2015 - Deep Head Pose - Gaze-Direction Estimation in Multimodal Video.pdf",
  keywords  = "Head;Estimation;Human computer interaction;Surveillance;Magnetic
               heads;Visualization;Image resolution;Convolutional neural
               networks (CNNs);deep learning;gaze direction;head-pose;RGB-D",
  doi       = "10.1109/TMM.2015.2482819",
  issn      = "1941-0077"
}

@ARTICLE{Mukherjee2015-le,
  title    = "{Deep head pose: Gaze-Direction estimation in multimodal video}",
  author   = "Mukherjee, Sankha S and Robertson, Neil Martin",
  journal  = "IEEE Transactions on Multimedia",
  volume   =  17,
  number   =  11,
  pages    = "2094–2107",
  abstract = "In this paper we present a convolutional neural network
              (CNN)-based model for human head pose estimation in low-resolution
              multi-modal RGB-D data. We pose the problem as one of
              classification of human gazing direction. We further fine-tune a
              regressor based on the learned deep classifier. Next we combine
              the two models (classification and regression) to estimate
              approximate regression confidence. We present state-of-the-art
              results in datasets that span the range of high-resolution human
              robot interaction (close up faces plus depth information) data to
              challenging low resolution outdoor surveillance data. We build
              upon our robust head-pose estimation and further introduce a new
              visual attention model to recover interaction with the environment
              . Using this probabilistic model, we show that many higher level
              scene understanding like human-human/scene interaction detection
              can be achieved. Our solution runs in real-time on commercial
              hardware.",
  month    =  nov,
  year     =  2015,
  url      = "http://dx.doi.org/10.1109/TMM.2015.2482819",
  file     = "All Papers/My Library/Mukherjee and Robertson 2015 - Deep head pose - Gaze-Direction estimation in multimodal video.pdf",
  doi      = "10.1109/TMM.2015.2482819",
  issn     = "1941-0077"
}

@ARTICLE{Otsuka2018-as,
  title    = "{Behavioral Analysis of Kinetic Telepresence for Small Symmetric
              {Group-to-Group} Meetings}",
  author   = "Otsuka, Kazuhiro",
  journal  = "IEEE Transactions on Multimedia",
  volume   =  20,
  number   =  6,
  pages    = "1432--1447",
  abstract = "Nonverbal behavior analysis revealed the effect of MMSpace, a
              kinetic telepresence developed for social telepresence, on small
              symmetric group-to-group conversations. MMSpace consists of
              kinetic avatars, equipped with flat projection screen panels as
              faces, that can change their pose and position automatically to
              mirror the remote user's head motions. The advantage is the
              realistic kinetic expression of human head movements, which form
              gestures like nodding and indicate the focus of visual attention,
              through the use of four degree-of-freedom low-latency precision
              actuators. Another feature is the support of eye contact among
              remote participants, which is made possible by the avatar's
              kinetic pose changes and by adaptive camera selection for
              orienting the user's face toward the remote addressee. Its
              limitation is its room-scale infrastructure and restricted
              participant positions. Targeting a symmetric 2 $\times$ 2 setting,
              participants' nonverbal behaviors, including gaze directions and
              head gestures, were compared among three conditions, MMSpace
              with/without physical motions and face-to-face settings. There was
              a significant difference between the conditions in terms of the
              duration of glance/mutual glances, total gaze transition time,
              amount of head gesturing, and co-occurrences of head gestures in
              the remote participants. The results indicate that the avatar's
              physical motion can elicit longer (mutual) glances with a shorter
              total transition time and more (co-)occurrences of head gestures,
              and it makes MMSpace -based conversations closer, in terms of
              these nonverbal statistics, to face-to-face ones compared with
              those of a static version of MMSpace without physical motion.",
  month    =  jun,
  year     =  2018,
  url      = "http://dx.doi.org/10.1109/TMM.2017.2771396",
  file     = "All Papers/Other/Otsuka 2018 - Behavioral Analysis of Kinetic Telepresence for Small Symmetric Group-to-Group Meetings.pdf",
  keywords = "Kinetic
              theory;Avatars;Cameras;Monitoring;Mirrors;Visualization;Teleconferencing;multimodal
              interactions;non-verbal communication;and telepresence",
  doi      = "10.1109/TMM.2017.2771396",
  issn     = "1941-0077"
}

@ARTICLE{Pi2020-zd,
  title    = "{Dynamic Bayesian Adjustment of Dwell Time for Faster Eye Typing}",
  author   = "Pi, Jimin and Koljonen, Paul A and Hu, Yong and Shi, Bertram E",
  journal  = "IEEE transactions on neural systems and rehabilitation
              engineering: a publication of the IEEE Engineering in Medicine and
              Biology Society",
  volume   =  28,
  number   =  10,
  pages    = "2315--2324",
  abstract = "Eye typing is a hands-free method of human computer interaction,
              which is especially useful for people with upper limb
              disabilities. Users select a desired key by gazing at it in an
              image of a keyboard for a fixed dwell time. There is a tradeoff in
              selecting the dwell time; shorter dwell times lead to errors due
              to unintentional selections, while longer dwell times lead to a
              slow input speed. We propose to speed up eye typing while
              maintaining low error by dynamically adjusting the dwell time for
              each letter based on the past input history. More likely letters
              are assigned shorter dwell times. Our method is based on a
              probabilistic generative model of gaze, which enables us to assign
              dwell times using a principled model that requires only a few free
              parameters. We evaluate our model on both able-bodied subjects and
              subjects with a spinal cord injury (SCI). Compared to the standard
              dwell time method, we find consistent increases in typing speed in
              both cases. e.g., 41.8\% faster typing for able-bodied subjects on
              a transcription task and 49.5\% faster typing for SCI subjects in
              a chatbot task. We observed more inter-subject variability for SCI
              subjects.",
  month    =  oct,
  year     =  2020,
  url      = "http://dx.doi.org/10.1109/TNSRE.2020.3016747",
  keywords = "prj-gaze-shorthand",
  doi      = "10.1109/TNSRE.2020.3016747",
  pmid     =  32795970,
  issn     = "1534-4320,1558-0210",
  language = "en"
}

@ARTICLE{Pi2020-sq,
  title    = "{Dynamic bayesian adjustment of dwell time for faster eye typing}",
  author   = "Pi, Jimin and Koljonen, Paul A and Hu, Yong and Shi, Bertram E",
  journal  = "IEEE transactions on neural systems and rehabilitation
              engineering: a publication of the IEEE Engineering in Medicine and
              Biology Society",
  volume   =  28,
  number   =  10,
  pages    = "2315–2324",
  abstract = "Eye typing is a hands-free method of human computer interaction,
              which is especially useful for people with upper limb
              disabilities. Users select a desired key by gazing at it in an
              image of a keyboard for a fixed dwell time. There is a tradeoff in
              selecting the dwell time; shorter dwell times lead to errors due
              to unintentional selections, while longer dwell times lead to a
              slow input speed. We propose to speed up eye typing while
              maintaining low error by dynamically adjusting the dwell time for
              each letter based on the past input history. More likely letters
              are assigned shorter dwell times. Our method is based on a
              probabilistic generative model of gaze, which enables us to assign
              dwell times using a principled model that requires only a few free
              parameters. We evaluate our model on both able-bodied subjects and
              subjects with a spinal cord injury (SCI). Compared to the standard
              dwell time method, we find consistent increases in typing speed in
              both cases. e.g., 41.8\% faster typing for able-bodied subjects on
              a transcription task and 49.5\% faster typing for SCI subjects in
              a chatbot task. We observed more inter-subject variability for SCI
              subjects.",
  month    =  oct,
  year     =  2020,
  url      = "http://dx.doi.org/10.1109/TNSRE.2020.3016747",
  doi      = "10.1109/TNSRE.2020.3016747",
  issn     = "1534-4320,1558-0210",
  language = "en"
}

@ARTICLE{Pacchierotti2017-kr,
  title    = "{Wearable haptic systems for the fingertip and the hand: Taxonomy,
              review, and perspectives}",
  author   = "Pacchierotti, Claudio and Sinclair, Stephen and Solazzi,
              Massimiliano and Frisoli, Antonio and Hayward, Vincent and
              Prattichizzo, Domenico",
  journal  = "IEEE transactions on haptics",
  volume   =  10,
  number   =  4,
  pages    = "580–600",
  abstract = "In the last decade, we have witnessed a drastic change in the form
              factor of audio and vision technologies, from heavy and grounded
              machines to lightweight devices that naturally fit our bodies.
              However, only recently, haptic systems have started to be designed
              with wearability in mind. The wearability of haptic systems
              enables novel forms of communication, cooperation, and integration
              between humans and machines. Wearable haptic interfaces are
              capable of communicating with the human wearers during their
              interaction with the environment they share, in a natural and yet
              private way. This paper presents a taxonomy and review of wearable
              haptic systems for the fingertip and the hand, focusing on those
              systems directly addressing wearability challenges. The paper also
              discusses the main technological and design challenges for the
              development of wearable haptic interfaces, and it reports on the
              future perspectives of the field. Finally, the paper includes two
              tables summarizing the characteristics and features of the most
              representative wearable haptic systems for the fingertip and the
              hand.",
  month    =  oct,
  year     =  2017,
  url      = "http://dx.doi.org/10.1109/TOH.2017.2689006",
  file     = "All Papers/My Library/Pacchierotti et al. 2017 - Wearable haptic systems for the fingertip and the hand - Taxonomy, review, and perspectives.pdf",
  doi      = "10.1109/TOH.2017.2689006",
  issn     = "1939-1412,2329-4051",
  language = "en"
}

@ARTICLE{Beck2013-hh,
  title    = "{Immersive group-to-group telepresence}",
  author   = "Beck, Stephan and Kunert, André and Kulik, Alexander and
              Froehlich, Bernd",
  journal  = "IEEE transactions on visualization and computer graphics",
  volume   =  19,
  number   =  4,
  pages    = "616--625",
  abstract = "We present a novel immersive telepresence system that allows
              distributed groups of users to meet in a shared virtual 3D world.
              Our approach is based on two coupled projection-based multi-user
              setups, each providing multiple users with perspectively correct
              stereoscopic images. At each site the users and their local
              interaction space are continuously captured using a cluster of
              registered depth and color cameras. The captured 3D information is
              transferred to the respective other location, where the remote
              participants are virtually reconstructed. We explore the use of
              these virtual user representations in various interaction
              scenarios in which local and remote users are face-to-face,
              side-by-side or decoupled. Initial experiments with distributed
              user groups indicate the mutual understanding of pointing and
              tracing gestures independent of whether they were performed by
              local or remote participants. Our users were excited about the new
              possibilities of jointly exploring a virtual city, where they
              relied on a world-in-miniature metaphor for mutual awareness of
              their respective locations.",
  month    =  apr,
  year     =  2013,
  url      = "http://dx.doi.org/10.1109/TVCG.2013.33",
  doi      = "10.1109/TVCG.2013.33",
  pmid     =  23428446,
  issn     = "1077-2626,1941-0506",
  language = "en"
}

@ARTICLE{Beck2013-gl,
  title    = "{Immersive group-to-group telepresence}",
  author   = "Beck, Stephan and Kunert, André and Kulik, Alexander and
              Froehlich, Bernd",
  journal  = "IEEE transactions on visualization and computer graphics",
  volume   =  19,
  number   =  4,
  pages    = "616–625",
  abstract = "We present a novel immersive telepresence system that allows
              distributed groups of users to meet in a shared virtual 3D world.
              Our approach is based on two coupled projection-based multi-user
              setups, each providing multiple users with perspectively correct
              stereoscopic images. At each site the users and their local
              interaction space are continuously captured using a cluster of
              registered depth and color cameras. The captured 3D information is
              transferred to the respective other location, where the remote
              participants are virtually reconstructed. We explore the use of
              these virtual user representations in various interaction
              scenarios in which local and remote users are face-to-face,
              side-by-side or decoupled. Initial experiments with distributed
              user groups indicate the mutual understanding of pointing and
              tracing gestures independent of whether they were performed by
              local or remote participants. Our users were excited about the new
              possibilities of jointly exploring a virtual city, where they
              relied on a world-in-miniature metaphor for mutual awareness of
              their respective locations.",
  month    =  apr,
  year     =  2013,
  url      = "http://dx.doi.org/10.1109/TVCG.2013.33",
  file     = "All Papers/My Library/Beck et al. 2013 - Immersive group-to-group telepresence.pdf",
  doi      = "10.1109/TVCG.2013.33",
  issn     = "1077-2626,1941-0506",
  language = "en"
}

@ARTICLE{Kurzhals2016-vo,
  title     = "{Gaze stripes: Image-based visualization of eye tracking data}",
  author    = "Kurzhals, Kuno and Hlawatsch, Marcel and Heimerl, Florian and
               Burch, Michael and Ertl, Thomas and Weiskopf, Daniel",
  journal   = "IEEE transactions on visualization and computer graphics",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  22,
  number    =  1,
  pages     = "1005--1014",
  abstract  = "We present a new visualization approach for displaying eye
               tracking data from multiple participants. We aim to show the
               spatio-temporal data of the gaze points in the context of the
               underlying image or video stimulus without occlusion. Our
               technique, denoted as gaze stripes, does not require the explicit
               definition of areas of interest but directly uses the image data
               around the gaze points, similar to thumbnails for images. A gaze
               stripe consists of a sequence of such gaze point images, oriented
               along a horizontal timeline. By displaying multiple aligned gaze
               stripes, it is possible to analyze and compare the viewing
               behavior of the participants over time. Since the analysis is
               carried out directly on the image data, expensive post-processing
               or manual annotation are not required. Therefore, not only
               patterns and outliers in the participants' scanpaths can be
               detected, but the context of the stimulus is available as well.
               Furthermore, our approach is especially well suited for dynamic
               stimuli due to the non-aggregated temporal mapping. Complementary
               views, i.e., markers, notes, screenshots, histograms, and results
               from automatic clustering, can be added to the visualization to
               display analysis results. We illustrate the usefulness of our
               technique on static and dynamic stimuli. Furthermore, we discuss
               the limitations and scalability of our approach in comparison to
               established visualization techniques.",
  month     =  jan,
  year      =  2016,
  url       = "https://consensus.apphttps://consensus.app/papers/gaze-stripes-imagebased-visualization-tracking-data-kurzhals/c6323bf13caf511e883aa9ead7b3698f/?extracted-answer=Gaze+stripes+display+gaze+points+in+the+context+of+an+image+or+video+stimulus%2C+allowing+for+analysis+and+comparison+of+viewing+behavior+over+time.&q=the+dataset+of+gaze+point+and+video+QA&copilot=on&lang=en",
  doi       = "10.1109/TVCG.2015.2468091",
  pmid      =  26529744,
  issn      = "1077-2626,1941-0506",
  language  = "en"
}

@ARTICLE{Gupta2016-is,
  title     = "{Do you see what I see? The effect of gaze tracking on task space
               remote collaboration}",
  author    = "Gupta, Kunal and Lee, Gun A and Billinghurst, Mark",
  journal   = "IEEE transactions on visualization and computer graphics",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  22,
  number    =  11,
  pages     = "2413--2422",
  abstract  = "We present results from research exploring the effect of sharing
               virtual gaze and pointing cues in a wearable interface for remote
               collaboration. A local worker wears a Head-mounted Camera,
               Eye-tracking camera and a Head-Mounted Display and shares video
               and virtual gaze information with a remote helper. The remote
               helper can provide feedback using a virtual pointer on the live
               video view. The prototype system was evaluated with a formal user
               study. Comparing four conditions, (1) NONE (no cue), (2) POINTER,
               (3) EYE-TRACKER and (4) BOTH (both pointer and eye-tracker cues),
               we observed that the task completion performance was best in the
               BOTH condition with a significant difference of POINTER and
               EYETRACKER individually. The use of eye-tracking and a pointer
               also significantly improved the co-presence felt between the
               users. We discuss the implications of this research and the
               limitations of the developed system that could be improved in
               further work.",
  month     =  nov,
  year      =  2016,
  url       = "https://consensus.apphttps://consensus.app/papers/what-effect-gaze-tracking-task-space-remote-collaboration-gupta/0d737f0895e151a1867a9598f05b5597/?q=eye+tracking+research+on+remote+conversation",
  doi       = "10.1109/TVCG.2016.2593778",
  pmid      =  27479970,
  issn      = "1077-2626,1941-0506",
  language  = "en"
}

@ARTICLE{Goc2018-it,
  title    = "{Dynamic composite data physicalization using wheeled
              micro-robots}",
  author   = "Goc, Mathieu Le and Perin, Charles and Follmer, Sean and Fekete,
              Jean-Daniel and Dragicevic, Pierre",
  journal  = "IEEE transactions on visualization and computer graphics",
  abstract = "This paper introduces dynamic composite physicalizations, a new
              class of physical visualizations that use collections of
              self-propelled objects to represent data. Dynamic composite
              physicalizations can be used both to give physical form to
              well-known interactive visualization techniques, and to explore
              new visualizations and interaction paradigms. We first propose a
              design space characterizing composite physicalizations based on
              previous work in the fields of Information Visualization and Human
              Computer Interaction. We illustrate dynamic composite
              physicalizations in two scenarios demonstrating potential benefits
              for collaboration and decision making, as well as new
              opportunities for physical interaction. We then describe our
              implementation using wheeled micro-robots capable of locating
              themselves and sensing user input, before discussing limitations
              and opportunities for future work.",
  month    =  "20~" # aug,
  year     =  2018,
  url      = "http://dx.doi.org/10.1109/TVCG.2018.2865159",
  file     = "All Papers/My Library/Goc et al. 2018 - Dynamic composite data physicalization using wheeled micro-robots.pdf",
  doi      = "10.1109/TVCG.2018.2865159",
  issn     = "1077-2626,1941-0506",
  language = "en"
}

@ARTICLE{Wang2021-ja,
  title    = "{Realtime and Accurate {3D} Eye Gaze Capture with {DCNN-Based}
              Iris and Pupil Segmentation}",
  author   = "Wang, Zhiyong and Chai, Jinxiang and Xia, Shihong",
  journal  = "IEEE transactions on visualization and computer graphics",
  volume   =  27,
  number   =  1,
  pages    = "190--203",
  abstract = "This paper presents a realtime and accurate method for 3D eye gaze
              tracking with a monocular RGB camera. Our key idea is to train a
              deep convolutional neural network(DCNN) that automatically
              extracts the iris and pupil pixels of each eye from input images.
              To achieve this goal, we combine the power of Unet [1] and
              Squeezenet [2] to train an efficient convolutional neural network
              for pixel classification. In addition, we track the 3D eye gaze
              state in the Maximum A Posteriori (MAP) framework, which
              sequentially searches for the most likely state of the 3D eye gaze
              at each frame. When eye blinking occurs, the eye gaze tracker can
              obtain an inaccurate result. We further extend the convolutional
              neural network for eye close detection in order to improve the
              robustness and accuracy of the eye gaze tracker. Our system runs
              in realtime on desktop PCs and smart phones. We have evaluated our
              system on live videos and Internet videos, and our results
              demonstrate that the system is robust and accurate for various
              genders, races, lighting conditions, poses, shapes and facial
              expressions. A comparison against Wang et al. [3] shows that our
              method advances the state of the art in 3D eye tracking using a
              single RGB camera.",
  month    =  jan,
  year     =  2021,
  url      = "http://dx.doi.org/10.1109/TVCG.2019.2938165",
  keywords = "Three-dimensional displays;Gaze
              tracking;Iris;Cameras;Convolutional neural nets;Image
              reconstruction;Videos;3D eye gaze tracking;convolutional neural
              network;facial capture",
  doi      = "10.1109/TVCG.2019.2938165",
  issn     = "1077-2626,1941-0506"
}

@ARTICLE{Wang2021-vm,
  title    = "{Realtime and accurate 3D eye gaze capture with DCNN-Based iris
              and pupil segmentation}",
  author   = "Wang, Zhiyong and Chai, Jinxiang and Xia, Shihong",
  journal  = "IEEE transactions on visualization and computer graphics",
  volume   =  27,
  number   =  1,
  pages    = "190–203",
  abstract = "This paper presents a realtime and accurate method for 3D eye gaze
              tracking with a monocular RGB camera. Our key idea is to train a
              deep convolutional neural network(DCNN) that automatically
              extracts the iris and pupil pixels of each eye from input images.
              To achieve this goal, we combine the power of Unet [1] and
              Squeezenet [2] to train an efficient convolutional neural network
              for pixel classification. In addition, we track the 3D eye gaze
              state in the Maximum A Posteriori (MAP) framework, which
              sequentially searches for the most likely state of the 3D eye gaze
              at each frame. When eye blinking occurs, the eye gaze tracker can
              obtain an inaccurate result. We further extend the convolutional
              neural network for eye close detection in order to improve the
              robustness and accuracy of the eye gaze tracker. Our system runs
              in realtime on desktop PCs and smart phones. We have evaluated our
              system on live videos and Internet videos, and our results
              demonstrate that the system is robust and accurate for various
              genders, races, lighting conditions, poses, shapes and facial
              expressions. A comparison against Wang et al. [3] shows that our
              method advances the state of the art in 3D eye tracking using a
              single RGB camera.",
  month    =  jan,
  year     =  2021,
  url      = "http://dx.doi.org/10.1109/TVCG.2019.2938165",
  file     = "All Papers/My Library/Wang et al. 2021 - Realtime and accurate 3D eye gaze capture with DCNN-Based iris and pupil segmentation.pdf",
  doi      = "10.1109/TVCG.2019.2938165",
  issn     = "1077-2626"
}

@INPROCEEDINGS{Otsuka2016-wx,
  title     = "{MMSpace: Kinetically-augmented telepresence for small
               group-to-group conversations}",
  author    = "Otsuka, Kazuhiro",
  booktitle = "{2016 IEEE virtual reality (VR)}",
  publisher = "IEEE",
  address   = "Greenville, SC, USA",
  pages     = "19–28",
  abstract  = "A novel research prototype, called MMSpace, was developed for
               realistic social telepresence in small group-to-group
               conversations. MMSpace consists of kinetic display avatars, which
               can change pose and position by automatically mirroring the
               remote user's head motions. To fully explore its potential beyond
               previous alternatives, MMSpace has the following novel features.
               First, it targets symmetric group-to-group telepresence. Second,
               the kinetic avatars of MMSpace can produce highly accurate, low
               latency, and silent physical motions, by using
               4-Degree-of-Freedom (DoF) direct-drive actuators, and they can
               express a wide range of natural human behaviors like head
               gestures and changing attitudes, as well as indicating the focus
               of attention. Third, MMSpace supports eye contact between every
               pair of participants, by integrating i) directional visual
               attention cues indicated by avatar's kinetic pose change, ii)
               line-of-sight alignment among the positions of persons, avatars
               and cameras, and iii) attention-based camera switching, which
               allows an avatar to always show its owner's face looking directly
               toward the person that the avatar's owner is looking at. The
               prototype targets the 2 × 2 setting, and subjective evaluations
               based on group discussions indicate that the kinetic display
               avatar is superior to static displays in various aspects
               including gaze-awareness, eye-contact, perception of other
               nonverbal behaviors, mutual understanding, and sense of
               telepresence.",
  month     =  mar,
  year      =  2016,
  url       = "http://dx.doi.org/10.1109/VR.2016.7504684",
  file      = "All Papers/My Library/Otsuka 2016 - MMSpace - Kinetically-augmented telepresence for small group-to-group conversations.pdf",
  doi       = "10.1109/VR.2016.7504684",
  isbn      =  9781509008360,
  language  = "en"
}

@INPROCEEDINGS{Yi2022-mu,
  title     = "{GazeDock: Gaze-Only Menu Selection in Virtual Reality using
               Auto-Triggering Peripheral Menu}",
  author    = "Yi, Xin and Lu, Yiqin and Cai, Ziyin and Wu, Zihan and Wang,
               Yuntao and Shi, Yuanchun",
  booktitle = "{2022 IEEE Conference on Virtual Reality and 3D User Interfaces
               (VR)}",
  publisher = "IEEE",
  address   = "Christchurch, New Zealand",
  pages     = "832--842",
  year      =  2022,
  url       = "https://ieeexplore.ieee.org/document/9756763/",
  file      = "All Papers/My Library/Yi et al. 2022 - GazeDock - Gaze-Only Menu Selection in Virtual Reality using Auto-Triggering Peripheral Menu.pdf",
  doi       = "10.1109/VR51125.2022.00105",
  isbn      =  9781665496179
}

@INPROCEEDINGS{Zhang2021-tt,
  title     = "{Saliency Prediction with External Knowledge}",
  author    = "Zhang, Yifeng and Jiang, Ming and Zhao, Qi",
  booktitle = "{2021 IEEE Winter Conference on Applications of Computer Vision
               (WACV)}",
  publisher = "IEEE",
  address   = "Waikoloa, HI, USA",
  pages     = "484--493",
  abstract  = "The last decades have seen great progress in saliency prediction,
               with the success of deep neural networks that are able to encode
               high-level semantics. Yet, while humans have the innate
               capability in leveraging their knowledge to decide where to look
               (e.g. people pay more attention to familiar faces such as
               celebrities), saliency prediction models have only been trained
               with large eye-tracking datasets. This work proposes to bridge
               this gap by explicitly incorporating external knowledge for
               saliency models as humans do. We develop networks that learn to
               highlight regions by incorporating prior knowledge of semantic
               relationships, be it general or domain-speciﬁc, depending on the
               task of interest. At the core of the method is a new Graph
               Semantic Saliency Network (GraSSNet) that constructs a graph that
               encodes semantic relationships learned from external knowledge. A
               Spatial Graph Attention Network is then developed to update
               saliency features based on the learned graph. Experiments show
               that the proposed model learns to predict saliency from the
               external knowledge and outperforms the state-of-the-art on four
               saliency benchmarks.",
  year      =  2021,
  url       = "https://ieeexplore.ieee.org/document/9423113/",
  file      = "All Papers/My Library/Zhang et al. 2021 - Saliency Prediction with External Knowledge.pdf",
  doi       = "10.1109/WACV48630.2021.00053",
  isbn      =  9781665404778,
  language  = "en"
}

@ARTICLE{Dembinsky2024-zd,
  title     = "{Gaze Generation for Avatars Using {GANs}}",
  author    = "Dembinsky, David and Watanabe, Ko and Dengel, Andreas and
               Ishimaru, Shoya",
  journal   = "IEEE access: practical innovations, open solutions",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  12,
  pages     = "101536--101548",
  year      =  2024,
  url       = "https://ieeexplore.ieee.org/abstract/document/10601689/",
  file      = "All Papers/Other/Dembinsky et al. 2024 - Gaze Generation for Avatars Using GANs.pdf",
  doi       = "10.1109/access.2024.3430835",
  issn      = "2169-3536"
}

@ARTICLE{Yoshida2024-vj,
  title     = "{Towards AI-mediated avatar-based telecommunication:
               Investigating visual impression of switching between user- and
               AI-controlled avatars in video chat}",
  author    = "Yoshida, Shigeo and Koyama, Yuki and Ushiku, Yoshitaka",
  journal   = "IEEE access: practical innovations, open solutions",
  publisher = "ieeexplore.ieee.org",
  year      =  2024,
  url       = "https://ieeexplore.ieee.org/abstract/document/10632136/",
  file      = "All Papers/Other/Yoshida et al. 2024 - Towards AI-mediated avatar-based telecommunication ... on of switching between user- and AI-controlled avatars in video chat.pdf",
  doi       = "10.1109/access.2024.3441233",
  issn      = "2169-3536"
}

@INPROCEEDINGS{Lu2016-ph,
  title     = "{Chunk-wise face model based gaze correction in conversational
               videos with single camera}",
  author    = "Lu, Jichuan and Tao, Xiaoming and Dong, Linhao and Ge, Ning",
  booktitle = "{2016 International Conference on Computer, Information and
               Telecommunication Systems ({CITS})}",
  publisher = "IEEE",
  address   = "Kunming, China",
  pages     = "1--5",
  abstract  = "A novel model-based gaze correction scheme to independently build
               the eye models, the nose model and the mouth model independently
               so that the occlusion problem can be solved effectively is
               proposed. Eye contact is one of critical aspects of video
               conference. In current video conference systems, an important
               problem is the shortage of eye contact. This is due to the
               direction disparity between the normal of focus plane of camera
               and the interlocutor's gaze. Currently, the state-of-the-method
               performs well on gaze correction with only single webcam.
               However, this method that applied to the sequences with face
               obscured has shown unsatisfying results. In this paper, we
               propose a novel model-based gaze correction scheme to
               independently build the eye models, the nose model and the mouth
               model independently so that the occlusion problem can be solved
               effectively. To be more specific, by rotating the aforementioned
               four models at a pre-detected displacement, the degree of gaze
               correction can be obtained. The facial appearance that is
               occluded will not be rotated, which will prevent the occluding
               object from being distorted. Experiment results show that, under
               different parts of occluding objects, our method can achieve
               better performance on gaze correction than the current method in
               terms of perceived quality in two scenes usually appearing in
               conversational videos.",
  month     =  jul,
  year      =  2016,
  url       = "http://dx.doi.org/10.1109/cits.2016.7546437",
  doi       = "10.1109/cits.2016.7546437",
  isbn      =  9781509006908,
  language  = "en"
}

@INPROCEEDINGS{Kononenko2015-cr,
  title     = "{Learning to look up: Realtime monocular gaze correction using
               machine learning}",
  author    = "Kononenko, Daniil and Lempitsky, Victor",
  booktitle = "{2015 {IEEE} Conference on Computer Vision and Pattern
               Recognition ({CVPR})}",
  publisher = "IEEE",
  address   = "Boston, MA, USA",
  pages     = "4667--4675",
  abstract  = "This work revisits the well-known problem of gaze correction in
               videoconferencing and presents a solution based on supervised
               machine learning that is fast and able to redirect the gaze of a
               previously unseen person by the same angular difference as in the
               training set. We revisit the well-known problem of gaze
               correction and present a solution based on supervised machine
               learning. At training time, our system observes pairs of images,
               where each pair contains the face of the same person with a fixed
               angular difference in gaze direction. It then learns to
               synthesize the second image of a pair from the first one. After
               learning, the system gets the ability to redirect the gaze of a
               previously unseen person by the same angular difference as in the
               training set. Unlike many previous solutions to gaze problem in
               videoconferencing, ours is purely monocular, i.e. it does not
               require any hardware apart from an in-built web-camera of a
               laptop. Being based on efficient machine learning predictors such
               as decision forests, the system is fast (runs in real-time on a
               single core of a modern laptop). In the paper, we demonstrate
               results on a variety of videoconferencing frames and evaluate the
               method quantitatively on the hold-out set of registered images.
               The supplementary video shows example sessions of our system at
               work.",
  month     =  jun,
  year      =  2015,
  url       = "http://dx.doi.org/10.1109/cvpr.2015.7299098",
  doi       = "10.1109/cvpr.2015.7299098",
  isbn      =  9781467369640,
  language  = "en"
}

@INPROCEEDINGS{Jeni2016-yc,
  title     = "{Person-independent 3D gaze estimation using face frontalization}",
  author    = "Jeni, Laszlo A and Cohn, Jeffrey F",
  booktitle = "{2016 IEEE conference on computer vision and pattern recognition
               workshops (CVPRW)}",
  publisher = "IEEE",
  abstract  = "From a 2D image of a person's face (a) a dense, part-based 3D
               deformable model is aligned (b) to reconstruct a partial frontal
               view of the face (c). Binary features are extracted around eye
               and pupil markers (d) for the 3D gaze calculation (e).",
  month     =  jun,
  year      =  2016,
  url       = "https://openaccess.thecvf.com/content_cvpr_2016_workshops/w18/papers/Jeni_Person-Independent_3D_Gaze_CVPR_2016_paper.pdf",
  file      = "All Papers/My Library/Jeni and Cohn 2016 - Person-independent 3D gaze estimation using face frontalization.pdf",
  doi       = "10.1109/cvprw.2016.104",
  isbn      =  9781509014378
}

@INPROCEEDINGS{Alnajar2013-uc,
  title     = "{Calibration-free gaze estimation using human gaze patterns}",
  author    = "Alnajar, Fares and Gevers, Theo and Valenti, Roberto and
               Ghebreab, Sennay",
  booktitle = "{2013 IEEE international conference on computer vision}",
  publisher = "IEEE",
  pages     = "137–144",
  abstract  = "… - calibrate gaze estimators based on gaze patterns obtained
               from other viewers. Our method is based on the observation that
               the gaze pat… that to help disabled users (eg eye typing ) [2]. …",
  month     =  dec,
  year      =  2013,
  url       = "https://www.cv-foundation.org/openaccess/content_iccv_2013/html/Alnajar_Calibration-Free_Gaze_Estimation_2013_ICCV_paper.html",
  file      = "All Papers/My Library/Alnajar et al. 2013 - Calibration-free gaze estimation using human gaze patterns.pdf",
  doi       = "10.1109/iccv.2013.24",
  isbn      =  9781479928408
}

@INPROCEEDINGS{He2019-rl,
  title     = "{Photo-realistic monocular gaze redirection using generative
               adversarial networks}",
  author    = "He, Zhe and Spurr, Adrian and Zhang, Xucong and Hilliges, Otmar",
  booktitle = "{2019 {IEEE/CVF} International Conference on Computer Vision
               ({ICCV})}",
  publisher = "IEEE",
  address   = "Seoul, Korea (South)",
  pages     = "6931--6940",
  abstract  = "This work presents a novel method by leveraging generative
               adversarial training to synthesize an eye image conditioned on a
               target gaze direction that outperforms state-of-the-art
               approaches in terms of both image quality and redirection
               precision. Gaze redirection is the task of changing the gaze to a
               desired direction for a given monocular eye patch image. Many
               applications such as videoconferencing, films, games, and
               generation of training data for gaze estimation require
               redirecting the gaze, without distorting the appearance of the
               area surrounding the eye and while producing photo-realistic
               images. Existing methods lack the ability to generate
               perceptually plausible images. In this work, we present a novel
               method to alleviate this problem by leveraging generative
               adversarial training to synthesize an eye image conditioned on a
               target gaze direction. Our method ensures perceptual similarity
               and consistency of synthesized images to the real images.
               Furthermore, a gaze estimation loss is used to control the gaze
               direction accurately. To attain high-quality images, we
               incorporate perceptual and cycle consistency losses into our
               architecture. In extensive evaluations we show that the proposed
               method outperforms state-of-the-art approaches in terms of both
               image quality and redirection precision. Finally, we show that
               generated images can bring significant improvement for the gaze
               estimation task if used to augment real training data.",
  month     =  oct,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/iccv.2019.00703",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1109/iccv.2019.00703",
  isbn      =  9781728148038,
  language  = "en"
}

@INPROCEEDINGS{He2019-sd,
  title     = "{Photo-realistic monocular gaze redirection using generative
               adversarial networks}",
  author    = "He, Zhe and Spurr, Adrian and Zhang, Xucong and Hilliges, Otmar",
  booktitle = "{2019 IEEE/CVF international conference on computer vision
               (ICCV)}",
  publisher = "IEEE",
  address   = "Seoul, Korea (South)",
  pages     = "6931–6940",
  abstract  = "This work presents a novel method by leveraging generative
               adversarial training to synthesize an eye image conditioned on a
               target gaze direction that outperforms state-of-the-art
               approaches in terms of both image quality and redirection
               precision. Gaze redirection is the task of changing the gaze to a
               desired direction for a given monocular eye patch image. Many
               applications such as videoconferencing, films, games, and
               generation of training data for gaze estimation require
               redirecting the gaze, without distorting the appearance of the
               area surrounding the eye and while producing photo-realistic
               images. Existing methods lack the ability to generate
               perceptually plausible images. In this work, we present a novel
               method to alleviate this problem by leveraging generative
               adversarial training to synthesize an eye image conditioned on a
               target gaze direction. Our method ensures perceptual similarity
               and consistency of synthesized images to the real images.
               Furthermore, a gaze estimation loss is used to control the gaze
               direction accurately. To attain high-quality images, we
               incorporate perceptual and cycle consistency losses into our
               architecture. In extensive evaluations we show that the proposed
               method outperforms state-of-the-art approaches in terms of both
               image quality and redirection precision. Finally, we show that
               generated images can bring significant improvement for the gaze
               estimation task if used to augment real training data.",
  month     =  oct,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/iccv.2019.00703",
  file      = "All Papers/My Library/He et al. 2019 - Photo-realistic monocular gaze redirection using generative adversarial networks.pdf",
  doi       = "10.1109/iccv.2019.00703",
  isbn      =  9781728148038,
  language  = "en"
}

@INPROCEEDINGS{Giger2014-ox,
  title     = "{Gaze correction with a single webcam}",
  author    = "Giger, Dominik and Bazin, Jean-Charles and Kuster, Claudia and
               Popa, Tiberiu and Gross, Markus",
  booktitle = "{2014 {IEEE} International Conference on Multimedia and Expo
               ({ICME})}",
  publisher = "IEEE",
  address   = "Chengdu",
  pages     = "1--6",
  abstract  = "This work applies recent shape deformation techniques to generate
               a 3D face model that matches the user's face and renders a
               gaze-corrected version of this face model, which is then
               seamlessly inserted into the original image. Eye contact is a
               critical aspect of human communication. However, when talking
               over a video conferencing system, such as Skype, it is not
               possible for users to have eye contact when looking at the
               conversation partner's face displayed on the screen. This is due
               to the location disparity between the video conferencing window
               and the camera. This issue has been tackled by expensive high-end
               systems or hybrid depth+color cameras, but such equipment is
               still largely unavailable at the consumer level and on platforms
               such as laptops or tablets. In contrast, we propose a gaze
               correction method that needs just a single webcam. We apply
               recent shape deformation techniques to generate a 3D face model
               that matches the user's face. We then render a gaze-corrected
               version of this face model and seamlessly insert it into the
               original image. Experiments on real data and various platforms
               confirm the validity of the approach and demonstrate that the
               visual quality of our results is at least equivalent to those
               obtained by state-of-the-art methods requiring additional
               equipment.",
  month     =  jul,
  year      =  2014,
  url       = "http://dx.doi.org/10.1109/icme.2014.6890306",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1109/icme.2014.6890306",
  isbn      =  9781479947614,
  language  = "en"
}

@INPROCEEDINGS{Amirpour2019-wx,
  title     = "{Design and optimization of a {multi-DOF} hand exoskeleton for
               haptic applications}",
  author    = "Amirpour, E and Savabi, M and Saboukhi, A and Gorii, M Rahimi and
               Ghafarirad, H and Fesharakifard, R and Rezaei, S Mehdi",
  booktitle = "{2019 7th International Conference on Robotics and Mechatronics
               ({ICRoM})}",
  publisher = "IEEE",
  address   = "Tehran, Iran",
  pages     = "270--275",
  abstract  = "The design and kinematic optimization of a novel, underactuated,
               linkage driven exoskeleton mechanism to provide haptic force
               feedback to the index and thumb fingers is described. This paper
               describes the design and kinematic optimization of a novel,
               underactuated, linkage driven exoskeleton mechanism to provide
               haptic force feedback to the index and thumb fingers. Existing
               exoskeletons are either not compatible with the human hand
               kinematic chain or so heavy that they cannot be compatibly
               installed on a human hand, which in turn affects the natural
               motion of the hand. In order to improve functionalities, the
               design of a novel (HEXON11Hand Exoskeleton of New technologies
               research center) based on a multi-criteria optimization is
               proposed to simultaneously maximize applied force to the finger
               and workspace of the attached exoskeleton. The optimization
               procedure consists of both the Perpendicular Impact Force (PIF)
               and the Global Isotropy Index (GIl), considering worst-case
               collision avoidance. Finally, the exoskeleton mechanism
               functionalities within the achieved link length through the
               optimization procedure are validated, and design is proposed for
               further fabrication.",
  month     =  nov,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/icrom48714.2019.9071884",
  doi       = "10.1109/icrom48714.2019.9071884",
  isbn      =  9781728166049,
  language  = "en"
}

@INPROCEEDINGS{Roth2018-dn,
  title     = "{Effects of hybrid and synthetic social gaze in avatar-mediated
               interactions}",
  author    = "Roth, Daniel and Kullmann, Peter and Bente, Gary and Gall,
               Dominik and Latoschik, Marc Erich",
  booktitle = "{2018 IEEE International Symposium on Mixed and Augmented Reality
               Adjunct (ISMAR-Adjunct)}",
  publisher = "IEEE",
  pages     = "103--108",
  month     =  oct,
  year      =  2018,
  url       = "https://ieeexplore.ieee.org/abstract/document/8699286/",
  doi       = "10.1109/ismar-adjunct.2018.00044",
  isbn      = "9781538675922,9781538675939"
}

@INPROCEEDINGS{Eng2013-ln,
  title     = "{Gaze correction for {3D} tele-immersive communication system}",
  author    = "Eng, Wei Yong and Min, Dongbo and Nguyen, Viet-Anh and Lu,
               Jiangbo and Do, Minh N",
  booktitle = "{{IVMSP} 2013}",
  publisher = "IEEE",
  address   = "Seoul, Korea (South)",
  pages     = "1--4",
  abstract  = "An effective and efficient gaze correction solution for a 3D
               tele-conferencing system in a single color/depth camera set-up
               that works in real-time on a single core CPU without requiring
               dedicated hardware, including data acquisition, post-processing,
               rendering, and so on. The lack of eye contact between
               participants in a tele-conferencing makes nonverbal communication
               unnatural and ineffective. A lot of research has focused on
               correcting the user gaze for a natural communication. Most of
               prior solutions require expensive and bulky hardware, or
               incorporate a complicated algorithm causing inefficiency and
               deployment. In this paper, we propose an effective and efficient
               gaze correction solution for a 3D tele-conferencing system in a
               single color/depth camera set-up. A raw depth map is first
               refined using the corresponding color image. Then, both color and
               depth data of the participant are accurately segmented. A novel
               view is synthesized in the location of the display screen which
               coincides with the user gaze. Stereoscopic views, i.e. virtual
               left and right images, can also be generated for 3D immersive
               conferencing, and are displayed in a 3D monitor with 3D virtual
               background scenes. Finally, to handle large hole regions that
               often occur in the view synthesized with a single color camera,
               we propose a simple yet robust hole filling technique that works
               in real-time. This novel inpainting method can effectively
               reconstruct missing parts of the synthesized image under various
               challenging situations. Our proposed system works in real-time on
               a single core CPU without requiring dedicated hardware, including
               data acquisition, post-processing, rendering, and so on.",
  month     =  jun,
  year      =  2013,
  url       = "http://dx.doi.org/10.1109/ivmspw.2013.6611942",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1109/ivmspw.2013.6611942",
  isbn      =  9781467358583,
  language  = "en"
}

@INPROCEEDINGS{Cinieri2020-wm,
  title     = "{Eye tracking and speech driven human-avatar emotion-based
               communication}",
  author    = "Cinieri, Saverio and Kapralos, Bill and Uribe-Quevedo, Alvaro and
               Lamberti, Fabrizio",
  booktitle = "{2020 IEEE 8th International Conference on Serious Games and
               Applications for Health (SeGAH)}",
  publisher = "IEEE",
  pages     = "1--5",
  month     =  aug,
  year      =  2020,
  url       = "https://ieeexplore.ieee.org/abstract/document/9201874/",
  doi       = "10.1109/segah49190.2020.9201874",
  isbn      = "9781728190426,9781728190433",
  issn      = "2573-3060,2330-5649"
}

@ARTICLE{Otsuka2018-oc,
  title     = "{Behavioral analysis of kinetic telepresence for small symmetric
               group-to-group meetings}",
  author    = "Otsuka, Kazuhiro",
  journal   = "IEEE Transactions on Multimedia",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  20,
  number    =  6,
  pages     = "1432--1447",
  abstract  = "The results indicate that the avatar's physical motion can elicit
               longer (mutual) glances with a shorter total transition time and
               more (co-)occurrences of head gestures, and it makes face-to-face
               conversations closer, in terms of these nonverbal statistics, to
               face- to-face ones compared with those of a static version of
               MMSpace without physical motion. Nonverbal behavior analysis
               revealed the effect of MMSpace, a kinetic telepresence developed
               for social telepresence, on small symmetric group-to-group
               conversations. MMSpace consists of kinetic avatars, equipped with
               flat projection screen panels as faces, that can change their
               pose and position automatically to mirror the remote user's head
               motions. The advantage is the realistic kinetic expression of
               human head movements, which form gestures like nodding and
               indicate the focus of visual attention, through the use of four
               degree-of-freedom low-latency precision actuators. Another
               feature is the support of eye contact among remote participants,
               which is made possible by the avatar's kinetic pose changes and
               by adaptive camera selection for orienting the user's face toward
               the remote addressee. Its limitation is its room-scale
               infrastructure and restricted participant positions. Targeting a
               symmetric 2 $\times$ 2 setting, participants' nonverbal
               behaviors, including gaze directions and head gestures, were
               compared among three conditions, MMSpace with/without physical
               motions and face-to-face settings. There was a significant
               difference between the conditions in terms of the duration of
               glance/mutual glances, total gaze transition time, amount of head
               gesturing, and co-occurrences of head gestures in the remote
               participants. The results indicate that the avatar's physical
               motion can elicit longer (mutual) glances with a shorter total
               transition time and more (co-)occurrences of head gestures, and
               it makes MMSpace -based conversations closer, in terms of these
               nonverbal statistics, to face-to-face ones compared with those of
               a static version of MMSpace without physical motion.",
  month     =  jun,
  year      =  2018,
  url       = "http://dx.doi.org/10.1109/tmm.2017.2771396",
  file      = "All Papers/Other/Otsuka 2018 - Behavioral analysis of kinetic telepresence for small symmetric group-to-group meetings.pdf",
  doi       = "10.1109/tmm.2017.2771396",
  issn      = "1520-9210,1941-0077",
  language  = "en"
}

@ARTICLE{Otsuka2018-wj,
  title    = "{Behavioral analysis of kinetic telepresence for small symmetric
              group-to-group meetings}",
  author   = "Otsuka, Kazuhiro",
  journal  = "IEEE Transactions on Multimedia",
  volume   =  20,
  number   =  6,
  pages    = "1432–1447",
  abstract = "The results indicate that the avatar's physical motion can elicit
              longer (mutual) glances with a shorter total transition time and
              more (co-)occurrences of head gestures, and it makes face-to-face
              conversations closer, in terms of these nonverbal statistics, to
              face- to-face ones compared with those of a static version of
              MMSpace without physical motion. Nonverbal behavior analysis
              revealed the effect of MMSpace, a kinetic telepresence developed
              for social telepresence, on small symmetric group-to-group
              conversations. MMSpace consists of kinetic avatars, equipped with
              flat projection screen panels as faces, that can change their pose
              and position automatically to mirror the remote user's head
              motions. The advantage is the realistic kinetic expression of
              human head movements, which form gestures like nodding and
              indicate the focus of visual attention, through the use of four
              degree-of-freedom low-latency precision actuators. Another feature
              is the support of eye contact among remote participants, which is
              made possible by the avatar's kinetic pose changes and by adaptive
              camera selection for orienting the user's face toward the remote
              addressee. Its limitation is its room-scale infrastructure and
              restricted participant positions. Targeting a symmetric 2 × 2
              setting, participants' nonverbal behaviors, including gaze
              directions and head gestures, were compared among three
              conditions, MMSpace with/without physical motions and face-to-face
              settings. There was a significant difference between the
              conditions in terms of the duration of glance/mutual glances,
              total gaze transition time, amount of head gesturing, and
              co-occurrences of head gestures in the remote participants. The
              results indicate that the avatar's physical motion can elicit
              longer (mutual) glances with a shorter total transition time and
              more (co-)occurrences of head gestures, and it makes MMSpace
              -based conversations closer, in terms of these nonverbal
              statistics, to face-to-face ones compared with those of a static
              version of MMSpace without physical motion.",
  month    =  jun,
  year     =  2018,
  url      = "http://dx.doi.org/10.1109/tmm.2017.2771396",
  file     = "All Papers/My Library/Otsuka 2018 - Behavioral analysis of kinetic telepresence for small symmetric group-to-group meetings.pdf",
  doi      = "10.1109/tmm.2017.2771396",
  issn     = "1520-9210,1941-0077",
  language = "en"
}

@INPROCEEDINGS{Otsuka2016-gq,
  title     = "{{MMSpace}: Kinetically-augmented telepresence for small
               group-to-group conversations}",
  author    = "Otsuka, Kazuhiro",
  booktitle = "{2016 {IEEE} Virtual Reality ({VR})}",
  publisher = "IEEE",
  address   = "Greenville, SC, USA",
  pages     = "19--28",
  abstract  = "The prototype targets the 2 $\times$ 2 setting, and subjective
               evaluations based on group discussions indicate that the kinetic
               display avatar is superior to static displays in various aspects
               including gaze-awareness, eye-contact, perception of other
               nonverbal behaviors, mutual understanding, and sense of
               telepresence. A novel research prototype, called MMSpace, was
               developed for realistic social telepresence in small
               group-to-group conversations. MMSpace consists of kinetic display
               avatars, which can change pose and position by automatically
               mirroring the remote user's head motions. To fully explore its
               potential beyond previous alternatives, MMSpace has the following
               novel features. First, it targets symmetric group-to-group
               telepresence. Second, the kinetic avatars of MMSpace can produce
               highly accurate, low latency, and silent physical motions, by
               using 4-Degree-of-Freedom (DoF) direct-drive actuators, and they
               can express a wide range of natural human behaviors like head
               gestures and changing attitudes, as well as indicating the focus
               of attention. Third, MMSpace supports eye contact between every
               pair of participants, by integrating i) directional visual
               attention cues indicated by avatar's kinetic pose change, ii)
               line-of-sight alignment among the positions of persons, avatars
               and cameras, and iii) attention-based camera switching, which
               allows an avatar to always show its owner's face looking directly
               toward the person that the avatar's owner is looking at. The
               prototype targets the 2 $\times$ 2 setting, and subjective
               evaluations based on group discussions indicate that the kinetic
               display avatar is superior to static displays in various aspects
               including gaze-awareness, eye-contact, perception of other
               nonverbal behaviors, mutual understanding, and sense of
               telepresence.",
  month     =  mar,
  year      =  2016,
  url       = "http://dx.doi.org/10.1109/vr.2016.7504684",
  doi       = "10.1109/vr.2016.7504684",
  isbn      =  9781509008360,
  language  = "en"
}

@INPROCEEDINGS{Otsuka2016-gk,
  title     = "{MMSpace: Kinetically-augmented telepresence for small
               group-to-group conversations}",
  author    = "Otsuka, Kazuhiro",
  booktitle = "{2016 IEEE Virtual Reality (VR)}",
  publisher = "IEEE",
  pages     = "19--28",
  month     =  mar,
  year      =  2016,
  url       = "https://ieeexplore.ieee.org/document/7504684",
  file      = "All Papers/Other/Otsuka 2016 - MMSpace - Kinetically-augmented telepresence for small group-to-group conversations.pdf",
  doi       = "10.1109/vr.2016.7504684",
  isbn      =  9781509008360,
  issn      = "2375-5334"
}

@INPROCEEDINGS{Mercado2020-rg,
  title     = "{Design and evaluation of interaction techniques dedicated to
               integrate encountered-type haptic displays in virtual
               environments}",
  author    = "Mercado, Victor and Marchai, Maud and Lecuyer, Anatole",
  booktitle = "{2020 {IEEE} Conference on Virtual Reality and {3D} User
               Interfaces ({VR})}",
  publisher = "IEEE",
  address   = "Atlanta, GA, USA",
  pages     = "230--238",
  abstract  = "A design framework based on several parameters defining the
               interactive process between user and ETHD (input, movement
               control, displacement and contact) is proposed and five
               techniques based on different ramifications of the design space
               framework were conceived, respectively named. Encountered-Type
               Haptic Displays (ETHDs) represent a field of haptic displays with
               the premise of not using any type of actuator directly in contact
               with the user skin, thus providing an alternative integration of
               haptic displays in virtual environments. In this paper, we
               present novel interaction techniques (ITs) dedicated to ETHDs.
               The techniques aim at addressing the issues commonly presented
               for these devices such as limited contact areas, lags and
               unexpected collisions with the user. First, our paper proposes a
               design framework based on several parameters defining the
               interactive process between user and ETHD (input, movement
               control, displacement and contact). Five techniques based on
               different ramifications of the design space framework were
               conceived, respectively named: Swipe, Drag, Clutch, Bubble and
               Follow. Then, a use-case scenario was designed to depict the
               usage of these techniques on the task of touching and coloring a
               wide, flat surface. Finally, a user study based on the coloring
               task was conducted to assess the performance and user experience
               for each IT. Results were in favor of Drag and Clutch techniques
               which are based on manual surface displacement, absolute position
               selection and intermittent contact interaction. Taken together
               our results and design methodology pave the way to the design of
               future ITs for ETHDs in virtual environments.",
  month     =  mar,
  year      =  2020,
  url       = "http://dx.doi.org/10.1109/vr46266.2020.1581077413070",
  doi       = "10.1109/vr46266.2020.1581077413070",
  isbn      =  9781728156088,
  language  = "en"
}

@INPROCEEDINGS{Mercado2020-zo,
  title     = "{Design and evaluation of interaction techniques dedicated to
               integrate encountered-type haptic displays in virtual
               environments}",
  author    = "Mercado, Victor and Marchai, Maud and Lecuyer, Anatole",
  booktitle = "{2020 IEEE conference on virtual reality and 3D user interfaces
               (VR)}",
  publisher = "IEEE",
  address   = "Atlanta, GA, USA",
  pages     = "230–238",
  abstract  = "A design framework based on several parameters defining the
               interactive process between user and ETHD (input, movement
               control, displacement and contact) is proposed and five
               techniques based on different ramifications of the design space
               framework were conceived, respectively named. Encountered-Type
               Haptic Displays (ETHDs) represent a field of haptic displays with
               the premise of not using any type of actuator directly in contact
               with the user skin, thus providing an alternative integration of
               haptic displays in virtual environments. In this paper, we
               present novel interaction techniques (ITs) dedicated to ETHDs.
               The techniques aim at addressing the issues commonly presented
               for these devices such as limited contact areas, lags and
               unexpected collisions with the user. First, our paper proposes a
               design framework based on several parameters defining the
               interactive process between user and ETHD (input, movement
               control, displacement and contact). Five techniques based on
               different ramifications of the design space framework were
               conceived, respectively named: Swipe, Drag, Clutch, Bubble and
               Follow. Then, a use-case scenario was designed to depict the
               usage of these techniques on the task of touching and coloring a
               wide, flat surface. Finally, a user study based on the coloring
               task was conducted to assess the performance and user experience
               for each IT. Results were in favor of Drag and Clutch techniques
               which are based on manual surface displacement, absolute position
               selection and intermittent contact interaction. Taken together
               our results and design methodology pave the way to the design of
               future ITs for ETHDs in virtual environments.",
  month     =  mar,
  year      =  2020,
  url       = "http://dx.doi.org/10.1109/vr46266.2020.1581077413070",
  file      = "All Papers/My Library/Mercado et al. 2020 - Design and evaluation of interaction techniques de ... to integrate encountered-type haptic displays in virtual environments.pdf",
  doi       = "10.1109/vr46266.2020.1581077413070",
  isbn      =  9781728156088,
  language  = "en"
}

@ARTICLE{Wood2018-ia,
  title     = "{{GazeDirector}: Fully articulated eye gaze redirection in video}",
  author    = "Wood, Erroll and Baltrušaitis, Tadas and Morency, Louis-Philippe
               and Robinson, Peter and Bulling, Andreas",
  journal   = "Computer graphics forum: journal of the European Association for
               Computer Graphics",
  publisher = "Wiley",
  volume    =  37,
  number    =  2,
  pages     = "217--225",
  abstract  = "We present GazeDirector, a new approach for eye gaze redirection
               that uses model-fitting. Our method first tracks the eyes by
               fitting a multi-part eye region model to video frames using
               analysis-by-synthesis, thereby recovering eye region shape,
               texture, pose, and gaze simultaneously. It then redirects gaze by
               1) warping the eyelids from the original image using a
               model-derived flow field, and 2) rendering and compositing
               synthesized 3D eyeballs onto the output image in a photorealistic
               manner. GazeDirector allows us to change where people are looking
               without person-specific training data, and with full
               articulation, i.e. we can precisely specify new gaze directions
               in 3D. Quantitatively, we evaluate both model-fitting and gaze
               synthesis, with experiments for gaze estimation and redirection
               on the Columbia gaze dataset. Qualitatively, we compare
               GazeDirector against recent work on gaze redirection, showing
               better results especially for large redirection angles. Finally,
               we demonstrate gaze redirection on YouTube videos by introducing
               new 3D gaze targets and by manipulating visual behavior.",
  month     =  may,
  year      =  2018,
  url       = "http://dx.doi.org/10.1111/cgf.13355",
  file      = "All Papers/Other/Wood et al. 2018 - GazeDirector - Fully articulated eye gaze redirection in video.pdf",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1111/cgf.13355",
  issn      = "0167-7055,1467-8659",
  language  = "en"
}

@ARTICLE{Wood2018-bq,
  title    = "{GazeDirector: Fully articulated eye gaze redirection in video}",
  author   = "Wood, Erroll and Baltrušaitis, Tadas and Morency, Louis-Philippe
              and Robinson, Peter and Bulling, Andreas",
  journal  = "Computer graphics forum: journal of the European Association for
              Computer Graphics",
  volume   =  37,
  number   =  2,
  pages    = "217–225",
  abstract = "We present GazeDirector, a new approach for eye gaze redirection
              that uses model-fitting. Our method first tracks the eyes by
              fitting a multi-part eye region model to video frames using
              analysis-by-synthesis, thereby recovering eye region shape,
              texture, pose, and gaze simultaneously. It then redirects gaze by
              1) warping the eyelids from the original image using a
              model-derived flow field, and 2) rendering and compositing
              synthesized 3D eyeballs onto the output image in a photorealistic
              manner. GazeDirector allows us to change where people are looking
              without person-specific training data, and with full articulation,
              i.e. we can precisely specify new gaze directions in 3D.
              Quantitatively, we evaluate both model-fitting and gaze synthesis,
              with experiments for gaze estimation and redirection on the
              Columbia gaze dataset. Qualitatively, we compare GazeDirector
              against recent work on gaze redirection, showing better results
              especially for large redirection angles. Finally, we demonstrate
              gaze redirection on YouTube videos by introducing new 3D gaze
              targets and by manipulating visual behavior.",
  month    =  may,
  year     =  2018,
  url      = "http://dx.doi.org/10.1111/cgf.13355",
  file     = "All Papers/My Library/Wood et al. 2018 - GazeDirector - Fully articulated eye gaze redirection in video.pdf",
  doi      = "10.1111/cgf.13355",
  issn     = "0167-7055,1467-8659",
  language = "en"
}

@ARTICLE{Stephenson1970-en,
  title     = "{Eye-contact, distance and affiliation: A re-evaluation}",
  author    = "Stephenson, Geoffrey M and Rutter, D R",
  journal   = "British journal of psychology",
  publisher = "Wiley",
  volume    =  61,
  number    =  3,
  pages     = "385--393",
  abstract  = "Argyle \& Dean (1965) presented and tested the
               affiliative--conflict theory of eye-contact and the Intimacy
               model which stems from it. Their finding that, as the distance
               between subjects in a dyadic discussion increased, recorded
               eye-contact increased, was held to support the theory and model,
               and has assumed a central place in Argyle's work on social skill.
               The present experiment tests the hypothesis that, with increasing
               distance, gaze directed at the ear and shoulder is increasingly
               recorded as eye-contact by observers in the Argyle \& Dean
               situation. The hypothesis is strongly confirmed: recorded
               eye-contact increases with distance but as a function of observer
               performance, not subject performance. It is suggested that Argyle
               \& Dean's results may have been an artifact of observer
               performance, not subject performance. Implications both for the
               methodology of work on eye-contact and for the models of Intimacy
               and social skill are discussed.",
  month     =  aug,
  year      =  1970,
  url       = "http://dx.doi.org/10.1111/j.2044-8295.1970.tb01257.x",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1111/j.2044-8295.1970.tb01257.x",
  issn      = "0007-1269,2044-8295",
  language  = "en"
}

@ARTICLE{Stephenson1970-qm,
  title    = "{Eye-contact, distance and affiliation: A re-evaluation}",
  author   = "Stephenson, Geoffrey M and Rutter, D R",
  journal  = "British journal of psychology",
  volume   =  61,
  number   =  3,
  pages    = "385–393",
  abstract = "Argyle \& Dean (1965) presented and tested the
              affiliative–conflict theory of eye-contact and the Intimacy model
              which stems from it. Their finding that, as the distance between
              subjects in a dyadic discussion increased, recorded eye-contact
              increased, was held to support the theory and model, and has
              assumed a central place in Argyle's work on social skill. The
              present experiment tests the hypothesis that, with increasing
              distance, gaze directed at the ear and shoulder is increasingly
              recorded as eye-contact by observers in the Argyle \& Dean
              situation. The hypothesis is strongly confirmed: recorded
              eye-contact increases with distance but as a function of observer
              performance, not subject performance. It is suggested that Argyle
              \& Dean's results may have been an artifact of observer
              performance, not subject performance. Implications both for the
              methodology of work on eye-contact and for the models of Intimacy
              and social skill are discussed.",
  month    =  aug,
  year     =  1970,
  url      = "https://onlinelibrary.wiley.com/doi/10.1111/j.2044-8295.1970.tb01257.x",
  doi      = "10.1111/j.2044-8295.1970.tb01257.x",
  issn     = "0007-1269,2044-8295",
  language = "en"
}

@ARTICLE{Coskun2022-kg,
  title    = "{A systematic review of eye‐tracking‐based research on animated
              multimedia learning}",
  author   = "Coskun, Atakan and Cagiltay, Kursat",
  journal  = "Journal of computer assisted learning",
  volume   =  38,
  number   =  2,
  pages    = "581–598",
  abstract = "Abstract Background The most challenging task in
              eye-tracking-based multimedia research is to establish a
              relationship between eye-tracking metrics (or cognitive processes)
              and learners' performance scores. Additionally, there are current
              debates about the effectiveness of animations (or simulations) in
              promoting learning in multimedia settings. Objectives As a result,
              the current study aimed to review eye tracking-based research on
              learners' cognitive processes in the animated/simulated multimedia
              learning domain. Method For this purpose, fifty-seven (57) studies
              were systematically determined based on PRISMA guidelines, and
              they were synthesized. Results and Conclusions The notable
              findings are that (1) most of the reviewed studies have tried to
              relate eye-tracking measures to at least one cognitive process
              (e.g., selecting, organizing, and integrating) which are assumed
              to take place during multimedia learning; (2) eye-tracking
              measurements show whether design features of animation are
              successful in directing learners' attention; (3) the success of
              animation in attention-guiding to the relevant parts may not have
              a positive impact on learning because individual differences
              (e.g., prior knowledge, spatial ability, or working capacity) and
              animation content are the other factors that directly affect the
              effectiveness of animation over learning. Implications The current
              study implies that (i) use of signaling cues in animated
              multimedia settings diminish visual search and promote higher
              learning scores, (ii) giving users control over animations causes
              negative outcomes (e.g., missing micro or macro events); (iii)
              research on animated multimedia learning should be replicated with
              the participant group from K-12, special education (e.g., autistic
              children), and elders.",
  month    =  apr,
  year     =  2022,
  url      = "https://onlinelibrary.wiley.com/doi/10.1111/jcal.12629",
  file     = "All Papers/My Library/Coskun and Cagiltay 2022 - A systematic review of eye‐tracking‐based research on animated multimedia learning.pdf",
  doi      = "10.1111/jcal.12629",
  issn     = "0266-4909,1365-2729",
  language = "en"
}

@ARTICLE{Hietanen2020-kr,
  title     = "{Psychophysiological responses to eye contact in a live
               interaction and in video call}",
  author    = "Hietanen, Jonne O and Peltola, Mikko J and Hietanen, Jari K",
  journal   = "Psychophysiology",
  publisher = "Wiley Online Library",
  volume    =  57,
  number    =  6,
  pages     = "e13587",
  abstract  = "Another person's gaze directed to oneself elicits autonomic
               arousal and facial reactions indicating positive affect in its
               observer. These effects have only been found to occur with
               mutual, live eye contact and not in response to direct gaze
               pictures or when the observer believes that the live person
               cannot see them. The question remains whether the physical
               presence of the other person is necessary for these effects. We
               measured psychophysiological responses to another person's direct
               versus averted gaze in three conditions: live interaction,
               bidirectional video call, and watching a mere video. Autonomic
               arousal was measured with skin conductance responses and facial
               reactions with facial electromyography. In the live and video
               call conditions, but not in the mere video condition, direct gaze
               increased autonomic arousal in comparison to averted gaze. In all
               three conditions, however, direct gaze elicited positive
               affective facial reactions. Therefore, an experience of being
               seen is essential for the autonomic reactions but not for the
               facial responses that are elicited by another person's direct
               gaze. Most importantly, the results suggest that the physical
               presence or proximity of the other person is not necessary for
               these psychophysiological responses to eye contact.",
  month     =  jun,
  year      =  2020,
  url       = "http://dx.doi.org/10.1111/psyp.13587",
  file      = "All Papers/Other/Hietanen et al. 2020 - Psychophysiological responses to eye contact in a live interaction and in video call.pdf",
  keywords  = "EMG; autonomic arousal; direct gaze; eye contact; skin
               conductance; video call",
  doi       = "10.1111/psyp.13587",
  pmid      =  32320067,
  issn      = "0048-5772,1540-5958",
  language  = "en"
}

@ARTICLE{Hietanen2020-zi,
  title    = "{Psychophysiological responses to eye contact in a live
              interaction and in video call}",
  author   = "Hietanen, Jonne O and Peltola, Mikko J and Hietanen, Jari K",
  journal  = "Psychophysiology",
  volume   =  57,
  number   =  6,
  pages    = "e13587",
  abstract = "Another person's gaze directed to oneself elicits autonomic
              arousal and facial reactions indicating positive affect in its
              observer. These effects have only been found to occur with mutual,
              live eye contact and not in response to direct gaze pictures or
              when the observer believes that the live person cannot see them.
              The question remains whether the physical presence of the other
              person is necessary for these effects. We measured
              psychophysiological responses to another person's direct versus
              averted gaze in three conditions: live interaction, bidirectional
              video call, and watching a mere video. Autonomic arousal was
              measured with skin conductance responses and facial reactions with
              facial electromyography. In the live and video call conditions,
              but not in the mere video condition, direct gaze increased
              autonomic arousal in comparison to averted gaze. In all three
              conditions, however, direct gaze elicited positive affective
              facial reactions. Therefore, an experience of being seen is
              essential for the autonomic reactions but not for the facial
              responses that are elicited by another person's direct gaze. Most
              importantly, the results suggest that the physical presence or
              proximity of the other person is not necessary for these
              psychophysiological responses to eye contact.",
  month    =  jun,
  year     =  2020,
  url      = "http://dx.doi.org/10.1111/psyp.13587",
  file     = "All Papers/My Library/Hietanen et al. 2020 - Psychophysiological responses to eye contact in a live interaction and in video call.pdf",
  doi      = "10.1111/psyp.13587",
  issn     = "0048-5772,1540-5958",
  language = "en"
}

@INPROCEEDINGS{Hornof2003-rp,
  title     = "{Eyedraw: a system for drawing pictures with eye movements}",
  author    = "Hornof, Anthony and Cavender, Anna and Hoselton, Rob",
  booktitle = "{Proceedings of the 6th international ACM SIGACCESS conference on
               Computers and accessibility}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "86–93",
  abstract  = "This paper describes the design and development of EyeDraw, a
               software program that will enable children with severe mobility
               impairments to use an eye tracker to draw pictures with their
               eyes so that they can have the same creative developmental
               experiences as nondisabled children. EyeDraw incorporates
               computer-control and software application advances that address
               the special needs of people with motor impairments, with emphasis
               on the needs of children. The contributions of the project
               include (a) a new technique for using the eyes to control the
               computer when accomplishing a spatial task, (b) the crafting of
               task-relevant functionality to support this new technique in its
               application to drawing pictures, and (c) a user-tested
               implementation of the idea within a working computer program.
               User testing with nondisabled users suggests that we have
               designed and built an eye-cursor and eye drawing control system
               that can be used by almost anyone with normal control of their
               eyes. The core technique will be generally useful for a range of
               computer control tasks such as selecting a group of icons on the
               desktop by drawing a box around them.",
  series    = "Assets '04",
  month     =  "9~" # jan,
  year      =  2003,
  url       = "https://doi.org/10.1145/1028630.1028647",
  doi       = "10.1145/1028630.1028647",
  isbn      =  9781581139112
}

@INPROCEEDINGS{Fono2005-at,
  title     = "{EyeWindows: evaluation of eye-controlled zooming windows for
               focus selection}",
  author    = "Fono, David and Vertegaal, Roel",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "151–160",
  abstract  = "In this paper, we present an attentive windowing technique that
               uses eye tracking, rather than manual pointing, for focus window
               selection. We evaluated the performance of 4 focus selection
               techniques: eye tracking with key activation, eye tracking with
               automatic activation, mouse and hotkeys in a typing task with
               many open windows. We also evaluated a zooming windowing
               technique designed specifically for eye-based control, comparing
               its performance to that of a stan-dard tiled windowing
               environment. Results indicated that eye tracking with automatic
               activation was, on average, about twice as fast as mouse and
               hotkeys. Eye tracking with key activation was about 72\% faster
               than manual conditions, and preferred by most participants. We
               believe eye input performed well because it allows manual input
               to be provided in parallel to focus selection tasks. Results also
               suggested that zooming windows outperform static tiled windows by
               about 30\%. Furthermore, this performance gain scaled with the
               number of windows used. We conclude that eye-controlled zooming
               windows with key activation pro-vides an efficient and effective
               alternative to current focus window selection techniques.",
  series    = "CHI '05",
  month     =  "4~" # feb,
  year      =  2005,
  url       = "https://doi.org/10.1145/1054972.1054994",
  file      = "All Papers/My Library/Fono and Vertegaal 2005 - EyeWindows - evaluation of eye-controlled zooming windows for focus selection.pdf",
  doi       = "10.1145/1054972.1054994",
  isbn      =  9781581139983
}

@INPROCEEDINGS{Hornof2005-cn,
  title     = "{{EyeDraw}: enabling children with severe motor impairments to
               draw with their eyes}",
  author    = "Hornof, Anthony J and Cavender, Anna",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "161--170",
  abstract  = "EyeDraw is a software program that, when run on a computer with
               an eye tracking device, enables children with severe motor
               disabilities to draw pictures by just moving their eyes. This
               paper discusses the motivation for building the software, how the
               program works, the iterative development of two versions of the
               software, user testing of the two versions by people with and
               without disabilities, and modifications to the software based on
               user testing. Feedback from both children and adults with
               disabilities, and from their caregivers, was especially helpful
               in the design process. The project identifies challenges that are
               unique to controlling a computer with the eyes, and unique to
               writing software for children with severe motor impairments.",
  series    = "CHI '05",
  month     =  apr,
  year      =  2005,
  url       = "http://dx.doi.org/10.1145/1054972.1054995",
  keywords  = "eye tracking, drawing, input devices, interaction techniques,
               children, art, universal access;prj-gaze-shorthand",
  doi       = "10.1145/1054972.1054995",
  isbn      =  9781581139983
}

@INPROCEEDINGS{Hornof2005-pf,
  title     = "{EyeDraw: enabling children with severe motor impairments to draw
               with their eyes}",
  author    = "Hornof, Anthony J and Cavender, Anna",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "161–170",
  abstract  = "EyeDraw is a software program that, when run on a computer with
               an eye tracking device, enables children with severe motor
               disabilities to draw pictures by just moving their eyes. This
               paper discusses the motivation for building the software, how the
               program works, the iterative development of two versions of the
               software, user testing of the two versions by people with and
               without disabilities, and modifications to the software based on
               user testing. Feedback from both children and adults with
               disabilities, and from their caregivers, was especially helpful
               in the design process. The project identifies challenges that are
               unique to controlling a computer with the eyes, and unique to
               writing software for children with severe motor impairments.",
  series    = "CHI '05",
  month     =  "4~" # feb,
  year      =  2005,
  url       = "https://doi.org/10.1145/1054972.1054995",
  doi       = "10.1145/1054972.1054995",
  isbn      =  9781581139983
}

@INPROCEEDINGS{Grossman2005-vn,
  title     = "{The bubble cursor: enhancing target acquisition by dynamic
               resizing of the cursor's activation area}",
  author    = "Grossman, Tovi and Balakrishnan, Ravin",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "Portland Oregon USA",
  pages     = "281--290",
  abstract  = "We present the bubble cursor – a new target acquisition technique
               based on area cursors. The bubble cursor improves upon area
               cursors by dynamically resizing its activation area depending on
               the proximity of surrounding targets, such that only one target
               is selectable at any time. We also present two controlled
               experiments that evaluate bubble cursor performance in 1D and 2D
               target acquisition tasks, in complex situations with multiple
               targets of varying layout densities. Results show that the bubble
               cursor significantly outperforms the point cursor and the object
               pointing technique [8], and that bubble cursor performance can be
               accurately modeled and predicted using Fitts’ law.",
  month     =  "2~" # apr,
  year      =  2005,
  url       = "https://dl.acm.org/doi/10.1145/1054972.1055012",
  doi       = "10.1145/1054972.1055012",
  isbn      =  9781581139983,
  language  = "en"
}

@INPROCEEDINGS{Nguyen2005-jb,
  title     = "{{MultiView}: spatially faithful group video conferencing}",
  author    = "Nguyen, David and Canny, John",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "799--808",
  abstract  = "MultiView is a new video conferencing system that supports
               collaboration between remote groups of people. MultiView
               accomplishes this by being spatially faithful. As a result,
               MultiView preserves a myriad of nonverbal cues, includ-ing gaze
               and gesture, in a way that should improve com-munication.
               Previous systems fail to support many of these cues because a
               single camera perspective warps spatial char-acteristics in
               group-to-group meetings. In this paper, we present a formal
               definition of spatial faithfulness. We then apply a
               metaphor-based design methodology to help us spec-ify and
               evaluate MultiView's support of spatial faithfulness. We then
               present results from a low-level user study to mea-sure
               MultiView's effectiveness at conveying gaze and ges-ture
               perception. MultiView is the first practical solution to
               spatially faithful group-to-group conferencing, one of the most
               common applications of video conferencing.",
  series    = "CHI '05",
  month     =  apr,
  year      =  2005,
  url       = "http://dx.doi.org/10.1145/1054972.1055084",
  keywords  = "deixis, eye contact, gaze, video conferencing, spatial
               faithfulness;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/1054972.1055084",
  isbn      =  9781581139983
}

@INPROCEEDINGS{Nguyen2005-pp,
  title     = "{MultiView: spatially faithful group video conferencing}",
  author    = "Nguyen, David and Canny, John",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "799–808",
  abstract  = "MultiView is a new video conferencing system that supports
               collaboration between remote groups of people. MultiView
               accomplishes this by being spatially faithful. As a result,
               MultiView preserves a myriad of nonverbal cues, includ-ing gaze
               and gesture, in a way that should improve com-munication.
               Previous systems fail to support many of these cues because a
               single camera perspective warps spatial char-acteristics in
               group-to-group meetings. In this paper, we present a formal
               definition of spatial faithfulness. We then apply a
               metaphor-based design methodology to help us spec-ify and
               evaluate MultiView's support of spatial faithfulness. We then
               present results from a low-level user study to mea-sure
               MultiView's effectiveness at conveying gaze and ges-ture
               perception. MultiView is the first practical solution to
               spatially faithful group-to-group conferencing, one of the most
               common applications of video conferencing.",
  series    = "CHI '05",
  month     =  apr,
  year      =  2005,
  url       = "http://dx.doi.org/10.1145/1054972.1055084",
  doi       = "10.1145/1054972.1055084",
  isbn      =  9781581139983
}

@INPROCEEDINGS{Mukawa2005-gu,
  title     = "{What is connected by mutual gaze? user's behavior in
               video-mediated communication}",
  author    = "Mukawa, Naoki and Oka, Tsugumi and Arai, Kumiko and Yuasa,
               Masahide",
  booktitle = "{{CHI} '05 Extended Abstracts on Human Factors in Computing
               Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1677--1680",
  abstract  = "Video-mediated communication systems such as teleconferencing and
               videophone have become popular. As with face-to-face
               communication, non-verbal cues such as gaze, facial expression,
               head orientation and gestures in visual systems play an important
               role. Existing systems, however, do not support mutual gaze
               because the lay-out of the camera and monitor is restricted.
               Thus, conversations using visual systems differ from those in
               face-to-face communication. This paper clarifies the problems of
               the video-mediated system, specifically for comparing the system
               with communication using eye-contact and with communication using
               no-eye-contact. This study focuses on the protocol of opening
               communication, e.g. establishment of a visual-audio link, person
               identification and confirmation of the acceptance of
               conversation. We conducted experiments using the two systems.
               Analysis of recorded video sequences revealed that the system
               using communication with eye-contact induced behavior similar to
               the system using face-to-face communication.",
  series    = "CHI EA '05",
  month     =  apr,
  year      =  2005,
  url       = "http://dx.doi.org/10.1145/1056808.1056995",
  keywords  = "gaze, video-mediated communication, ethenographical approach, eye
               contact;eye contact;telepresence",
  doi       = "10.1145/1056808.1056995",
  isbn      =  9781595930026
}

@INPROCEEDINGS{Mukawa2005-rm,
  title     = "{What is connected by mutual gaze? user's behavior in
               video-mediated communication}",
  author    = "Mukawa, Naoki and Oka, Tsugumi and Arai, Kumiko and Yuasa,
               Masahide",
  booktitle = "{CHI '05 extended abstracts on human factors in computing
               systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1677–1680",
  abstract  = "Video-mediated communication systems such as teleconferencing and
               videophone have become popular. As with face-to-face
               communication, non-verbal cues such as gaze, facial expression,
               head orientation and gestures in visual systems play an important
               role. Existing systems, however, do not support mutual gaze
               because the lay-out of the camera and monitor is restricted.
               Thus, conversations using visual systems differ from those in
               face-to-face communication. This paper clarifies the problems of
               the video-mediated system, specifically for comparing the system
               with communication using eye-contact and with communication using
               no-eye-contact. This study focuses on the protocol of opening
               communication, e.g. establishment of a visual-audio link, person
               identification and confirmation of the acceptance of
               conversation. We conducted experiments using the two systems.
               Analysis of recorded video sequences revealed that the system
               using communication with eye-contact induced behavior similar to
               the system using face-to-face communication.",
  series    = "CHI EA '05",
  month     =  apr,
  year      =  2005,
  url       = "http://dx.doi.org/10.1145/1056808.1056995",
  doi       = "10.1145/1056808.1056995",
  isbn      =  9781595930026
}

@INPROCEEDINGS{Hansen2006-im,
  title     = "{Eye typing with common cameras}",
  author    = "Hansen, Dan Witzner and Hansen, John Paulin",
  booktitle = "{Proceedings of the 2006 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     =  55,
  abstract  = "Low cost eye tracking has received an increased attention due to
               the rapid developments in tracking hardware (video boards,
               digital camera and CPU's) [Hansen and Pece 2005; OpenEyes 2005].
               We present a gaze typing system based on components that can be
               bought in most consumer hardware stores around the world. These
               components are for example cameras and graphics cards that are
               made in large quantities. This kind of hardware differs from what
               is often claimed to be “off-the-shelf components”, but which in
               fact is hardware only available from particular
               vendors.Institutions that supply citizens with communication aids
               may be reluctant to invest large amounts of money in new
               equipment that they are unfamiliar with. Recent investiagtions
               estimate that less than 2000 systems have actually been used by
               Europeans, even though more than half a million disabled people
               in Europe could potentially benefit from it. The main group of
               present users consists of people with motor neuron disease (MND)
               and amyotrophic lateral sclerosis (ALS). If the price of gaze
               communication systems can be lowered, it could become a preferred
               means of control for a large group of people [Jordansen et al.
               2005]. Present commercial gaze trackers e.g. [Tobii 2005;
               LC-Technologies 2004] are easy to use, robust and sufficiently
               accurate for many screen-based applications but their costs
               exceed the budget of most people.We use a standard uncalibrated
               400\$ Sony consumer camera (Sony handycam DCR-HC14E) to obtain
               the image data. The camera is stationary and placed on a tripod
               close (variable) to the monitor, but the geometry of the user,
               monitor and camera varies among sequences. However, the users are
               sitting about 50 - 60 cm away from a 17“ screen. A typical
               example of the setup is shown in figure 1. We use Sony standard
               video option for 'night vision' to create an glint with the
               build-in IR light emitter.Eye tracking based on common components
               is subject to several unknown factors as various system
               parameters (i.e. camera parameters and geometry) are unknown.
               Algorithms that employ robust statistical principles to
               accommodate uncertainties in image data as well as in gaze
               estimates in the typing process are therefore needed. We propose
               to use the RANSAC algorithm [Fischler and Bolles 1981] for both
               robust maximum likelihood estimation of iris observations [Hansen
               and Pece 2005] as well as for handling outliers in the
               calibration procedure [Morimoto et al. 2000].Our low-resolution
               gaze tracker can be calibrated in less than 3 minutes by looking
               at 9 predefined positions on the screen. The users sit on a
               standard office chair without headrests or other physical
               constraints. Under these conditions we have succeeded in tracking
               the gaze of people, obtaining accuracies about 160 pixels on
               screen. This is still less than accuracies claimed by the best
               current off-the-shelf eye trackers systems (i.e. 30-60 pixels).
               However comparing these eye trackers wouldn't be correct as they
               are based on different hardware and image data.Low-cost gaze
               trackers do not need to be as accurate and robust as the
               commercial systems, if they are used together with applications
               designed to tolerate noisy inputs.We use the GazeTalk [COGAIN
               2005] typing communication system components and have through
               proper design of the typing interface, reduced the need for high
               accuracy. We have observed typing speeds in the range of 3 - 5
               words per minute for untrained subjects using large on-screen
               buttons and a new noise tolerant dwell-time principle. We modify
               the traditional dwell-time activation to one that maintains a
               full distribution of all hypothetical button selections and then
               activate one button when the evidence become high enough.",
  series    = "ETRA '06",
  month     =  "27~" # mar,
  year      =  2006,
  url       = "https://doi.org/10.1145/1117309.1117340",
  doi       = "10.1145/1117309.1117340",
  isbn      =  9781595933058
}

@INPROCEEDINGS{Itoh2006-tl,
  title     = "{A comparative usability study of two Japanese gaze typing
               systems}",
  author    = "Itoh, Kenji and Aoki, Hirotaka and Hansen, John Paulin",
  booktitle = "{Proceedings of the 2006 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "59–66",
  abstract  = "The complex interplay between gaze tracker accuracy and interface
               design is the focus of this paper. Two slightly different
               variants of GazeTalk, a hierarchical typing interface, were
               contrasted with a novel interface, Dasher, in which text entry is
               done by continuous navigation. All of the interfaces were tested
               with a good and a deliberate bad calibration of the tracker. The
               purpose was to investigate, if performance indices normally used
               for evaluation of typing systems, such as characters per minute
               (CPM) and error-rate, could differentiate between the conditions,
               and thus guide an iterative system development of both trackers
               and interfaces. Gaze typing with one version of the static,
               hierarchical menu systems was slightly faster than the others.
               Error measures, in terms of rate of backspacing, were also
               significantly different for the systems, while the deliberate bad
               tracker calibrations did not have any measurable effect. Learning
               effects were evident under all conditions. Power-law-of-practice
               learning models suggested that Dasher might be more efficient
               than GazeTalk in the long run.",
  series    = "ETRA '06",
  month     =  "27~" # mar,
  year      =  2006,
  url       = "https://doi.org/10.1145/1117309.1117344",
  doi       = "10.1145/1117309.1117344",
  isbn      =  9781595933058
}

@ARTICLE{Bekkering2006-zu,
  title    = "{Trust in videoconferencing}",
  author   = "Bekkering, Ernst and Shim, J P",
  journal  = "Communications of the ACM",
  volume   =  49,
  number   =  7,
  pages    = "103–107",
  abstract = "People associate poor eye contact with deception. This perception
              may have hurt large-scale adoption of videoconferencing
              technology.",
  month    =  jul,
  year     =  2006,
  url      = "https://doi.org/10.1145%2F1139922.1139925",
  file     = "All Papers/My Library/Bekkering and Shim 2006 - Trust in videoconferencing.pdf",
  doi      = "10.1145/1139922.1139925",
  issn     = "0001-0782,1557-7317",
  language = "en"
}

@INPROCEEDINGS{Poupyrev2007-mx,
  title     = "{Actuation and tangible user interfaces}",
  author    = "Poupyrev, Ivan and Nashida, Tatsushi and Okabe, Makoto",
  booktitle = "{Proceedings of the 1st international conference on Tangible and
               embedded interaction - TEI '07}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  year      =  2007,
  url       = "https://doi.org/10.1145%2F1226969.1227012",
  doi       = "10.1145/1226969.1227012",
  isbn      =  9781595936196
}

@INPROCEEDINGS{Kumar2007-mr,
  title     = "{EyePoint: practical pointing and selection using gaze and
               keyboard}",
  author    = "Kumar, Manu and Paepcke, Andreas and Winograd, Terry",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "421–430",
  abstract  = "We present a practical technique for pointing and selection using
               a combination of eye gaze and keyboard triggers. EyePoint uses a
               two-step progressive refinement process fluidly stitched together
               in a look-press-look-release action, which makes it possible to
               compensate for the accuracy limitations of the current
               state-of-the-art eye gaze trackers. While research in gaze-based
               pointing has traditionally focused on disabled users, EyePoint
               makes gaze-based pointing effective and simple enough for even
               able-bodied users to use for their everyday computing tasks. As
               the cost of eye gaze tracking devices decreases, it will become
               possible for such gaze-based techniques to be used as a viable
               alternative for users who choose not to use a mouse depending on
               their abilities, tasks and preferences.",
  series    = "CHI '07",
  month     =  "29~" # apr,
  year      =  2007,
  url       = "https://doi.org/10.1145/1240624.1240692",
  file      = "All Papers/My Library/Kumar et al. 2007 - EyePoint - practical pointing and selection using gaze and keyboard.pdf",
  doi       = "10.1145/1240624.1240692",
  isbn      =  9781595935939
}

@INPROCEEDINGS{Nguyen2007-je,
  title     = "{Multiview: improving trust in group video conferencing through
               spatial faithfulness}",
  author    = "Nguyen, David T and Canny, John",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1465--1474",
  abstract  = "Video conferencing is still considered a poor alternative to
               face-to-face meetings. In the business setting, where these
               systems are most prevalent, the misuse of video conferencing
               systems can have detrimental results, especially in high-stakes
               communications. Prior work suggests that spatial distortions of
               nonverbal cues, particularly gaze and deixis, negatively impact
               many aspects of effective communication in dyadic communications.
               However, video conferencing systems are often used for
               group-to-group meetings where spatial distortions are
               exacerbated. Meanwhile, its effects on the group dynamic are not
               well understood. In this study, we examine the effects that
               spatial distortions of nonverbal cues have on inter-group trust
               formation. We conducted a large (169 participant) study of group
               conferencing under various conditions. We found that the use of
               systems that introduce spatial distortions negatively affect
               trust formation patterns. On the other hand, these effects are
               essentially eliminated by using a spatially faithful video
               conferencing system.",
  series    = "CHI '07",
  month     =  apr,
  year      =  2007,
  url       = "http://dx.doi.org/10.1145/1240624.1240846",
  keywords  = "CMC, CSCW, gaze awareness, trust, prisoner's dilemma, social
               dilemmas, video conferencing, eye contact, spatial
               faithfulness;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/1240624.1240846",
  isbn      =  9781595935939
}

@INPROCEEDINGS{Nguyen2007-zm,
  title     = "{Multiview: improving trust in group video conferencing through
               spatial faithfulness}",
  author    = "Nguyen, David T and Canny, John",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1465–1474",
  abstract  = "Video conferencing is still considered a poor alternative to
               face-to-face meetings. In the business setting, where these
               systems are most prevalent, the misuse of video conferencing
               systems can have detrimental results, especially in high-stakes
               communications. Prior work suggests that spatial distortions of
               nonverbal cues, particularly gaze and deixis, negatively impact
               many aspects of effective communication in dyadic communications.
               However, video conferencing systems are often used for
               group-to-group meetings where spatial distortions are
               exacerbated. Meanwhile, its effects on the group dynamic are not
               well understood. In this study, we examine the effects that
               spatial distortions of nonverbal cues have on inter-group trust
               formation. We conducted a large (169 participant) study of group
               conferencing under various conditions. We found that the use of
               systems that introduce spatial distortions negatively affect
               trust formation patterns. On the other hand, these effects are
               essentially eliminated by using a spatially faithful video
               conferencing system.",
  series    = "CHI '07",
  month     =  apr,
  year      =  2007,
  url       = "http://dx.doi.org/10.1145/1240624.1240846",
  doi       = "10.1145/1240624.1240846",
  isbn      =  9781595935939
}

@INPROCEEDINGS{Kumar2007-nm,
  title     = "{GUIDe: gaze-enhanced UI design}",
  author    = "Kumar, Manu and Winograd, Terry",
  booktitle = "{CHI '07 extended abstracts on human factors in computing
               systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1977–1982",
  abstract  = "The GUIDe (Gaze-enhanced User Interface Design) project in the
               HCI Group at Stanford University explores how gaze information
               can be effectively used as an augmented input in addition to
               keyboard and mouse. We present three practical applications of
               gaze as an augmented input for pointing and selection,
               application switching, and scrolling. Our gaze-based interaction
               techniques do not overload the visual channel and present a
               natural, universally-accessible and general purpose use of gaze
               information to facilitate interaction with everyday computing
               devices.",
  series    = "CHI EA '07",
  month     =  "28~" # apr,
  year      =  2007,
  url       = "https://doi.org/10.1145/1240866.1240935",
  file      = "All Papers/My Library/Kumar and Winograd 2007 - GUIDe - gaze-enhanced UI design.pdf",
  doi       = "10.1145/1240866.1240935",
  isbn      =  9781595936424
}

@INPROCEEDINGS{Kumar2007-re,
  title     = "{Gaze-enhanced scrolling techniques}",
  author    = "Kumar, Manu and Winograd, Terry and Paepcke, Andreas",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2531–2536",
  abstract  = "We present several gaze-enhanced scrolling techniques developed
               as part of continuing work in the GUIDe (Gaze-enhanced User
               Interface Design) project. This effort explores how gaze
               information can be effectively used as input that augments
               keyboard and mouse. The techniques presented below use gaze both
               as a primary input and as an augmented input in order to enhance
               scrolling and panning techniques. We also introduce the use of
               off-screen gaze-actuated buttons which can be used for document
               navigation and control.",
  series    = "CHI EA '07",
  month     =  "28~" # apr,
  year      =  2007,
  url       = "https://doi.org/10.1145/1240866.1241036",
  file      = "All Papers/My Library/Kumar et al. 2007 - Gaze-enhanced scrolling techniques.pdf",
  doi       = "10.1145/1240866.1241036",
  isbn      =  9781595936424
}

@INPROCEEDINGS{Kumar2007-bx,
  title     = "{Gaze-enhanced scrolling techniques}",
  author    = "Kumar, Manu and Winograd, Terry",
  booktitle = "{Proceedings of the 20th annual ACM symposium on User interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "213–216",
  abstract  = "Scrolling is an essential part of our everyday computing
               experience. Contemporary scrolling techniques rely on the
               explicit initiation of scrolling by the user. The act of
               scrolling is tightly coupled with the user?s ability to absorb
               information via the visual channel. The use of eye gaze
               information is therefore a natural choice for enhancing scrolling
               techniques. We present several gaze-enhanced scrolling techniques
               for manual and automatic scrolling which use gaze information as
               a primary input or as an augmented input. We also introduce the
               use off-screen gaze-actuated buttons for document navigation and
               control.",
  series    = "UIST '07",
  month     =  "10~" # jul,
  year      =  2007,
  url       = "https://doi.org/10.1145/1294211.1294249",
  file      = "All Papers/My Library/Kumar and Winograd 2007 - Gaze-enhanced scrolling techniques.pdf",
  doi       = "10.1145/1294211.1294249",
  isbn      =  9781595936790
}

@INPROCEEDINGS{Wobbrock2008-io,
  title     = "{Longitudinal evaluation of discrete consecutive gaze gestures
               for text entry}",
  author    = "Wobbrock, Jacob O and Rubinstein, James and Sawyer, Michael W and
               Duchowski, Andrew T",
  booktitle = "{Proceedings of the 2008 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "11--18",
  abstract  = "Eye-typing performance results are reported from controlled
               studies comparing an on-screen keyboard and Eye Write, a new
               on-screen gestural input alternative. Results from the first
               pilot study suggest the presence of a learning curve that novice
               users must overcome in order to gain proficiency in EyeWrite's
               use (requiring practice with its letter-like gestural alphabet).
               Results from the second longitudinal study indicate that
               EyeWrite's inherent multi-saccade handicap (4.52 saccades per
               character, frequency-weighted average) is sufficient for the
               on-screen keyboard to edge out Eye Write in speed performance.
               Eye-typing speeds with Eye Write approach 5 wpm on average (8 wpm
               attainable by proficient users), whereas keyboard users achieve
               about 7 wpm on average (in line with previous results). However,
               Eye Write users leave significantly fewer uncorrected errors in
               the final text, with no significant difference in the number of
               errors corrected during entry, indicating a speed-accuracy
               trade-off. Subjective results indicate that participants consider
               Eye Write significantly faster, easier to use, and prone to cause
               less ocular fatigue than the on-screen keyboard. In addition,
               Eye-Write consumes much less screen real-estate than an on-screen
               keyboard, giving it practical advantages for eye-based text
               entry.",
  series    = "ETRA '08",
  month     =  mar,
  year      =  2008,
  url       = "http://dx.doi.org/10.1145/1344471.1344475",
  file      = "All Papers/Other/Wobbrock et al. 2008 - Longitudinal evaluation of discrete consecutive gaze gestures for text entry.pdf",
  keywords  = "gestures, eye-typing, text input, text entry;prj-gaze-shorthand",
  doi       = "10.1145/1344471.1344475",
  isbn      =  9781595939821
}

@INPROCEEDINGS{Wobbrock2008-dj,
  title     = "{Longitudinal evaluation of discrete consecutive gaze gestures
               for text entry}",
  author    = "Wobbrock, Jacob O and Rubinstein, James and Sawyer, Michael W and
               Duchowski, Andrew T",
  booktitle = "{Proceedings of the 2008 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "11--18",
  abstract  = "Eye-typing performance results are reported from controlled
               studies comparing an on-screen keyboard and Eye Write, a new
               on-screen gestural input alternative. Results from the first
               pilot study suggest the presence of a learning curve that novice
               users must overcome in order to gain proficiency in EyeWrite's
               use (requiring practice with its letter-like gestural alphabet).
               Results from the second longitudinal study indicate that
               EyeWrite's inherent multi-saccade handicap (4.52 saccades per
               character, frequency-weighted average) is sufficient for the
               on-screen keyboard to edge out Eye Write in speed performance.
               Eye-typing speeds with Eye Write approach 5 wpm on average (8 wpm
               attainable by proficient users), whereas keyboard users achieve
               about 7 wpm on average (in line with previous results). However,
               Eye Write users leave significantly fewer uncorrected errors in
               the final text, with no significant difference in the number of
               errors corrected during entry, indicating a speed-accuracy
               trade-off. Subjective results indicate that participants consider
               Eye Write significantly faster, easier to use, and prone to cause
               less ocular fatigue than the on-screen keyboard. In addition,
               Eye-Write consumes much less screen real-estate than an on-screen
               keyboard, giving it practical advantages for eye-based text
               entry.",
  series    = "ETRA '08",
  month     =  mar,
  year      =  2008,
  url       = "http://dx.doi.org/10.1145/1344471.1344475",
  file      = "All Papers/Other/Wobbrock et al. 2008 - Longitudinal evaluation of discrete consecutive gaze gestures for text entry.pdf",
  keywords  = "gestures, eye-typing, text input, text entry;prj-gaze-shorthand",
  doi       = "10.1145/1344471.1344475",
  isbn      =  9781595939821
}

@INPROCEEDINGS{Wobbrock2008-cx,
  title     = "{Longitudinal evaluation of discrete consecutive gaze gestures
               for text entry}",
  author    = "Wobbrock, Jacob O and Rubinstein, James and Sawyer, Michael W and
               Duchowski, Andrew T",
  booktitle = "{Proceedings of the 2008 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "11–18",
  abstract  = "Eye-typing performance results are reported from controlled
               studies comparing an on-screen keyboard and Eye Write, a new
               on-screen gestural input alternative. Results from the first
               pilot study suggest the presence of a learning curve that novice
               users must overcome in order to gain proficiency in EyeWrite's
               use (requiring practice with its letter-like gestural alphabet).
               Results from the second longitudinal study indicate that
               EyeWrite's inherent multi-saccade handicap (4.52 saccades per
               character, frequency-weighted average) is sufficient for the
               on-screen keyboard to edge out Eye Write in speed performance.
               Eye-typing speeds with Eye Write approach 5 wpm on average (8 wpm
               attainable by proficient users), whereas keyboard users achieve
               about 7 wpm on average (in line with previous results). However,
               Eye Write users leave significantly fewer uncorrected errors in
               the final text, with no significant difference in the number of
               errors corrected during entry, indicating a speed-accuracy
               trade-off. Subjective results indicate that participants consider
               Eye Write significantly faster, easier to use, and prone to cause
               less ocular fatigue than the on-screen keyboard. In addition,
               Eye-Write consumes much less screen real-estate than an on-screen
               keyboard, giving it practical advantages for eye-based text
               entry.",
  series    = "ETRA '08",
  month     =  "26~" # mar,
  year      =  2008,
  url       = "https://doi.org/10.1145/1344471.1344475",
  file      = "All Papers/My Library/Wobbrock et al. 2008 - Longitudinal evaluation of discrete consecutive gaze gestures for text entry.pdf",
  doi       = "10.1145/1344471.1344475",
  isbn      =  9781595939821
}

@INPROCEEDINGS{Huckauf2008-ri,
  title     = "{Gazing with {pEYEs}: towards a universal input for various
               applications}",
  author    = "Huckauf, Anke and Urbina, Mario H",
  booktitle = "{Proceedings of the 2008 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "51--54",
  abstract  = "Various interfaces for gaze control (which are recommended due to
               certain requirements of controlling a machine by gaze) have
               already been developed. One problem, especially for novice users,
               is that respective interfaces all look different and require
               different steps to use. As a means to unify interfaces for gaze
               control, pie menus are suggested. Such pEYEs allow for universal
               input in various applications usable by novices and by experts.
               We present two examples for pEYE interfaces; one eye-typing
               application and one desktop navigation. Observations in user
               studies indicate effective and efficient performance and a large
               acceptance.",
  series    = "ETRA '08",
  month     =  mar,
  year      =  2008,
  url       = "http://dx.doi.org/10.1145/1344471.1344483",
  keywords  = "eye-typing, evaluation methodology, user interfaces, gaze
               control, input devices",
  doi       = "10.1145/1344471.1344483",
  isbn      =  9781595939821
}

@INPROCEEDINGS{Huckauf2008-re,
  title     = "{Gazing with pEYEs: towards a universal input for various
               applications}",
  author    = "Huckauf, Anke and Urbina, Mario H",
  booktitle = "{Proceedings of the 2008 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "51–54",
  abstract  = "Various interfaces for gaze control (which are recommended due to
               certain requirements of controlling a machine by gaze) have
               already been developed. One problem, especially for novice users,
               is that respective interfaces all look different and require
               different steps to use. As a means to unify interfaces for gaze
               control, pie menus are suggested. Such pEYEs allow for universal
               input in various applications usable by novices and by experts.
               We present two examples for pEYE interfaces; one eye-typing
               application and one desktop navigation. Observations in user
               studies indicate effective and efficient performance and a large
               acceptance.",
  series    = "ETRA '08",
  month     =  mar,
  year      =  2008,
  url       = "http://dx.doi.org/10.1145/1344471.1344483",
  doi       = "10.1145/1344471.1344483",
  isbn      =  9781595939821
}

@INPROCEEDINGS{MacKenzie2008-yz,
  title     = "{Eye typing using word and letter prediction and a fixation
               algorithm}",
  author    = "MacKenzie, I Scott and Zhang, Xuang",
  booktitle = "{Proceedings of the 2008 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "55–58",
  abstract  = "Two eye typing techniques and a fixation algorithm are described.
               Similar to word prediction, letter prediction chooses three
               highly probable next letters and highlights them on an onscreen
               keyboard. Letter prediction proved promising, as it was as good
               as word prediction, and in some cases better. The fixation
               algorithm chooses which button to select for eye-over
               highlighting. It often chooses the desired button even if another
               button is closer to the fixation location. Error rates were
               reduced when using the fixation algorithm combined with letter
               prediction; however, the algorithm was sensitive to the
               correctness of the first several letters in a word.",
  series    = "ETRA '08",
  month     =  "26~" # mar,
  year      =  2008,
  url       = "https://doi.org/10.1145/1344471.1344484",
  file      = "All Papers/My Library/MacKenzie and Zhang 2008 - Eye typing using word and letter prediction and a fixation algorithm.pdf",
  doi       = "10.1145/1344471.1344484",
  isbn      =  9781595939821
}

@INPROCEEDINGS{Hansen2008-ol,
  title     = "{Noise tolerant selection by gaze-controlled pan and zoom in
               {3D}}",
  author    = "Hansen, Dan Witzner and Skovsgaard, Henrik H T and Hansen, John
               Paulin and Møllenbach, Emilie",
  booktitle = "{Proceedings of the 2008 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "205–212",
  abstract  = "This paper presents StarGazer - a new 3D interface for gaze-based
               interaction and target selection using continuous pan and zoom.
               Through StarGazer we address the issues of interacting with graph
               structured data and applications (i.e. gaze typing systems) using
               low resolution eye trackers or small-size displays. We show that
               it is possible to make robust selection even with a large number
               of selectable items on the screen and noisy gaze trackers. A test
               with 48 subjects demonstrated that users who have never tried
               gaze interaction before could rapidly adapt to the navigation
               principles of StarGazer. We tested three different display sizes
               (down to PDA-sized displays) and found that large screens are
               faster to navigate than small displays and that the error rate is
               higher for the smallest display. Half of the subjects were
               exposed to severe noise deliberately added on the cursor
               positions. We found that this had a negative impact on
               efficiency. However, the user remained in control and the noise
               did not seem to effect the error rate. Additionally, three
               subjects tested the effects of temporally adding noise to
               simulate latency in the gaze tracker. Even with a significant
               latency (about 200 ms) the subjects were able to type at
               acceptable rates. In a second test, seven subjects were allowed
               to adjust the zooming speed themselves. They achieved typing
               rates of more than eight words per minute without using language
               modeling. We conclude that the StarGazer application is an
               intuitive 3D interface for gaze navigation, allowing more
               selectable objects to be displayed on the screen than the
               accuracy of the gaze trackers would otherwise permit.",
  series    = "ETRA '08",
  month     =  "26~" # mar,
  year      =  2008,
  url       = "https://doi.org/10.1145/1344471.1344521",
  doi       = "10.1145/1344471.1344521",
  isbn      =  9781595939821
}

@INPROCEEDINGS{Ishii2008-of,
  title     = "{Tangible bits}",
  author    = "Ishii, Hiroshi",
  booktitle = "{Proceedings of the 2nd international conference on Tangible and
               embedded interaction - TEI '08}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  year      =  2008,
  url       = "https://doi.org/10.1145%2F1347390.1347392",
  doi       = "10.1145/1347390.1347392",
  isbn      =  9781605580043
}

@INPROCEEDINGS{Kobayashi2008-sf,
  title     = "{Ninja cursors: using multiple cursors to assist target
               acquisition on large screens}",
  author    = "Kobayashi, Masatomo and Igarashi, Takeo",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "949–958",
  abstract  = "We propose the “ninja cursor” to improve the performance of
               target acquisition, particularly on large screens. This technique
               uses multiple distributed cursors to reduce the average distance
               to targets. Each cursor moves synchronously following mouse
               movement. We present the design and implementation of the
               proposed technique, including a method to resolve the ambiguity
               that results when multiple cursors indicate different targets
               simultaneously. We also conducted an experiment to assess the
               performance of the ninja cursor. The results indicate that it can
               generally reduce movement time. However, the performance is
               greatly affected by the number of cursors and target density.
               Based on these results, we discuss how our technique can be put
               into practical use. In addition to presenting a novel method to
               improve pointing performance, our study is the first to explore a
               variable number of cursors for performing pointing tasks.",
  series    = "CHI '08",
  month     =  "4~" # jun,
  year      =  2008,
  url       = "https://doi.org/10.1145/1357054.1357201",
  file      = "All Papers/My Library/Kobayashi and Igarashi 2008 - Ninja cursors - using multiple cursors to assist target acquisition on large screens.pdf",
  doi       = "10.1145/1357054.1357201",
  isbn      =  9781605580111
}

@INPROCEEDINGS{Rodden2008-cp,
  title     = "{Eye-mouse coordination patterns on web search results pages}",
  author    = "Rodden, Kerry and Fu, Xin and Aula, Anne and Spiro, Ian",
  booktitle = "{CHI '08 extended abstracts on human factors in computing
               systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2997–3002",
  abstract  = "We analyzed the patterns of coordination between users' eye
               movements and mouse movements when scanning a web search results
               page, using data gathered from a study with 32 participants. We
               discovered 3 patterns of active mouse usage: following the eye
               vertically with the mouse, following the eye horizontally with
               the mouse, and using the mouse to mark a promising result.",
  series    = "CHI EA '08",
  month     =  "4~" # may,
  year      =  2008,
  url       = "https://doi.org/10.1145/1358628.1358797",
  file      = "All Papers/My Library/Rodden et al. 2008 - Eye-mouse coordination patterns on web search results pages.pdf",
  doi       = "10.1145/1358628.1358797",
  isbn      =  9781605580128
}

@INPROCEEDINGS{Mateo2008-qd,
  title     = "{Gaze beats mouse: hands-free selection by combining gaze and
               emg}",
  author    = "Mateo, Julio C and San Agustin, Javier and Hansen, John Paulin",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "3039–3044",
  abstract  = "Facial EMG for selection is fast, easy and, combined with gaze
               pointing, it can provide completely hands-free interaction. In
               this pilot study, 5 participants performed a simple
               point-and-select task using mouse or gaze for pointing and a
               mouse button or a facial-EMG switch for selection. Gaze pointing
               was faster than mouse pointing, while maintaining a similar error
               rate. EMG and mouse-button selection had a comparable
               performance. From analyses of completion time, throughput and
               error rates, we concluded that the combination of gaze and facial
               EMG holds potential for outperforming the mouse.",
  series    = "CHI EA '08",
  month     =  "5~" # apr,
  year      =  2008,
  url       = "https://doi.org/10.1145/1358628.1358804",
  doi       = "10.1145/1358628.1358804",
  isbn      =  9781605580128
}

@INPROCEEDINGS{Choi2008-tg,
  title     = "{Laser pointers and a touch screen: intuitive interfaces for
               autonomous mobile manipulation for the motor impaired}",
  author    = "Choi, Young Sang and Anderson, Cressel D and Glass, Jonathan D
               and Kemp, Charles C",
  booktitle = "{Proceedings of the 10th international ACM SIGACCESS conference
               on Computers and accessibility}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "225–232",
  abstract  = "El-E (“Ellie”) is a prototype assistive robot designed to help
               people with severe motor impairments manipulate everyday objects.
               When given a 3D location, El-E can autonomously approach the
               location and pick up a nearby object. Based on interviews of
               patients with amyotrophic lateral sclerosis (ALS), we have
               developed and tested three distinct interfaces that enable a user
               to provide a 3D location to El-E and thereby select an object to
               be manipulated: an ear-mounted laser pointer, a hand-held laser
               pointer, and a touch screen interface. Within this paper, we
               present the results from a user study comparing these three user
               interfaces with a total of 134 trials involving eight patients
               with varying levels of impairment recruited from the Emory ALS
               Clinic. During this study, participants used the three interfaces
               to select everyday objects to be approached, grasped, and lifted
               off of the ground.The three interfaces enabled motor impaired
               users to command a robot to pick up an object with a 94.8\%
               success rate overall after less than 10 minutes of learning to
               use each interface. On average, users selected objects 69\% more
               quickly with the laser pointer interfaces than with the touch
               screen interface. We also found substantial variation in user
               preference. With respect to the Revised ALS Functional Rating
               Scale (ALSFRS-R), users with greater upper-limb mobility tended
               to prefer the hand-held laser pointer, while those with less
               upper-limb mobility tended to prefer the ear-mounted laser
               pointer. Despite the extra efficiency of the laser pointer
               interfaces, three patients preferred the touch screen interface,
               which has unique potential for manipulating remote objects out of
               the user's line of sight. In summary, these results indicate that
               robots can enhance accessibility by supporting multiple
               interfaces. Furthermore, this work demonstrates that the
               communication of 3D locations during human-robot interaction can
               serve as a powerful abstraction barrier that supports distinct
               interfaces to assistive robots while using identical, underlying
               robotic functionality.",
  series    = "Assets '08",
  month     =  "13~" # oct,
  year      =  2008,
  url       = "https://doi.org/10.1145/1414471.1414512",
  doi       = "10.1145/1414471.1414512",
  isbn      =  9781595939760
}

@INPROCEEDINGS{Sellen1992-xc,
  title     = "{Speech patterns in video-mediated conversations}",
  author    = "Sellen, Abigail J",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "49--59",
  abstract  = "This paper reports on the first of a series of analyses aimed at
               comparing same room and video-mediated conversations for
               multiparty meetings. This study compared patterns of spontaneous
               speech for same room versus two video-mediated conversations. One
               video system used a single camera, monitor and speaker, and a
               picture-in-a-picture device to display multiple people on one
               screen. The other system used multiple cameras, monitors, and
               speakers in order to support directional gaze cues and selective
               listening. Differences were found between same room and
               video-mediated conversations in terms of floor control and amount
               of simultaneous speech. While no differences were found between
               the video systems in terms of objective speech measures, other
               important differences are suggested and discussed.",
  series    = "CHI '92",
  month     =  jun,
  year      =  1992,
  url       = "http://dx.doi.org/10.1145/142750.142756",
  file      = "All Papers/Other/Sellen 1992 - Speech patterns in video-mediated conversations.pdf",
  keywords  = "CSCW, conversation patterns, videoconferencing",
  doi       = "10.1145/142750.142756",
  isbn      =  9780897915137
}

@INPROCEEDINGS{Sellen1992-vd,
  title     = "{Speech patterns in video-mediated conversations}",
  author    = "Sellen, Abigail J",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "49–59",
  abstract  = "This paper reports on the first of a series of analyses aimed at
               comparing same room and video-mediated conversations for
               multiparty meetings. This study compared patterns of spontaneous
               speech for same room versus two video-mediated conversations. One
               video system used a single camera, monitor and speaker, and a
               picture-in-a-picture device to display multiple people on one
               screen. The other system used multiple cameras, monitors, and
               speakers in order to support directional gaze cues and selective
               listening. Differences were found between same room and
               video-mediated conversations in terms of floor control and amount
               of simultaneous speech. While no differences were found between
               the video systems in terms of objective speech measures, other
               important differences are suggested and discussed.",
  series    = "CHI '92",
  month     =  jun,
  year      =  1992,
  url       = "http://dx.doi.org/10.1145/142750.142756",
  file      = "All Papers/My Library/Sellen 1992 - Speech patterns in video-mediated conversations.pdf",
  doi       = "10.1145/142750.142756",
  isbn      =  9780897915137
}

@INPROCEEDINGS{Ishii1992-nk,
  title     = "{{ClearBoard}: a seamless medium for shared drawing and
               conversation with eye contact}",
  author    = "Ishii, Hiroshi and Kobayashi, Minoru",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "525--532",
  abstract  = "This paper introduces a novel shared drawing medium called
               ClearBoard. It realizes (1) a seamless shared drawing space and
               (2) eye contact to support realtime and remote collaboration by
               two users. We devised the key metaphor: ``talking through and
               drawing on a transparent glass window'' to design ClearBoard. A
               prototype of ClearBoard is implemented based on the
               ``Drafter-Mirror'' architecture. This paper first reviews
               previous work on shared drawing support to clarify the design
               goals. We then examine three methaphors that fulfill these goals.
               The design requirements and the two possible system architectures
               of ClearBoard are described. Finally, some findings gained
               through the experimental use of the prototype, including the
               feature of ``gaze awareness'', are discussed.",
  series    = "CHI '92",
  month     =  jun,
  year      =  1992,
  url       = "http://dx.doi.org/10.1145/142750.142977",
  file      = "All Papers/Other/Ishii and Kobayashi 1992 - ClearBoard - a seamless medium for shared drawing and conversation with eye contact.pdf",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/142750.142977",
  isbn      =  9780897915137
}

@INPROCEEDINGS{Ishii1992-ox,
  title     = "{ClearBoard: a seamless medium for shared drawing and
               conversation with eye contact}",
  author    = "Ishii, Hiroshi and Kobayashi, Minoru",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "525–532",
  abstract  = "This paper introduces a novel shared drawing medium called
               ClearBoard. It realizes (1) a seamless shared drawing space and
               (2) eye contact to support realtime and remote collaboration by
               two users. We devised the key metaphor: “talking through and
               drawing on a transparent glass window” to design ClearBoard. A
               prototype of ClearBoard is implemented based on the
               “Drafter-Mirror” architecture. This paper first reviews previous
               work on shared drawing support to clarify the design goals. We
               then examine three methaphors that fulfill these goals. The
               design requirements and the two possible system architectures of
               ClearBoard are described. Finally, some findings gained through
               the experimental use of the prototype, including the feature of
               “gaze awareness”, are discussed.",
  series    = "CHI '92",
  year      =  1992,
  url       = "https://dl.acm.org/doi/10.1145/142750.142977",
  file      = "All Papers/My Library/Ishii and Kobayashi 1992 - ClearBoard - a seamless medium for shared drawing and conversation with eye contact.pdf",
  doi       = "10.1145/142750.142977",
  isbn      =  9780897915137
}

@INPROCEEDINGS{Ishii1992-ib,
  title     = "{ClearBoard: a seamless medium for shared drawing and
               conversation with eye contact}",
  author    = "Ishii, Hiroshi and Kobayashi, Minoru",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "525–532",
  abstract  = "This paper introduces a novel shared drawing medium called
               ClearBoard. It realizes (1) a seamless shared drawing space and
               (2) eye contact to support realtime and remote collaboration by
               two users. We devised the key metaphor: “talking through and
               drawing on a transparent glass window” to design ClearBoard. A
               prototype of ClearBoard is implemented based on the
               “Drafter-Mirror” architecture. This paper first reviews previous
               work on shared drawing support to clarify the design goals. We
               then examine three methaphors that fulfill these goals. The
               design requirements and the two possible system architectures of
               ClearBoard are described. Finally, some findings gained through
               the experimental use of the prototype, including the feature of
               “gaze awareness”, are discussed.",
  series    = "CHI '92",
  month     =  jun,
  year      =  1992,
  url       = "http://dx.doi.org/10.1145/142750.142977",
  file      = "All Papers/My Library/Ishii and Kobayashi 1992 - ClearBoard - a seamless medium for shared drawing and conversation with eye contact.pdf",
  doi       = "10.1145/142750.142977",
  isbn      =  9780897915137
}

@INPROCEEDINGS{Sellen1992-sn,
  title     = "{Using spatial cues to improve videoconferencing}",
  author    = "Sellen, Abigail and Buxton, Bill and Arnott, John",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "651--652",
  abstract  = "… Because each participant has a single monitor, camera, and
               loudspeaker, PIP videoconferences are limited in their support of
               participants' ability to: … Hydra units are, in effect, ``video
               surrogates'' for the participants, occupying the physical space
               that would be held by people, if they were physically present.
               The technique used is similar to that of Fields (1983), although
               it was developed independently. The result of this technique is
               that each participant is presented with a unique view of each
               remote participant, and that view …",
  series    = "CHI '92",
  month     =  jun,
  year      =  1992,
  url       = "http://dx.doi.org/10.1145/142750.143070",
  file      = "All Papers/Other/Sellen et al. 1992 - Using spatial cues to improve videoconferencing.pdf",
  doi       = "10.1145/142750.143070",
  isbn      =  9780897915137
}

@INPROCEEDINGS{Sellen1992-qn,
  title     = "{Using spatial cues to improve videoconferencing}",
  author    = "Sellen, Abigail and Buxton, Bill and Arnott, John",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "651–652",
  abstract  = "… Because each participant has a single monitor, camera, and
               loudspeaker, PIP videoconferences are limited in their support of
               participants' ability to: … Hydra units are, in effect, “video
               surrogates” for the participants, occupying the physical space
               that would be held by people, if they were physically present.
               The technique used is similar to that of Fields (1983), although
               it was developed independently. The result of this technique is
               that each participant is presented with a unique view of each
               remote participant, and that view …",
  series    = "CHI '92",
  month     =  jun,
  year      =  1992,
  url       = "http://dx.doi.org/10.1145/142750.143070",
  file      = "All Papers/My Library/Sellen et al. 1992 - Using spatial cues to improve videoconferencing.pdf",
  doi       = "10.1145/142750.143070",
  isbn      =  9780897915137
}

@INPROCEEDINGS{Izadi2008-xg,
  title     = "{Going beyond the display: a surface technology with an
               electronically switchable diffuser}",
  author    = "Izadi, Shahram and Hodges, Steve and Taylor, Stuart and
               Rosenfeld, Dan and Villar, Nicolas and Butler, Alex and Westhues,
               Jonathan",
  booktitle = "{Proceedings of the 21st annual ACM symposium on User interface
               software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "19~" # oct,
  year      =  2008,
  url       = "https://dl.acm.org/doi/10.1145/1449715.1449760",
  file      = "All Papers/Other/Izadi et al. 2008 - Going beyond the display - a surface technology with an electronically switchable diffuser.pdf",
  doi       = "10.1145/1449715.1449760",
  isbn      =  9781595939753,
  language  = "en"
}

@INPROCEEDINGS{Vertegaal2008-vt,
  title     = "{A Fitts Law comparison of eye tracking and manual input in the
               selection of visual targets}",
  author    = "Vertegaal, Roel",
  booktitle = "{Proceedings of the 10th international conference on Multimodal
               interfaces}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "241–248",
  abstract  = "We present a Fitts' Law evaluation of a number of eye tracking
               and manual input devices in the selection of large visual
               targets. We compared performance of two eye tracking techniques,
               manual click and dwell time click, with that of mouse and stylus.
               Results show eye tracking with manual click outperformed the
               mouse by 16\%, with dwell time click 46\% faster. However, eye
               tracking conditions suffered a high error rate of 11.7\% for
               manual click and 43\% for dwell time click conditions. After
               Welford correction eye tracking still appears to outperform
               manual input, with IPs of 13.8 bits/s for dwell time click, and
               10.9 bits/s for manual click. Eye tracking with manual click
               provides the best tradeoff between speed and accuracy, and was
               preferred by 50\% of participants. Mouse and stylus had IPs of
               4.7 and 4.2 respectively. However, their low error rate of 5\%
               makes these techniques more suitable for refined target
               selection.",
  series    = "ICMI '08",
  month     =  "20~" # oct,
  year      =  2008,
  url       = "https://doi.org/10.1145/1452392.1452443",
  file      = "All Papers/My Library/Vertegaal 2008 - A Fitts Law comparison of eye tracking and manual input in the selection of visual targets.pdf",
  doi       = "10.1145/1452392.1452443",
  isbn      =  9781605581989
}

@INPROCEEDINGS{Majaranta2009-jd,
  title     = "{Fast gaze typing with an adjustable dwell time}",
  author    = "Majaranta, Päivi and Ahola, Ulla-Kaija and Špakov, Oleg",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "357–360",
  abstract  = "Previous research shows that text entry by gaze using dwell time
               is slow, about 5-10 words per minute (wpm). These results are
               based on experiments with novices using a constant dwell time,
               typically between 450 and 1000 ms. We conducted a longitudinal
               study to find out how fast novices learn to type by gaze using an
               adjustable dwell time. Our results show that the text entry rate
               increased from 6.9 wpm in the first session to 19.9 wpm in the
               tenth session. Correspondingly, the dwell time decreased from an
               average of 876 ms to 282 ms, and the error rates decreased from
               1.28\% to .36\%. The achieved typing speed of nearly 20 wpm is
               comparable with the result of 17.3 wpm achieved in an earlier,
               similar study with Dasher.",
  series    = "CHI '09",
  month     =  "4~" # apr,
  year      =  2009,
  url       = "https://doi.org/10.1145/1518701.1518758",
  doi       = "10.1145/1518701.1518758",
  isbn      =  9781605582467
}

@INPROCEEDINGS{Nguyen2009-sn,
  title     = "{More than face-to-face: empathy effects of video framing}",
  author    = "Nguyen, David T and Canny, John",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "423--432",
  abstract  = "Video conferencing attempts to convey subtle cues of face-to-face
               interaction (F2F), but it is generally believed to be less
               effective than F2F. We argue that careful design based on an
               understanding of non-verbal communication can mitigate these
               differences. In this paper, we study the effects of video image
               framing in one-on-one meetings on empathy formation. We alter the
               video image by framing the display such that, in one condition,
               only the head is visible while, in the other condition, the
               entire upper body is visible. We include a F2F control case. We
               used two measures of dyad empathy and found a significant
               difference between head-only framing and both upper-body framing
               and F2F, but no significant difference between upper-body framing
               and F2F.Based on these and earlier results, we present some
               design heuristics for video conferencing systems. We revisit
               earlier negative experimental results on video systems in the
               light of these new experiments. We conclude that for systems that
               preserve both gaze and upper-body cues, there is no evidence of
               deficit in communication effectiveness compared to face-to-face
               meetings.",
  series    = "CHI '09",
  month     =  apr,
  year      =  2009,
  url       = "http://dx.doi.org/10.1145/1518701.1518770",
  keywords  = "video conferencing, oneness, empathy",
  doi       = "10.1145/1518701.1518770",
  isbn      =  9781605582467
}

@INPROCEEDINGS{Nguyen2009-er,
  title     = "{More than face-to-face: empathy effects of video framing}",
  author    = "Nguyen, David T and Canny, John",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "423–432",
  abstract  = "Video conferencing attempts to convey subtle cues of face-to-face
               interaction (F2F), but it is generally believed to be less
               effective than F2F. We argue that careful design based on an
               understanding of non-verbal communication can mitigate these
               differences. In this paper, we study the effects of video image
               framing in one-on-one meetings on empathy formation. We alter the
               video image by framing the display such that, in one condition,
               only the head is visible while, in the other condition, the
               entire upper body is visible. We include a F2F control case. We
               used two measures of dyad empathy and found a significant
               difference between head-only framing and both upper-body framing
               and F2F, but no significant difference between upper-body framing
               and F2F.Based on these and earlier results, we present some
               design heuristics for video conferencing systems. We revisit
               earlier negative experimental results on video systems in the
               light of these new experiments. We conclude that for systems that
               preserve both gaze and upper-body cues, there is no evidence of
               deficit in communication effectiveness compared to face-to-face
               meetings.",
  series    = "CHI '09",
  month     =  apr,
  year      =  2009,
  url       = "http://dx.doi.org/10.1145/1518701.1518770",
  doi       = "10.1145/1518701.1518770",
  isbn      =  9781605582467
}

@INPROCEEDINGS{Wigdor2009-qd,
  title     = "{WeSpace}",
  author    = "Wigdor, Daniel and Jiang, Hao and Forlines, Clifton and Borkin,
               Michelle and Shen, Chia",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "4~" # apr,
  year      =  2009,
  url       = "https://doi.org/10.1145%2F1518701.1518886",
  doi       = "10.1145/1518701.1518886",
  isbn      =  9781605582467
}

@INPROCEEDINGS{Blanch2009-lv,
  title     = "{Rake cursor: improving pointing performance with concurrent
               input channels}",
  author    = "Blanch, Renaud and Ortega, Michaël",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1415–1418",
  abstract  = "We investigate the use of two concurrent input channels to
               perform a pointing task. The first channel is the traditional
               mouse input device whereas the second one is the gaze position.
               The rake cursor interaction technique combines a grid of cursors
               controlled by the mouse and the selection of the active cursor by
               the gaze. A controlled experiment shows that rake cursor pointing
               drastically outperforms mouse-only pointing and also
               significantly outperforms the state of the art of pointing
               techniques mixing gaze and mouse input. A theory explaining the
               improvement is proposed: the global difficulty of a task is split
               between those two channels, and the sub-tasks could partly be
               performed concurrently.",
  series    = "CHI '09",
  month     =  "4~" # apr,
  year      =  2009,
  url       = "https://doi.org/10.1145/1518701.1518914",
  file      = "All Papers/My Library/Blanch and Ortega 2009 - Rake cursor - improving pointing performance with concurrent input channels.pdf",
  doi       = "10.1145/1518701.1518914",
  isbn      =  9781605582467
}

@INPROCEEDINGS{Bulling2009-dx,
  title     = "{Wearable EOG goggles: eye-based interaction in everyday
               environments}",
  author    = "Bulling, Andreas and Roggen, Daniel and Tröster, Gerhard",
  booktitle = "{CHI '09 Extended Abstracts on Human Factors in Computing
               Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "3259–3264",
  abstract  = "In this paper, we present an embedded eye tracker for
               context-awareness and eye-based human-computer interaction - the
               wearable EOG goggles. In contrast to common systems using video,
               this unobtrusive device relies on Electrooculography (EOG). It
               consists of goggles with dry electrodes integrated into the frame
               and a small pocket-worn component with a powerful microcontroller
               for EOG signal processing. Using this lightweight system,
               sequences of eye movements, so-called eye gestures, can be
               efficiently recognised from EOG signals in real-time for HCI
               purposes. The device is self-contained solution and allows for
               seamless eye motion sensing, context-recognition and eye-based
               interaction in everyday environments.",
  series    = "CHI EA '09",
  year      =  2009,
  url       = "https://dl.acm.org/doi/10.1145/1520340.1520468",
  file      = "All Papers/My Library/Bulling et al. 2009 - Wearable EOG goggles - eye-based interaction in everyday environments.pdf",
  doi       = "10.1145/1520340.1520468",
  isbn      =  9781605582474
}

@INPROCEEDINGS{Tall2009-pn,
  title     = "{Gaze-controlled driving}",
  author    = "Tall, Martin and Alapetite, Alexandre and San Agustin, Javier and
               Skovsgaard, Henrik H T and Hansen, John Paulin and Hansen, Dan
               Witzner and Møllenbach, Emilie",
  booktitle = "{CHI '09 extended abstracts on human factors in computing
               systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "4387–4392",
  abstract  = "We investigate if the gaze (point of regard) can control a remote
               vehicle driving on a racing track. Five different input devices
               (on-screen buttons, mouse-pointing low-cost webcam eye tracker
               and two commercial eye tracking systems) provide heading and
               speed control on the scene view transmitted from the moving
               robot. Gaze control was found to be similar to mouse control.
               This suggests that robots and wheelchairs may be controlled
               “hands-free” through gaze. Low precision gaze tracking and image
               transmission delays had noticeable effect on performance.",
  series    = "CHI EA '09",
  month     =  "4~" # apr,
  year      =  2009,
  url       = "https://doi.org/10.1145/1520340.1520671",
  file      = "All Papers/My Library/Tall et al. 2009 - Gaze-controlled driving.pdf",
  doi       = "10.1145/1520340.1520671",
  isbn      =  9781605582474
}

@ARTICLE{Jones2009-jp,
  title     = "{Achieving eye contact in a one-to-many {3D} video
               teleconferencing system}",
  author    = "Jones, Andrew and Lang, Magnus and Fyffe, Graham and Yu, Xueming
               and Busch, Jay and McDowall, Ian and Bolas, Mark and Debevec,
               Paul",
  journal   = "ACM transactions on graphics",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  28,
  number    =  3,
  pages     = "1--8",
  abstract  = "We present a set of algorithms and an associated display system
               capable of producing correctly rendered eye contact between a
               three-dimensionally transmitted remote participant and a group of
               observers in a 3D teleconferencing system. The participant's face
               is scanned in 3D at 30Hz and transmitted in real time to an
               autostereoscopic horizontal-parallax 3D display, displaying him
               or her over more than a 180° field of view observable to multiple
               observers. To render the geometry with correct perspective, we
               create a fast vertex shader based on a 6D lookup table for
               projecting 3D scene vertices to a range of subject angles,
               heights, and distances. We generalize the projection mathematics
               to arbitrarily shaped display surfaces, which allows us to employ
               a curved concave display surface to focus the high speed imagery
               to individual observers. To achieve two-way eye contact, we
               capture 2D video from a cross-polarized camera reflected to the
               position of the virtual participant's eyes, and display this 2D
               video feed on a large screen in front of the real participant,
               replicating the viewpoint of their virtual self. To achieve
               correct vertical perspective, we further leverage this image to
               track the position of each audience member's eyes, allowing the
               3D display to render correct vertical perspective for each of the
               viewers around the device. The result is a one-to-many 3D
               teleconferencing system able to reproduce the effects of gaze,
               attention, and eye contact generally missing in traditional
               teleconferencing systems.",
  month     =  jul,
  year      =  2009,
  url       = "http://dx.doi.org/10.1145/1531326.1531370",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/1531326.1531370",
  issn      = "0730-0301"
}

@ARTICLE{Jones2009-cs,
  title    = "{Achieving eye contact in a one-to-many 3D video teleconferencing
              system}",
  author   = "Jones, Andrew and Lang, Magnus and Fyffe, Graham and Yu, Xueming
              and Busch, Jay and McDowall, Ian and Bolas, Mark and Debevec, Paul",
  journal  = "ACM transactions on graphics",
  volume   =  28,
  number   =  3,
  pages    = "1–8",
  abstract = "We present a set of algorithms and an associated display system
              capable of producing correctly rendered eye contact between a
              three-dimensionally transmitted remote participant and a group of
              observers in a 3D teleconferencing system. The participant's face
              is scanned in 3D at 30Hz and transmitted in real time to an
              autostereoscopic horizontal-parallax 3D display, displaying him or
              her over more than a 180° field of view observable to multiple
              observers. To render the geometry with correct perspective, we
              create a fast vertex shader based on a 6D lookup table for
              projecting 3D scene vertices to a range of subject angles,
              heights, and distances. We generalize the projection mathematics
              to arbitrarily shaped display surfaces, which allows us to employ
              a curved concave display surface to focus the high speed imagery
              to individual observers. To achieve two-way eye contact, we
              capture 2D video from a cross-polarized camera reflected to the
              position of the virtual participant's eyes, and display this 2D
              video feed on a large screen in front of the real participant,
              replicating the viewpoint of their virtual self. To achieve
              correct vertical perspective, we further leverage this image to
              track the position of each audience member's eyes, allowing the 3D
              display to render correct vertical perspective for each of the
              viewers around the device. The result is a one-to-many 3D
              teleconferencing system able to reproduce the effects of gaze,
              attention, and eye contact generally missing in traditional
              teleconferencing systems.",
  month    =  jul,
  year     =  2009,
  url      = "http://dx.doi.org/10.1145/1531326.1531370",
  doi      = "10.1145/1531326.1531370",
  issn     = "0730-0301"
}

@INPROCEEDINGS{Tsai2009-if,
  title     = "{Eye-writing communication for patients with amyotrophic lateral
               sclerosis}",
  author    = "Tsai, Jang-Zern and Chen, Tsai-Shih",
  booktitle = "{Proceedings of the 11th international ACM SIGACCESS conference
               on Computers and accessibility}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "239–240",
  abstract  = "The eye-writing method predefines a symbol set containing symbols
               with distinct writing traces. A user of this method rotates his
               or her eye balls to “write” a symbol according to its designated
               writing trace. Meanwhile, the eye movement is detected using a
               suitable technique such as the electro-oculography, which
               measures voltage differences on the skin around the user's eyes.
               Distinct features of the acquired eye-movement signals are
               extracted in order to determine which symbol, among those in the
               symbol set, the user's eyes have just written. An eye-writing
               system has been implemented in this study. Tests on subjects with
               no known disabilities have been conducted and the performance has
               been evaluated. The study found that eye-writing system is
               potentially useful for facilitating communication of sever ALS
               patients who have lost most of their oral speaking and
               handwriting abilities.",
  series    = "Assets '09",
  month     =  "25~" # oct,
  year      =  2009,
  url       = "https://doi.org/10.1145/1639642.1639694",
  doi       = "10.1145/1639642.1639694",
  isbn      =  9781605585581
}

@INPROCEEDINGS{Bader2009-fh,
  title     = "{Multimodal integration of natural gaze behavior for intention
               recognition during object manipulation}",
  author    = "Bader, Thomas and Vogelgesang, Matthias and Klaus, Edmund",
  booktitle = "{Proceedings of the 2009 international conference on Multimodal
               interfaces}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "199–206",
  abstract  = "Naturally gaze is used for visual perception of our environment
               and gaze movements are mainly controlled subconsciously. Forcing
               the user to consciously diverge from that natural gaze behavior
               for interaction purposes causes high cognitive workload and
               destroys information contained in natural gaze movements. Instead
               of proposing a new gaze-based interaction technique, we analyze
               natural gaze behavior during an object manipulation task and show
               ways how it can be used for intention recognition, which provides
               a universal basis for integrating gaze into multimodal interfaces
               for different applications. We propose a model for multimodal
               integration of natural gaze behavior and evaluate it for two
               different use cases, namely for improvement of robustness of
               other potentially noisy input cues and for the design of
               proactive interaction techniques.",
  series    = "ICMI-MLMI '09",
  year      =  2009,
  url       = "https://dl.acm.org/doi/10.1145/1647314.1647350",
  file      = "All Papers/My Library/Bader et al. 2009 - Multimodal integration of natural gaze behavior for intention recognition during object manipulation.pdf",
  doi       = "10.1145/1647314.1647350",
  isbn      =  9781605587721
}

@INPROCEEDINGS{Skovsgaard2010-an,
  title     = "{Small-target selection with gaze alone}",
  author    = "Skovsgaard, Henrik and Mateo, Julio C and Flach, John M and
               Hansen, John Paulin",
  booktitle = "{Proceedings of the 2010 Symposium on Eye-Tracking Research \&
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "145–148",
  abstract  = "Accessing the smallest targets in mainstream interfaces using
               gaze alone is difficult, but interface tools that effectively
               increase the size of selectable objects can help. In this paper,
               we propose a conceptual framework to organize existing tools and
               guide the development of new tools. We designed a discrete zoom
               tool and conducted a proof-of-concept experiment to test the
               potential of the framework and the tool. Our tool was as fast as
               and more accurate than the currently available two-step
               magnification tool. Our framework shows potential to guide the
               design, development, and testing of zoom tools to facilitate the
               accessibility of mainstream interfaces for gaze users.",
  series    = "ETRA '10",
  year      =  2010,
  url       = "https://dl.acm.org/doi/10.1145/1743666.1743702",
  doi       = "10.1145/1743666.1743702",
  isbn      =  9781605589947
}

@INPROCEEDINGS{Ashtiani2010-oz,
  title     = "{BlinkWrite2: an improved text entry method using eye blinks}",
  author    = "Ashtiani, Behrooz and MacKenzie, I Scott",
  booktitle = "{Proceedings of the 2010 symposium on eye-tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "339–345",
  abstract  = "Areas of design improvements for BlinkWrite, an eye blink text
               entry system, are examined, implemented, and evaluated. The
               result, BlinkWrite2, is a text entry system for individuals with
               severe motor impairment. Since the ability to blink is often
               preserved, even in severe conditions such as locked-in syndrome,
               BlinkWrite2 allows text entry and correction with blinks as the
               only input modality. Advantages of BlinkWrite2 over its
               predecessor include an increase in text entry speed. In a user
               evaluation, 12 participants achieved an average text entry rate
               of 5.3 wpm, representing a 16\% increase over BlinkWrite and a
               657\% increase over the next fastest video-based eye blink text
               entry system reported in the literature.",
  series    = "ETRA '10",
  month     =  "22~" # mar,
  year      =  2010,
  url       = "https://doi.org/10.1145/1743666.1743742",
  doi       = "10.1145/1743666.1743742",
  isbn      =  9781605589947
}

@INPROCEEDINGS{Zhang2010-gz,
  title     = "{Modeling dwell-based eye pointing target acquisition}",
  author    = "Zhang, Xinyong and Ren, Xiangshi and Zha, Hongbin",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2083–2092",
  abstract  = "We propose a quantitative model for dwell-based eye pointing
               tasks. Using the concepts of information theory to analogize eye
               pointing, we define an index of difficulty (IDeye) for the
               corresponding tasks in a similar manner to the definition that
               Fitts made for hand pointing. According to our validations in
               different situations, IDeye, which takes account of the distinct
               characteristics of rapid saccades and involuntary eye jitters,
               can accurately and meaningfully describe eye pointing tasks. To
               the best of our knowledge, this work is the first successful
               attempt to model eye gaze interactions.",
  series    = "CHI '10",
  month     =  "4~" # oct,
  year      =  2010,
  url       = "https://doi.org/10.1145/1753326.1753645",
  file      = "All Papers/My Library/Zhang et al. 2010 - Modeling dwell-based eye pointing target acquisition.pdf",
  doi       = "10.1145/1753326.1753645",
  isbn      =  9781605589299
}

@INPROCEEDINGS{Lee2010-xi,
  title     = "{Beyond: collapsible tools and gestures for computational design}",
  author    = "Lee, Jinha and Ishii, Hiroshi",
  booktitle = "{CHI '10 Extended Abstracts on Human Factors in Computing
               Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "3931–3936",
  abstract  = "Since the invention of the personal computer, digital media has
               remained separate from the physical world, blocked by a rigid
               screen. In this paper, we present Beyond, an interface for 3-D
               design where users can directly manipulate digital media with
               physically retractable tools and hand gestures. When pushed onto
               the screen, these tools physically collapse and project
               themselves onto the screen, letting users perceive as if they
               were inserting the tools into the digital space beyond the
               screen. The aim of Beyond is to make the digital 3-D design
               process straightforward, and more accessible to general users by
               extending physical affordances to the digital space beyond the
               computer screen.",
  series    = "CHI EA '10",
  year      =  2010,
  url       = "https://doi.org/10.1145/1753846.1754081",
  doi       = "10.1145/1753846.1754081",
  isbn      =  9781605589305
}

@INPROCEEDINGS{Okada1994-ux,
  title     = "{Multiparty videoconferencing at virtual social distance: {MAJIC}
               design}",
  author    = "Okada, Ken-Ichi and Maeda, Fumihiko and Ichikawaa, Yusuke and
               Matsushita, Yutaka",
  booktitle = "{Proceedings of the 1994 {ACM} conference on Computer supported
               cooperative work}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "385--393",
  abstract  = "This paper describes the design and implementation of MAJIC, a
               multi-party videoconferencing system that projects life-size
               video images of participants onto a large curved screen as if
               users in various locations are attending a meeting together and
               sitting around a table. MAJIC also supports multiple eye contact
               among the participants and awareness of the direction of the
               participants' gaze. Hence, users can carry on a discussion in a
               manner comparable to face-to-face meetings. We made video-tape
               recordings of about twenty visitors who used the prototype of
               MAJIC at the Nikkei Collaboration Fair in Tokyo. Our initial
               observations based on this experiment are also reported in this
               paper.",
  series    = "CSCW '94",
  month     =  oct,
  year      =  1994,
  url       = "http://dx.doi.org/10.1145/192844.193054",
  file      = "All Papers/Other/Okada et al. 1994 - Multiparty videoconferencing at virtual social distance - MAJIC design.pdf",
  keywords  = "groupware, gaze awareness, multiple eye contact, multiparty
               videoconferencing, tele-presence, networked realities,
               MAJIC;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/192844.193054",
  isbn      =  9780897916899
}

@INPROCEEDINGS{Okada1994-oa,
  title     = "{Multiparty videoconferencing at virtual social distance: MAJIC
               design}",
  author    = "Okada, Ken-Ichi and Maeda, Fumihiko and Ichikawaa, Yusuke and
               Matsushita, Yutaka",
  booktitle = "{Proceedings of the 1994 ACM conference on Computer supported
               cooperative work}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "385–393",
  abstract  = "This paper describes the design and implementation of MAJIC, a
               multi-party videoconferencing system that projects life-size
               video images of participants onto a large curved screen as if
               users in various locations are attending a meeting together and
               sitting around a table. MAJIC also supports multiple eye contact
               among the participants and awareness of the direction of the
               participants' gaze. Hence, users can carry on a discussion in a
               manner comparable to face-to-face meetings. We made video-tape
               recordings of about twenty visitors who used the prototype of
               MAJIC at the Nikkei Collaboration Fair in Tokyo. Our initial
               observations based on this experiment are also reported in this
               paper.",
  series    = "CSCW '94",
  month     =  oct,
  year      =  1994,
  url       = "http://dx.doi.org/10.1145/192844.193054",
  file      = "All Papers/My Library/Okada et al. 1994 - Multiparty videoconferencing at virtual social distance - MAJIC design.pdf",
  doi       = "10.1145/192844.193054",
  isbn      =  9780897916899
}

@INPROCEEDINGS{Stellmach2011-ot,
  title     = "{Designing gaze-supported multimodal interactions for the
               exploration of large image collections}",
  author    = "Stellmach, Sophie and Stober, Sebastian and Nürnberger, Andreas
               and Dachselt, Raimund",
  booktitle = "{Proceedings of the 1st Conference on Novel Gaze-Controlled
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "While eye tracking is becoming more and more relevant as a
               promising input channel, diverse applications using gaze control
               in a more natural way are still rather limited. Though several
               researchers have indicated the particular high potential of
               gaze-based interaction for pointing tasks, often gaze-only
               approaches are investigated. However, time-consuming dwell-time
               activations limit this potential. To overcome this, we present a
               gaze-supported fisheye lens in combination with (1) a keyboard
               and (2) and a tilt-sensitive mobile multi-touch device. In a
               user-centered design approach, we elicited how users would use
               the aforementioned input combinations. Based on the received
               feedback we designed a prototype system for the interaction with
               a remote display using gaze and a touch-and-tilt device. This
               eliminates gaze dwell-time activations and the well-known Midas
               Touch problem (unintentionally issuing an action via gaze). A
               formative user study testing our prototype provided further
               insights into how well the elaborated gaze-supported interaction
               techniques were experienced by users.",
  series    = "NGCA '11",
  year      =  2011,
  url       = "https://dl.acm.org/doi/10.1145/1983302.1983303",
  doi       = "10.1145/1983302.1983303",
  isbn      =  9781450306805
}

@INPROCEEDINGS{Van_der_Kamp2011-ez,
  title     = "{Gaze and voice controlled drawing}",
  author    = "van der Kamp, Jan and Sundstedt, Veronica",
  booktitle = "{Proceedings of the 1st Conference on Novel {Gaze-Controlled}
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--8",
  abstract  = "Eye tracking is a process that allows an observers gaze to be
               determined in real time by measuring their eye movements. Recent
               work has examined the possibility of using gaze control as an
               alternative input modality in interactive applications.
               Alternative means of interaction are especially important for
               disabled users for whom traditional techniques, such as mouse and
               keyboard, may not be feasible. This paper proposes a novel
               combination of gaze and voice commands as a means of hands free
               interaction in a paint style program. A drawing application is
               implemented which is controllable by input from gaze and voice.
               Voice commands are used to activate drawing which allow gaze to
               be used only for positioning the cursor. In previous work gaze
               has also been used to activate drawing using dwell time. The
               drawing application is evaluated using subjective responses from
               participant user trials. The main result indicates that although
               gaze and voice offered less control that traditional input
               devices, the participants reported that it was more enjoyable.",
  series    = "NGCA '11",
  month     =  may,
  year      =  2011,
  url       = "http://dx.doi.org/10.1145/1983302.1983311",
  file      = "All Papers/Other/van der Kamp and Sundstedt 2011 - Gaze and voice controlled drawing.pdf",
  keywords  = "drawing, eye tracking, gaze based interaction;prj-gaze-shorthand",
  doi       = "10.1145/1983302.1983311",
  isbn      =  9781450306805
}

@INPROCEEDINGS{Van_der_Kamp2011-ot,
  title     = "{Gaze and voice controlled drawing}",
  author    = "van der Kamp, Jan and Sundstedt, Veronica",
  booktitle = "{Proceedings of the 1st conference on novel Gaze-Controlled
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Eye tracking is a process that allows an observers gaze to be
               determined in real time by measuring their eye movements. Recent
               work has examined the possibility of using gaze control as an
               alternative input modality in interactive applications.
               Alternative means of interaction are especially important for
               disabled users for whom traditional techniques, such as mouse and
               keyboard, may not be feasible. This paper proposes a novel
               combination of gaze and voice commands as a means of hands free
               interaction in a paint style program. A drawing application is
               implemented which is controllable by input from gaze and voice.
               Voice commands are used to activate drawing which allow gaze to
               be used only for positioning the cursor. In previous work gaze
               has also been used to activate drawing using dwell time. The
               drawing application is evaluated using subjective responses from
               participant user trials. The main result indicates that although
               gaze and voice offered less control that traditional input
               devices, the participants reported that it was more enjoyable.",
  series    = "NGCA '11",
  month     =  "26~" # may,
  year      =  2011,
  url       = "https://doi.org/10.1145/1983302.1983311",
  file      = "All Papers/My Library/van der Kamp and Sundstedt 2011 - Gaze and voice controlled drawing.pdf",
  doi       = "10.1145/1983302.1983311",
  isbn      =  9781450306805
}

@INPROCEEDINGS{Yagi2010-gg,
  title     = "{Eye-gaze interfaces using electro-oculography (EOG)}",
  author    = "Yagi, Tohru",
  booktitle = "{Proceedings of the 2010 workshop on Eye gaze in intelligent
               human machine interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "28–32",
  abstract  = "Using electro-oculography (EOG), two types of eye-gaze interfaces
               have been developed; “EOG Pointer” and “EOG Switch”. The former
               enables a user to move a computer cursor or to control a machine
               using only eye-gaze, regardless of drifting signal and blinking
               artifacts. In contrast, the latter output an ON/OFF signal only.
               Although it has the least simple function, it enables every user
               easily to turn ON/OFF a nurse-call device or to send one bit
               signal to a personal computer with high stability and
               reliability. Since the EOG Switch was commercialized in 2003, it
               has been widely used among amyotrophic lateral sclerosis (ALS)
               patients in Japan.",
  series    = "EGIHMI '10",
  month     =  "2~" # jul,
  year      =  2010,
  url       = "https://doi.org/10.1145/2002333.2002338",
  doi       = "10.1145/2002333.2002338",
  isbn      =  9781605589992
}

@INPROCEEDINGS{Mauro2011-zp,
  title     = "{Spatial attention orienting to improve the efficacy of a
               brain-computer interface for communication}",
  author    = "Mauro, Marchetti and Francesco, Piccione and Stefano, Silvoni and
               Luciano, Gamberini and Konstantinos, Priftis",
  booktitle = "{Proceedings of the 9th ACM SIGCHI italian chapter international
               conference on computer-human interaction: Facing complexity}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "114–117",
  abstract  = "Brain-computer interfaces (BCIs) are systems which allow users to
               control devices, by means of their brain signals, without the
               involvement of the users' muscles. BCIs represent a potential
               solution for completely paralyzed patients who cannot
               communicate. We designed two new visual interfaces for
               controlling the movement of a virtual cursor on a monitor,
               implementing the cognitive principles of exogenous and endogenous
               attention orienting in a BCI driven by the P300. A group of
               patients with amyotrophic lateral sclerosis (ALS) and a matched
               group of healthy controls were tested. Results show that ALS
               patients can use both interfaces for controlling the cursor,
               although they reached a better performance with the endogenous
               attention orienting interface. We propose that the implementation
               of cognitive principles can play a key role in the development of
               new and more efficient BCIs.",
  series    = "CHItaly",
  month     =  "13~" # sep,
  year      =  2011,
  url       = "https://doi.org/10.1145/2037296.2037325",
  doi       = "10.1145/2037296.2037325",
  isbn      =  9781450308762
}

@ARTICLE{Ishii2012-th,
  title    = "{Radical atoms}",
  author   = "Ishii, Hiroshi and Lakatos, Dávid and Bonanni, Leonardo and
              Labrune, Jean-Baptiste",
  journal  = "Interactions",
  volume   =  19,
  number   =  1,
  pages    = "38–51",
  month    =  jan,
  year     =  2012,
  url      = "https://doi.org/10.1145%2F2065327.2065337",
  doi      = "10.1145/2065327.2065337",
  issn     = "1072-5520,1558-3449",
  language = "en"
}

@INPROCEEDINGS{Stellmach2012-ou,
  title     = "{Designing gaze-based user interfaces for steering in virtual
               environments}",
  author    = "Stellmach, Sophie and Dachselt, Raimund",
  booktitle = "{Proceedings of the Symposium on Eye Tracking Research and
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "131–138",
  abstract  = "Since eye gaze may serve as an efficient and natural input for
               steering in virtual 3D scenes, we investigate the design of eye
               gaze steering user interfaces (UIs) in this paper. We discuss
               design considerations and propose design alternatives based on
               two selected steering approaches differing in input condition
               (discrete vs. continuous) and velocity selection (constant vs.
               gradient-based). The proposed UIs have been iteratively advanced
               based on two user studies with twelve participants each. In
               particular, the combination of continuous and gradient-based
               input shows a high potential, because it allows for gradually
               changing the moving speed and direction depending on a user's
               point-of-regard. This has the advantage of reducing overshooting
               problems and dwell-time activations. We also investigate discrete
               constant input for which virtual buttons are toggled using gaze
               dwelling. As an alternative, we propose the Sticky Gaze Pointer
               as a more flexible way of discrete input.",
  series    = "ETRA '12",
  year      =  2012,
  url       = "https://dl.acm.org/doi/10.1145/2168556.2168577",
  file      = "All Papers/My Library/Stellmach and Dachselt 2012 - Designing gaze-based user interfaces for steering in virtual environments.pdf",
  doi       = "10.1145/2168556.2168577",
  isbn      =  9781450312219
}

@INPROCEEDINGS{Liang2012-ze,
  title     = "{Eye typing of Chinese characters}",
  author    = "Liang, Zhen and Fu, Qiang and Chi, Zheru",
  booktitle = "{Proceedings of the symposium on eye tracking research and
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "237–240",
  abstract  = "Eye typing is one of the most intensively investigated topics in
               eye tracking technology. Currently, almost all eye typing systems
               are developed for English typing. Some preliminary studies have
               been made on developing eye typing systems for inputting Chinese
               characters/text. In this paper, a novel eye typing system is
               proposed for inputting Chinese characters, where a software
               keyboard is specially designed based on a study of Chinese
               Pinyin. Experimental results show the efficiency and usability of
               the proposed system.",
  series    = "ETRA '12",
  month     =  "28~" # mar,
  year      =  2012,
  url       = "https://doi.org/10.1145/2168556.2168604",
  doi       = "10.1145/2168556.2168604",
  isbn      =  9781450312219
}

@INPROCEEDINGS{Kristensson2012-wk,
  title     = "{The potential of dwell-free eye-typing for fast assistive gaze
               communication}",
  author    = "Kristensson, Per Ola and Vertanen, Keith",
  booktitle = "{Proceedings of the symposium on eye tracking research and
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "241–244",
  abstract  = "We propose a new research direction for eye-typing which is
               potentially much faster: dwell-free eye-typing. Dwell-free
               eye-typing is in principle possible because we can exploit the
               high redundancy of natural languages to allow users to simply
               look at or near their desired letters without stopping to dwell
               on each letter. As a first step we created a system that
               simulated a perfect recognizer for dwell-free eye-typing. We used
               this system to investigate how fast users can potentially write
               using a dwell-free eye-typing interface. We found that after 40
               minutes of practice, users reached a mean entry rate of 46 wpm.
               This indicates that dwell-free eye-typing may be more than twice
               as fast as the current state-of-the-art methods for writing by
               gaze. A human performance model further demonstrates that it is
               highly unlikely traditional eye-typing systems will ever surpass
               our dwell-free eye-typing performance estimate.",
  series    = "ETRA '12",
  month     =  "28~" # mar,
  year      =  2012,
  url       = "https://doi.org/10.1145/2168556.2168605",
  file      = "All Papers/My Library/Kristensson and Vertanen 2012 - The potential of dwell-free eye-typing for fast assistive gaze communication.pdf",
  doi       = "10.1145/2168556.2168605",
  isbn      =  9781450312219
}

@INPROCEEDINGS{Olsen2012-po,
  title     = "{Identifying parameter values for an {I-VT} fixation filter
               suitable for handling data sampled with various sampling
               frequencies}",
  author    = "Olsen, Anneli and Matos, Ricardo",
  booktitle = "{Proceedings of the Symposium on Eye Tracking Research and
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "317--320",
  abstract  = "Selecting values for fixation filters is a difficult task as not
               only the specifics of the selected filter algorithm has to be
               taken into account, but also what it is going to be used for and
               by whom. In this paper the selection and testing process of
               values for an I-VT fixation filter algorithm implementation is
               described.",
  series    = "ETRA '12",
  month     =  mar,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2168556.2168625",
  keywords  = "eye movements, scoring, algorithm, classification",
  doi       = "10.1145/2168556.2168625",
  isbn      =  9781450312219
}

@INPROCEEDINGS{Olsen2012-my,
  title     = "{Identifying parameter values for an I-VT fixation filter
               suitable for handling data sampled with various sampling
               frequencies}",
  author    = "Olsen, Anneli and Matos, Ricardo",
  booktitle = "{Proceedings of the symposium on eye tracking research and
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "317–320",
  abstract  = "Selecting values for fixation filters is a difficult task as not
               only the specifics of the selected filter algorithm has to be
               taken into account, but also what it is going to be used for and
               by whom. In this paper the selection and testing process of
               values for an I-VT fixation filter algorithm implementation is
               described.",
  series    = "ETRA '12",
  month     =  mar,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2168556.2168625",
  doi       = "10.1145/2168556.2168625",
  isbn      =  9781450312219
}

@INPROCEEDINGS{Zhao2012-mr,
  title     = "{Typing with eye-gaze and tooth-clicks}",
  author    = "Zhao, Xiaoyu (amy) and Guestrin, Elias D and Sayenko, Dimitry and
               Simpson, Tyler and Gauthier, Michel and Popovic, Milos R",
  booktitle = "{Proceedings of the symposium on eye tracking research and
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "341–344",
  abstract  = "In eye-gaze-based human-computer interfaces, the most commonly
               used mechanism for generating activation commands (i.e., mouse
               clicks) is dwell time (DT). While DT can be relatively efficient
               and easy to use, it is also associated with the possibility of
               generating unintentional activation commands – an issue that is
               known as the Midas' touch problem. To address this problem, we
               proposed to use a “tooth-clicker” (TC) device as a mechanism for
               generating activation commands independently of the activity of
               the eyes.This paper describes a pilot study that verifies the
               feasibility of using an eye-gaze tracker (EGT) and a TC to type
               on an on-screen keyboard, and compares the performance of the
               EGT-TC system with that of the EGT with two different DT
               thresholds (880 ms and 490 ms). The six subjects that
               participated in the study were able to attain typing speeds using
               the EGT-TC system that were slower than but comparable to the
               typing speeds that they attained using the EGT with the shorter
               DT threshold.",
  series    = "ETRA '12",
  month     =  "28~" # mar,
  year      =  2012,
  url       = "https://doi.org/10.1145/2168556.2168632",
  doi       = "10.1145/2168556.2168632",
  isbn      =  9781450312219
}

@INPROCEEDINGS{Rasmussen2012-nf,
  title     = "{Shape-changing interfaces}",
  author    = "Rasmussen, Majken K and Pedersen, Esben W and Petersen, Marianne
               G and Hornbæk, Kasper",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # may,
  year      =  2012,
  url       = "https://doi.org/10.1145%2F2207676.2207781",
  doi       = "10.1145/2207676.2207781",
  isbn      =  9781450310154
}

@INPROCEEDINGS{Neustaedter2012-pv,
  title     = "{Intimacy in long-distance relationships over video chat}",
  author    = "Neustaedter, Carman and Greenberg, Saul",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "753--762",
  abstract  = "Many couples live a portion of their lives in a long-distance
               relationship (LDR). This includes a large number of dating
               college students as well as couples who are
               geographically-separated because of situational demands such as
               work. We conducted interviews with individuals in LDRs to
               understand how they make use of video chat systems to maintain
               their relationships. In particular, we have investigated how
               couples use video to ``hang out'' together and engage in
               activities over extended periods of time. Our results show that
               regardless of the relationship situation, video chat affords a
               unique opportunity for couples to share presence over distance,
               which in turn provides intimacy. While beneficial, couples still
               face challenges in using video chat, including contextual (e.g.,
               location of partners, time zones), technical (e.g., mobility,
               audio/video quality, networking), and personal (e.g., a lack of
               physicality needed by most for intimate sexual acts) challenges.",
  series    = "CHI '12",
  month     =  may,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2207676.2207785",
  file      = "All Papers/Other/Neustaedter and Greenberg 2012 - Intimacy in long-distance relationships over video chat.pdf",
  keywords  = "long-distance relationships, video chat, intimacy",
  doi       = "10.1145/2207676.2207785",
  isbn      =  9781450310154
}

@INPROCEEDINGS{Neustaedter2012-pq,
  title     = "{Intimacy in long-distance relationships over video chat}",
  author    = "Neustaedter, Carman and Greenberg, Saul",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "753–762",
  abstract  = "Many couples live a portion of their lives in a long-distance
               relationship (LDR). This includes a large number of dating
               college students as well as couples who are
               geographically-separated because of situational demands such as
               work. We conducted interviews with individuals in LDRs to
               understand how they make use of video chat systems to maintain
               their relationships. In particular, we have investigated how
               couples use video to “hang out” together and engage in activities
               over extended periods of time. Our results show that regardless
               of the relationship situation, video chat affords a unique
               opportunity for couples to share presence over distance, which in
               turn provides intimacy. While beneficial, couples still face
               challenges in using video chat, including contextual (e.g.,
               location of partners, time zones), technical (e.g., mobility,
               audio/video quality, networking), and personal (e.g., a lack of
               physicality needed by most for intimate sexual acts) challenges.",
  series    = "CHI '12",
  month     =  may,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2207676.2207785",
  file      = "All Papers/My Library/Neustaedter and Greenberg 2012 - Intimacy in long-distance relationships over video chat.pdf",
  doi       = "10.1145/2207676.2207785",
  isbn      =  9781450310154
}

@INPROCEEDINGS{Casiez2012-gq,
  title     = "{1 € filter: a simple speed-based low-pass filter for noisy input
               in interactive systems}",
  author    = "Casiez, Géry and Roussel, Nicolas and Vogel, Daniel",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2527--2530",
  abstract  = "The 1 € filter (``one Euro filter'') is a simple algorithm to
               filter noisy signals for high precision and responsiveness. It
               uses a first order low-pass filter with an adaptive cutoff
               frequency: at low speeds, a low cutoff stabilizes the signal by
               reducing jitter, but as speed increases, the cutoff is increased
               to reduce lag. The algorithm is easy to implement, uses very few
               resources, and with two easily understood parameters, it is easy
               to tune. In a comparison with other filters, the 1 € filter has
               less lag using a reference amount of jitter reduction.",
  series    = "CHI '12",
  month     =  may,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2207676.2208639",
  file      = "All Papers/Other/Casiez et al. 2012 - 1 € filter - a simple speed-based low-pass filter for noisy input in interactive systems.pdf",
  keywords  = "lag, signal, filtering, jitter, precision, responsiveness,
               noise;uist2022-gaze-design",
  doi       = "10.1145/2207676.2208639",
  isbn      =  9781450310154
}

@INPROCEEDINGS{Casiez2012-vu,
  title     = "{1 € filter: a simple speed-based low-pass filter for noisy input
               in interactive systems}",
  author    = "Casiez, Géry and Roussel, Nicolas and Vogel, Daniel",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2527–2530",
  abstract  = "The 1 € filter (“one Euro filter”) is a simple algorithm to
               filter noisy signals for high precision and responsiveness. It
               uses a first order low-pass filter with an adaptive cutoff
               frequency: at low speeds, a low cutoff stabilizes the signal by
               reducing jitter, but as speed increases, the cutoff is increased
               to reduce lag. The algorithm is easy to implement, uses very few
               resources, and with two easily understood parameters, it is easy
               to tune. In a comparison with other filters, the 1 € filter has
               less lag using a reference amount of jitter reduction.",
  series    = "CHI '12",
  month     =  "5~" # may,
  year      =  2012,
  url       = "https://doi.org/10.1145/2207676.2208639",
  file      = "All Papers/My Library/Casiez et al. 2012 - 1 € filter - a simple speed-based low-pass filter for noisy input in interactive systems.pdf",
  doi       = "10.1145/2207676.2208639",
  isbn      =  9781450310154
}

@INPROCEEDINGS{Otsuka2012-fc,
  title     = "{Reconstructing multiparty conversation field by augmenting human
               head motions via dynamic displays}",
  author    = "Otsuka, Kazuhiro and Kumano, Shiro and Mikami, Dan and Matsuda,
               Masafumi and Yamato, Junji",
  booktitle = "{{CHI} '12 Extended Abstracts on Human Factors in Computing
               Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "The hypothesis is that physical screen motion with image motion
               can boost the viewer's understanding of others' visual attention
               and suggest that viewers can more clearly discern the attention
               of meeting participants, and more accurately identify the
               addressees. A novel system is presented for reconstructing
               multiparty face-to-face conversation scenes in the real world
               through the use of dynamic displays that augment human head
               motion. This system aims to display and playback recorded
               conversations as if the remote people were talking in front of
               the viewer. It consists of multiple projectors and transparent
               screens attached to actuators. The screens displaying the
               life-size faces are spatially arranged to recreate the actual
               scene. Screen pose is dynamically synchronized to the actual head
               motions of the participants to emulate their head motions, which
               typically indicate shifts in visual attention. Our hypothesis is
               that physical screen motion with image motion can boost the
               viewer's understanding of others' visual attention. Experiments
               suggest that viewers can more clearly discern the attention of
               meeting participants, and more accurately identify the
               addressees.",
  month     =  may,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2212776.2223783",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1145/2212776.2223783",
  isbn      =  9781450310161,
  language  = "en"
}

@INPROCEEDINGS{Otsuka2012-nk,
  title     = "{Reconstructing multiparty conversation field by augmenting human
               head motions via dynamic displays}",
  author    = "Otsuka, Kazuhiro and Kumano, Shiro and Mikami, Dan and Matsuda,
               Masafumi and Yamato, Junji",
  booktitle = "{CHI '12 extended abstracts on human factors in computing
               systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "The hypothesis is that physical screen motion with image motion
               can boost the viewer's understanding of others' visual attention
               and suggest that viewers can more clearly discern the attention
               of meeting participants, and more accurately identify the
               addressees. A novel system is presented for reconstructing
               multiparty face-to-face conversation scenes in the real world
               through the use of dynamic displays that augment human head
               motion. This system aims to display and playback recorded
               conversations as if the remote people were talking in front of
               the viewer. It consists of multiple projectors and transparent
               screens attached to actuators. The screens displaying the
               life-size faces are spatially arranged to recreate the actual
               scene. Screen pose is dynamically synchronized to the actual head
               motions of the participants to emulate their head motions, which
               typically indicate shifts in visual attention. Our hypothesis is
               that physical screen motion with image motion can boost the
               viewer's understanding of others' visual attention. Experiments
               suggest that viewers can more clearly discern the attention of
               meeting participants, and more accurately identify the
               addressees.",
  month     =  may,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2212776.2223783",
  doi       = "10.1145/2212776.2223783",
  isbn      =  9781450310161,
  language  = "en"
}

@INPROCEEDINGS{Fares2012-px,
  title     = "{Magic-sense: dynamic cursor sensitivity-based magic pointing}",
  author    = "Fares, Ribel and Downing, Dustin and Komogortsev, Oleg",
  booktitle = "{CHI '12 extended abstracts on human factors in computing
               systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2489–2494",
  abstract  = "MAGIC (Manual and Gaze Input Cascaded) pointing methods use eye
               gaze as a complementary input for the primary input device. This
               paper introduces a novel MAGIC pointing technique to provide fast
               and accurate selection. Cursor sensitivity is reduced near eye
               focus to allow fine selection, and increased away from target to
               improve selection speed. MAGIC-SENSE is tested against a
               traditional mouse and a gaze only pointing method using an ISO
               9241-9 compliant circular Fitts' Law experiment. Using
               MAGIC-SENSE, subjects achieved lower error rates without
               compromising movement times compared to mouse-only method. A
               local calibration method that can boost all MAGIC pointing
               techniques is discussed.",
  series    = "CHI EA '12",
  month     =  "5~" # may,
  year      =  2012,
  url       = "https://doi.org/10.1145/2212776.2223824",
  file      = "All Papers/My Library/Fares et al. 2012 - Magic-sense - dynamic cursor sensitivity-based magic pointing.pdf",
  doi       = "10.1145/2212776.2223824",
  isbn      =  9781450310161
}

@ARTICLE{Kuster2012-vt,
  title     = "{Gaze correction for home video conferencing}",
  author    = "Kuster, Claudia and Popa, Tiberiu and Bazin, Jean-Charles and
               Gotsman, Craig and Gross, Markus",
  journal   = "ACM transactions on graphics",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  31,
  number    =  6,
  pages     = "1--6",
  abstract  = "Effective communication using current video conferencing systems
               is severely hindered by the lack of eye contact caused by the
               disparity between the locations of the subject and the camera.
               While this problem has been partially solved for high-end
               expensive video conferencing systems, it has not been
               convincingly solved for consumer-level setups. We present a gaze
               correction approach based on a single Kinect sensor that
               preserves both the integrity and expressiveness of the face as
               well as the fidelity of the scene as a whole, producing nearly
               artifact-free imagery. Our method is suitable for mainstream home
               video conferencing: it uses inexpensive consumer hardware,
               achieves real-time performance and requires just a simple and
               short setup. Our approach is based on the observation that for
               our application it is sufficient to synthesize only the corrected
               face. Thus we render a gaze-corrected 3D model of the scene and,
               with the aid of a face tracker, transfer the gaze-corrected
               facial portion in a seamless manner onto the original image.",
  month     =  nov,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2366145.2366193",
  keywords  = "depth camera, video conferencing, gaze
               correction;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/2366145.2366193",
  issn      = "0730-0301"
}

@ARTICLE{Kuster2012-xc,
  title    = "{Gaze correction for home video conferencing}",
  author   = "Kuster, Claudia and Popa, Tiberiu and Bazin, Jean-Charles and
              Gotsman, Craig and Gross, Markus",
  journal  = "ACM transactions on graphics",
  volume   =  31,
  number   =  6,
  pages    = "1–6",
  abstract = "Effective communication using current video conferencing systems
              is severely hindered by the lack of eye contact caused by the
              disparity between the locations of the subject and the camera.
              While this problem has been partially solved for high-end
              expensive video conferencing systems, it has not been convincingly
              solved for consumer-level setups. We present a gaze correction
              approach based on a single Kinect sensor that preserves both the
              integrity and expressiveness of the face as well as the fidelity
              of the scene as a whole, producing nearly artifact-free imagery.
              Our method is suitable for mainstream home video conferencing: it
              uses inexpensive consumer hardware, achieves real-time performance
              and requires just a simple and short setup. Our approach is based
              on the observation that for our application it is sufficient to
              synthesize only the corrected face. Thus we render a
              gaze-corrected 3D model of the scene and, with the aid of a face
              tracker, transfer the gaze-corrected facial portion in a seamless
              manner onto the original image.",
  month    =  nov,
  year     =  2012,
  url      = "http://dx.doi.org/10.1145/2366145.2366193",
  doi      = "10.1145/2366145.2366193",
  issn     = "0730-0301"
}

@INPROCEEDINGS{De_Beugher2012-uo,
  title     = "{Automatic analysis of eye-tracking data using object detection
               algorithms}",
  author    = "De Beugher, Stijn and Ichiche, Younes and Brône, Geert and
               Goedemé, Toon",
  booktitle = "{Proceedings of the 2012 ACM Conference on Ubiquitous Computing}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "677–680",
  abstract  = "In this paper we investigate the integration of object detection
               algorithms with eye-tracking data. The emerging technology of
               lightweight mobile eye-trackers enables realistic in-the-wild
               user experience experiments. Unfortunately, mobile eye-trackers
               generate a large amount of video data, which up to now requires
               manual analysis. This time-consuming and repetitive task renders
               processing large datasets economically infeasible. Our main
               contribution is the use of object detection algorithms to perform
               this analysis task automatically. We compare several object
               detection algorithms with regard to both speed and accuracy. To
               prove their functionality, we have recorded an eye-tracker
               shopping experiment and processed the data using object detection
               techniques.",
  series    = "UbiComp '12",
  year      =  2012,
  url       = "https://dl.acm.org/doi/10.1145/2370216.2370363",
  doi       = "10.1145/2370216.2370363",
  isbn      =  9781450312240
}

@INPROCEEDINGS{Kim2012-hk,
  title     = "{Digits}",
  author    = "Kim, David and Hilliges, Otmar and Izadi, Shahram and Butler,
               Alex D and Chen, Jiawen and Oikonomidis, Iason and Olivier,
               Patrick",
  booktitle = "{Proceedings of the 25th annual ACM symposium on User interface
               software and technology - UIST '12}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  year      =  2012,
  url       = "https://doi.org/10.1145%2F2380116.2380139",
  doi       = "10.1145/2380116.2380139",
  isbn      =  9781450315807
}

@INPROCEEDINGS{Al-Wabil2012-ry,
  title     = "{Optimizing gaze typing for people with severe motor
               disabilities: the iWriter arabic interface}",
  author    = "Al-Wabil, Areej and Al-Issa, Arwa and Hazzaa, Itisam and
               Al-Humaimeedi, May and Al-Tamimi, Lujain and Al-Kadhi, Bushra",
  booktitle = "{Proceedings of the 14th international ACM SIGACCESS conference
               on Computers and accessibility}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "261–262",
  abstract  = "Communication in the Arabic language with gaze using dwell time
               has been made possible by the development of eye typing
               interfaces. This paper describes the design process for
               developing iWriter, an Arabic gaze communication system. Design
               considerations for the optimization of the gaze typing interfaces
               for Arabic script are discussed.",
  series    = "ASSETS '12",
  month     =  "22~" # oct,
  year      =  2012,
  url       = "https://doi.org/10.1145/2384916.2384983",
  doi       = "10.1145/2384916.2384983",
  isbn      =  9781450313216
}

@INPROCEEDINGS{Rychlowska2012-ki,
  title     = "{From the eye to the heart}",
  author    = "Rychlowska, Magdalena and Zinner, Leah and Musca, Serban C and
               Niedenthal, Paula M",
  booktitle = "{Proceedings of the 4th Workshop on Eye Gaze in Intelligent Human
               Machine Interaction - {Gaze-In} '12}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  abstract  = "Smiles are complex facial expressions that carry multiple
               meanings. Recent literature suggests that deep processing of
               smiles via embodied simulation can be triggered by achieved eye
               contact. Three studies supported this prediction. In Study 1,
               participants rated the emotional impact of portraits, which
               varied in eye contact and smiling. Smiling portraits that
               achieved eye contact were more emotionally impactful than smiling
               portraits that did not achieve eye contact. In Study 2,
               participants saw photographs of smiles in which eye contact …",
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2401836.2401841",
  doi       = "10.1145/2401836.2401841",
  isbn      =  9781450315166
}

@INPROCEEDINGS{Rychlowska2012-jq,
  title     = "{From the eye to the heart}",
  author    = "Rychlowska, Magdalena and Zinner, Leah and Musca, Serban C and
               Niedenthal, Paula M",
  booktitle = "{Proceedings of the 4th workshop on eye gaze in intelligent human
               machine interaction - Gaze-In '12}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  abstract  = "Smiles are complex facial expressions that carry multiple
               meanings. Recent literature suggests that deep processing of
               smiles via embodied simulation can be triggered by achieved eye
               contact. Three studies supported this prediction. In Study 1,
               participants rated the emotional impact of portraits, which
               varied in eye contact and smiling. Smiling portraits that
               achieved eye contact were more emotionally impactful than smiling
               portraits that did not achieve eye contact. In Study 2,
               participants saw photographs of smiles in which eye contact …",
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2401836.2401841",
  doi       = "10.1145/2401836.2401841",
  isbn      =  9781450315166
}

@INPROCEEDINGS{Beznosyk2012-cl,
  title     = "{Role and quality of communication in collaborative training for
               Multiple Sclerosis patients}",
  author    = "Beznosyk, Anastasiia and Quax, Peter and Coninx, Karin and
               Lamotte, Wim",
  booktitle = "{Proceedings of the 5th international conference on PErvasive
               technologies related to assistive environments}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Social support provided by family and friends plays an important
               role in the rehabilitation of people suffering from various types
               of disabilities, in particular, Multiple Sclerosis (MS).
               Unfortunately due to the necessity of spending a substantial
               amount of time in rehabilitation centers, most MS patients
               experience lower social interaction with their immediate
               surroundings. To overcome this, we utilize a collaborative game
               played over the Internet that facilitates interaction during
               absence. While staying at the rehabilitation center for intense
               and necessary training programs, patients can play this game with
               relatives or friends communicating through audio and/or video
               channels.In the research presented in this paper, we firstly
               investigate how patients perceived communication with a remote
               person. Secondly, the influence of network quality (i.c. presence
               of packet loss) on this perception was analyzed. Results show
               that patients prefer to see their game partners while interacting
               remotely. When exposed to varying levels of multimedia quality,
               they point out that the quality of audio is relatively more
               important than the quality of video.",
  series    = "PETRA '12",
  month     =  "6~" # jun,
  year      =  2012,
  url       = "https://doi.org/10.1145/2413097.2413103",
  doi       = "10.1145/2413097.2413103",
  isbn      =  9781450313001
}

@INPROCEEDINGS{Takano2012-mm,
  title     = "{Do tablets really support discussion? comparison between paper,
               tablet, and laptop {PC} used as discussion tools}",
  author    = "Takano, Kentaro and Shibata, Hirohito and Omura, Kengo and
               Ichino, Junko and Hashiyama, Tomonori and Tano, Shun'ichi",
  booktitle = "{Proceedings of the 24th Australian {Computer-Human} Interaction
               Conference}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "562--571",
  abstract  = "Touch-based tablet devices are starting to be used frequently in
               meetings and places of discussion. However, are tablets really
               ideal as discussion tools? Or do they actually obstruct
               communication? To answer these questions, this paper
               quantitatively compares discussion processes involving paper, an
               iPad2, and a laptop PC. We performed an experiment where 12
               groups of two participants each (24 participants in total) worked
               collaboratively by referring to documents and using paper, an
               iPad2, or a laptop PC as presentation media. We observed verbal
               and non-verbal interaction between participants. First, we
               investigated the total amount of speech between two participants
               and found they spoke more when using paper than when using the
               electronic media. Next, we observed that participants used more
               demonstrative pronouns when using paper than when using the iPad2
               but used more demonstrative pronouns when using the iPad2 than
               when using the laptop PC. Also, they made more eye contact when
               using paper than when using the other media. These results
               suggest that tablets may not currently be the best media to use
               when ideas should be actively exchanged, sensitivity is required
               toward other participants in the discussion, or work progress
               needs to be shared.",
  series    = "OzCHI '12",
  month     =  nov,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2414536.2414623",
  keywords  = "paper, reading to support discussion, tablet device",
  doi       = "10.1145/2414536.2414623",
  isbn      =  9781450314381
}

@INPROCEEDINGS{Takano2012-jz,
  title     = "{Do tablets really support discussion? comparison between paper,
               tablet, and laptop PC used as discussion tools}",
  author    = "Takano, Kentaro and Shibata, Hirohito and Omura, Kengo and
               Ichino, Junko and Hashiyama, Tomonori and Tano, Shun'ichi",
  booktitle = "{Proceedings of the 24th australian Computer-Human interaction
               conference}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "562–571",
  abstract  = "Touch-based tablet devices are starting to be used frequently in
               meetings and places of discussion. However, are tablets really
               ideal as discussion tools? Or do they actually obstruct
               communication? To answer these questions, this paper
               quantitatively compares discussion processes involving paper, an
               iPad2, and a laptop PC. We performed an experiment where 12
               groups of two participants each (24 participants in total) worked
               collaboratively by referring to documents and using paper, an
               iPad2, or a laptop PC as presentation media. We observed verbal
               and non-verbal interaction between participants. First, we
               investigated the total amount of speech between two participants
               and found they spoke more when using paper than when using the
               electronic media. Next, we observed that participants used more
               demonstrative pronouns when using paper than when using the iPad2
               but used more demonstrative pronouns when using the iPad2 than
               when using the laptop PC. Also, they made more eye contact when
               using paper than when using the other media. These results
               suggest that tablets may not currently be the best media to use
               when ideas should be actively exchanged, sensitivity is required
               toward other participants in the discussion, or work progress
               needs to be shared.",
  series    = "OzCHI '12",
  month     =  nov,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2414536.2414623",
  doi       = "10.1145/2414536.2414623",
  isbn      =  9781450314381
}

@INPROCEEDINGS{Jones2013-ly,
  title     = "{IllumiRoom: peripheral projected illusions for interactive
               experiences}",
  author    = "Jones, Brett R and Benko, Hrvoje and Ofek, Eyal and Wilson,
               Andrew D",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "869–878",
  abstract  = "IllumiRoom is a proof-of-concept system that augments the area
               surrounding a television with projected visualizations to enhance
               traditional gaming experiences. We investigate how projected
               visualizations in the periphery can negate, include, or augment
               the existing physical environment and complement the content
               displayed on the television screen. Peripheral projected
               illusions can change the appearance of the room, induce apparent
               motion, extend the field of view, and enable entirely new
               physical gaming experiences. Our system is entirely
               self-calibrating and is designed to work in any room. We present
               a detailed exploration of the design space of peripheral
               projected illusions and we demonstrate ways to trigger and drive
               such illusions from gaming content. We also contribute specific
               feedback from two groups of target users (10 gamers and 15 game
               designers); providing insights for enhancing game experiences
               through peripheral projected illusions.",
  series    = "CHI '13",
  year      =  2013,
  url       = "https://doi.org/10.1145/2470654.2466112",
  doi       = "10.1145/2470654.2466112",
  isbn      =  9781450318990
}

@INPROCEEDINGS{Wagner2013-hs,
  title     = "{Body-centric design space for multi-surface interaction}",
  author    = "Wagner, Julie and Nancel, Mathieu and Gustafson, Sean G and Huot,
               Stephane and Mackay, Wendy E",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1299–1308",
  abstract  = "We introduce BodyScape, a body-centric design space that allows
               us to describe, classify and systematically compare multi-surface
               interaction techniques, both individually and in combination.
               BodyScape reflects the relationship between users and their
               environment, specifically how different body parts enhance or
               restrict movement within particular interaction techniques and
               can be used to analyze existing techniques or suggest new ones.
               We illustrate the use of BodyScape by comparing two free-hand
               techniques, on-body touch and mid-air pointing, first separately,
               then combined. We found that touching the torso is faster than
               touching the lower legs, since it affects the user's balance; and
               touching targets on the dominant arm is slower than targets on
               the torso because the user must compensate for the applied force.",
  series    = "CHI '13",
  month     =  "27~" # apr,
  year      =  2013,
  url       = "https://doi.org/10.1145/2470654.2466170",
  file      = "All Papers/My Library/Wagner et al. 2013 - Body-centric design space for multi-surface interaction.pdf",
  doi       = "10.1145/2470654.2466170",
  isbn      =  9781450318990
}

@INPROCEEDINGS{Wallace2013-gr,
  title     = "{Collaborative sensemaking on a digital tabletop and personal
               tablets}",
  author    = "Wallace, James R and Scott, Stacey D and MacGregor, Carolyn G",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "We describe an investigation of the support that three different
               display configurations provided for a collaborative sensemaking
               task: a digital table; personal tablets; and both the tabletop
               and personal tablets. Mixed-methods analyses revealed that the
               presence of a digital tabletop display led to improved
               sensemaking performance, and identified activities that were
               supported by the shared workspace. The digital tabletop supported
               a group's ability to prioritize information, to make comparisons
               between task data, and to form and critique the group's working
               hypothesis. Analyses of group performance revealed a positive
               correlation with equity of member participation using the shared
               digital table, and a negative correlation of equity of member
               participation using personal tablets. Implications for the
               support of sensemaking groups, and the use of equity of member
               participation as a predictive measure of their performance are
               discussed.",
  month     =  apr,
  year      =  2013,
  url       = "http://dx.doi.org/10.1145/2470654.2466458",
  doi       = "10.1145/2470654.2466458",
  isbn      =  9781450318990
}

@INPROCEEDINGS{Wallace2013-vc,
  title     = "{Collaborative sensemaking on a digital tabletop and personal
               tablets}",
  author    = "Wallace, James R and Scott, Stacey D and MacGregor, Carolyn G",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "We describe an investigation of the support that three different
               display configurations provided for a collaborative sensemaking
               task: a digital table; personal tablets; and both the tabletop
               and personal tablets. Mixed-methods analyses revealed that the
               presence of a digital tabletop display led to improved
               sensemaking performance, and identified activities that were
               supported by the shared workspace. The digital tabletop supported
               a group's ability to prioritize information, to make comparisons
               between task data, and to form and critique the group's working
               hypothesis. Analyses of group performance revealed a positive
               correlation with equity of member participation using the shared
               digital table, and a negative correlation of equity of member
               participation using personal tablets. Implications for the
               support of sensemaking groups, and the use of equity of member
               participation as a predictive measure of their performance are
               discussed.",
  month     =  apr,
  year      =  2013,
  url       = "http://dx.doi.org/10.1145/2470654.2466458",
  doi       = "10.1145/2470654.2466458",
  isbn      =  9781450318990
}

@INPROCEEDINGS{Stellmach2013-nf,
  title     = "{Still looking: investigating seamless gaze-supported selection,
               positioning, and manipulation of distant targets}",
  author    = "Stellmach, Sophie and Dachselt, Raimund",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "285–294",
  abstract  = "We investigate how to seamlessly bridge the gap between users and
               distant displays for basic interaction tasks, such as object
               selection and manipulation. For this, we take advantage of very
               fast and implicit, yet imprecise gaze- and head-directed input in
               combination with ubiquitous smartphones for additional manual
               touch control. We have carefully elaborated two novel and
               consistent sets of gaze-supported interaction techniques based on
               touch-enhanced gaze pointers and local magnification lenses.
               These conflict-free sets allow for fluently selecting and
               positioning distant targets. Both sets were evaluated in a user
               study with 16 participants. Overall, users were fastest with a
               touch-enhanced gaze pointer for selecting and positioning an
               object after some training. While the positive user feedback for
               both sets suggests that our proposed gaze- and head-directed
               interaction techniques are suitable for a convenient and fluent
               selection and manipulation of distant targets, further
               improvements are necessary for more precise cursor control.",
  series    = "CHI '13",
  year      =  2013,
  url       = "https://dl.acm.org/doi/10.1145/2470654.2470695",
  doi       = "10.1145/2470654.2470695",
  isbn      =  9781450318990
}

@INPROCEEDINGS{Biswas2013-iw,
  title     = "{A new interaction technique involving eye gaze tracker and
               scanning system}",
  author    = "Biswas, Pradipta and Langdon, Pat",
  booktitle = "{Proceedings of the 2013 Conference on Eye Tracking South Africa}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "67--70",
  abstract  = "This paper presents a new input interaction system for people
               with severe disabilities by combining eye gaze tracking and
               single switch scanning interaction techniques. The system is
               faster than only scanning based systems while more comfortable to
               use than existing eye gaze tracking based systems. We reported
               results from a couple of user studies that show the new system is
               equally fast compared to existing eye tracking systems that does
               not involve scanning, participants with no prior experience with
               eye tracking based system could learn using this new system
               successfully within 10 minutes but it demands higher mental
               effort in comparison to another new modality of interaction- a
               gesture based system.",
  series    = "ETSA '13",
  month     =  aug,
  year      =  2013,
  url       = "http://dx.doi.org/10.1145/2509315.2509322",
  keywords  = "single switch scanning, eye gaze tracker, assistive
               technology;prj-gaze-shorthand",
  doi       = "10.1145/2509315.2509322",
  isbn      =  9781450321105
}

@INPROCEEDINGS{Biswas2013-ax,
  title     = "{A new interaction technique involving eye gaze tracker and
               scanning system}",
  author    = "Biswas, Pradipta and Langdon, Pat",
  booktitle = "{Proceedings of the 2013 conference on eye tracking south africa}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "67–70",
  abstract  = "This paper presents a new input interaction system for people
               with severe disabilities by combining eye gaze tracking and
               single switch scanning interaction techniques. The system is
               faster than only scanning based systems while more comfortable to
               use than existing eye gaze tracking based systems. We reported
               results from a couple of user studies that show the new system is
               equally fast compared to existing eye tracking systems that does
               not involve scanning, participants with no prior experience with
               eye tracking based system could learn using this new system
               successfully within 10 minutes but it demands higher mental
               effort in comparison to another new modality of interaction- a
               gesture based system.",
  series    = "ETSA '13",
  month     =  "29~" # aug,
  year      =  2013,
  url       = "https://doi.org/10.1145/2509315.2509322",
  doi       = "10.1145/2509315.2509322",
  isbn      =  9781450321105
}

@INPROCEEDINGS{Zhang2014-av,
  title     = "{Non-intrusive tongue machine interface}",
  author    = "Zhang, Qiao and Gollakota, Shyamnath and Taskar, Ben and Rao, Raj
               P N",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2555–2558",
  abstract  = "There has been recent interest in designing systems that use the
               tongue as an input interface. Prior work however either require
               surgical procedures or in-mouth sensor placements. In this paper,
               we introduce TongueSee, a non-intrusive tongue machine interface
               that can recognize a rich set of tongue gestures using
               electromyography (EMG) signals from the surface of the skin. We
               demonstrate the feasibility and robustness of TongueSee with
               experimental studies to classify six tongue gestures across eight
               participants. TongueSee achieves a classification accuracy of
               94.17\% and a false positive probability of 0.000358 per second
               using three-protrusion preamble design.",
  series    = "CHI '14",
  month     =  "26~" # apr,
  year      =  2014,
  url       = "https://doi.org/10.1145/2556288.2556981",
  doi       = "10.1145/2556288.2556981",
  isbn      =  9781450324731
}

@INPROCEEDINGS{Schwarz2014-qo,
  title     = "{Combining body pose, gaze, and gesture to determine intention to
               interact in vision-based interfaces}",
  author    = "Schwarz, Julia and Marais, Charles Claudius and Leyvand, Tommer
               and Hudson, Scott E and Mankoff, Jennifer",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "3443–3452",
  abstract  = "Vision-based interfaces, such as those made popular by the
               Microsoft Kinect, suffer from the Midas Touch problem: every user
               motion can be interpreted as an interaction. In response, we
               developed an algorithm that combines facial features, body pose
               and motion to approximate a user's intention to interact with the
               system. We show how this can be used to determine when to pay
               attention to a user's actions and when to ignore them. To
               demonstrate the value of our approach, we present results from a
               30-person lab study conducted to compare four engagement
               algorithms in single and multi-user scenarios. We found that
               combining intention to interact with a 'raise an open hand in
               front of you' gesture yielded the best results. The latter
               approach offers a 12\% improvement in accuracy and a 20\%
               reduction in time to engage over a baseline 'wave to engage'
               gesture currently used on the Xbox 360.",
  series    = "CHI '14",
  year      =  2014,
  url       = "https://dl.acm.org/doi/10.1145/2556288.2556989",
  doi       = "10.1145/2556288.2556989",
  isbn      =  9781450324731
}

@INPROCEEDINGS{Pedersen2014-qv,
  title     = "{Is my phone alive?}",
  author    = "Pedersen, Esben W and Subramanian, Sriram and Hornbæk, Kasper",
  booktitle = "{Proceedings of the 32nd annual ACM conference on Human factors
               in computing systems - CHI '14}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  year      =  2014,
  url       = "https://doi.org/10.1145%2F2556288.2557018",
  doi       = "10.1145/2556288.2557018",
  isbn      =  9781450324731
}

@INPROCEEDINGS{Pan2014-ea,
  title     = "{Comparing flat and spherical displays in a trust scenario in
               avatar-mediated interaction}",
  author    = "Pan, Ye and Steptoe, William and Steed, Anthony",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  apr,
  year      =  2014,
  url       = "https://doi.org/10.1145%2F2556288.2557276",
  doi       = "10.1145/2556288.2557276"
}

@INPROCEEDINGS{Kurzhals2014-qt,
  title     = "{ISeeCube: visual analysis of gaze data for video}",
  author    = "Kurzhals, Kuno and Heimerl, Florian and Weiskopf, Daniel",
  booktitle = "{Proceedings of the Symposium on Eye Tracking Research and
               Applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "26~" # mar,
  year      =  2014,
  url       = "https://consensus.apphttps://consensus.app/papers/iseecube-analysis-gaze-data-video-kurzhals/d8fb369e83d65036a0c1e7e3b8e0d5eb/?extracted-answer=ISeeCube+provides+a+space-time+cube+visualization+for+eye+tracking+data+and+a+timeline+visualization+for+analyzing+dynamic+Areas+of+Interest+%28AOIs%29+in+videos.&q=the+dataset+of+gaze+point+and+video+QA&copilot=on&lang=en",
  doi       = "10.1145/2578153.2578158",
  isbn      =  9781450327510
}

@INPROCEEDINGS{Maurus2014-ae,
  title     = "{Realistic heatmap visualization for interactive analysis of 3D
               gaze data}",
  author    = "Maurus, Michael and Hammer, Jan Hendrik and Beyerer, Jürgen",
  booktitle = "{Proceedings of the symposium on eye tracking research and
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "295–298",
  abstract  = "In this paper, a novel approach for real-time heatmap generation
               and visualization of 3D gaze data is presented. By projecting the
               gaze into the scene and considering occlusions from the
               observer's view, to our knowledge, for the first time a correct
               visualization of the actual scene perception in 3D environments
               is provided. Based on a graphics-centric approach utilizing the
               graphics pipeline, shaders and several optimization techniques,
               heatmap rendering is fast enough for an interactive online and
               offline gaze analysis of thousands of gaze samples.",
  series    = "ETRA '14",
  month     =  "26~" # mar,
  year      =  2014,
  url       = "https://doi.org/10.1145/2578153.2578204",
  doi       = "10.1145/2578153.2578204",
  isbn      =  9781450327510
}

@INPROCEEDINGS{Kjeldskov2014-ji,
  title     = "{{EyeGaze}: enabling eye contact over video}",
  author    = "Kjeldskov, Jesper and Smedegård, Jacob H and Nielsen, Thomas S
               and Skov, Mikael B and Paay, Jeni",
  booktitle = "{Proceedings of the 2014 International Working Conference on
               Advanced Visual Interfaces}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "105--112",
  abstract  = "Traditional video communication systems offer a very limited
               experience of eye contact due to the offset between cameras and
               the screen. In response, we present EyeGaze, which uses multiple
               Kinect cameras to generate a 3D model of the user, and then
               renders a virtual camera angle giving the user an experience of
               eye contact. As a novel approach, we use concepts from
               KinectFusion, such as a volumetric voxel data representation and
               GPU accelerated ray tracing for viewpoint rendering. This
               achieves detail from a noisy source, and allows the real-time
               video output to be a composite of old and new data. We frame our
               work in literature on eye contact and previous approaches to
               supporting it over video. We then describe EyeGaze, and an
               empirical study comparing it with communication face-to-face or
               over traditional video. The study shows that while face-to-face
               is still superior, EyeGaze has added value over traditional video
               in terms of eye contact, involvement, turn-taking and
               co-presence.",
  series    = "AVI '14",
  month     =  may,
  year      =  2014,
  url       = "http://dx.doi.org/10.1145/2598153.2598165",
  file      = "All Papers/Other/Kjeldskov et al. 2014 - EyeGaze - enabling eye contact over video.pdf",
  keywords  = "eye contact, gaze, virtual camera, kinect;uist2022-gaze-design",
  doi       = "10.1145/2598153.2598165",
  isbn      =  9781450327756
}

@INPROCEEDINGS{Kjeldskov2014-gk,
  title     = "{EyeGaze: enabling eye contact over video}",
  author    = "Kjeldskov, Jesper and Smedegård, Jacob H and Nielsen, Thomas S
               and Skov, Mikael B and Paay, Jeni",
  booktitle = "{Proceedings of the 2014 international working conference on
               advanced visual interfaces}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "105–112",
  abstract  = "Traditional video communication systems offer a very limited
               experience of eye contact due to the offset between cameras and
               the screen. In response, we present EyeGaze, which uses multiple
               Kinect cameras to generate a 3D model of the user, and then
               renders a virtual camera angle giving the user an experience of
               eye contact. As a novel approach, we use concepts from
               KinectFusion, such as a volumetric voxel data representation and
               GPU accelerated ray tracing for viewpoint rendering. This
               achieves detail from a noisy source, and allows the real-time
               video output to be a composite of old and new data. We frame our
               work in literature on eye contact and previous approaches to
               supporting it over video. We then describe EyeGaze, and an
               empirical study comparing it with communication face-to-face or
               over traditional video. The study shows that while face-to-face
               is still superior, EyeGaze has added value over traditional video
               in terms of eye contact, involvement, turn-taking and
               co-presence.",
  series    = "AVI '14",
  month     =  may,
  year      =  2014,
  url       = "http://dx.doi.org/10.1145/2598153.2598165",
  file      = "All Papers/My Library/Kjeldskov et al. 2014 - EyeGaze - enabling eye contact over video.pdf",
  doi       = "10.1145/2598153.2598165",
  isbn      =  9781450327756
}

@INPROCEEDINGS{Ott1993-ih,
  title     = "{Teleconferencing eye contract using a virtual camera}",
  author    = "Ott, Maximilian and Lewis, John P and Cox, Ingemar",
  booktitle = "{INTERACT 93 and CHI 93 conference companion on Human factors in
               computing systems - CHI 93}",
  publisher = "ACM Press",
  year      =  1993,
  url       = "https://doi.org/10.1145%2F259964.260136",
  file      = "All Papers/My Library/Ott et al. 1993 - Teleconferencing eye contract using a virtual camera.pdf",
  doi       = "10.1145/259964.260136"
}

@INPROCEEDINGS{Lindlbauer2014-il,
  title     = "{Tracs: transparency-control for see-through displays}",
  author    = "Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema,
               Yuji and Höchtl, Anita and Haller, Michael and Inami, Masahiko
               and Müller, Jörg",
  booktitle = "{Proceedings of the 27th annual ACM symposium on User interface
               software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # oct,
  year      =  2014,
  url       = "https://dl.acm.org/doi/10.1145/2642918.2647350",
  file      = "All Papers/Other/Lindlbauer et al. 2014 - Tracs - transparency-control for see-through displays.pdf",
  doi       = "10.1145/2642918.2647350",
  isbn      =  9781450330695,
  language  = "en"
}

@INPROCEEDINGS{Hinckley2014-sf,
  title     = "{Sensing techniques for tabletstylus interaction}",
  author    = "Hinckley, Ken and Pahud, Michel and Benko, Hrvoje and Irani,
               Pourang and Guimbretière, François and Gavriliu, Marcel and Chen,
               Xiang and Matulic, Fabrice and Buxton, William and Wilson, Andrew",
  booktitle = "{Proceedings of the 27th annual ACM symposium on User interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2014,
  url       = "https://doi.org/10.1145%2F2642918.2647379",
  file      = "All Papers/My Library/Hinckley et al. 2014 - Sensing techniques for tabletstylus interaction.pdf",
  doi       = "10.1145/2642918.2647379"
}

@INPROCEEDINGS{Pfeuffer2014-kk,
  title     = "{Gaze-touch: combining gaze with multi-touch for interaction on
               the same surface}",
  author    = "Pfeuffer, Ken and Alexander, Jason and Chong, Ming Ki and
               Gellersen, Hans",
  booktitle = "{Proceedings of the 27th annual {ACM} symposium on User interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "509--518",
  abstract  = "Gaze has the potential to complement multi-touch for interaction
               on the same surface. We present gaze-touch, a technique that
               combines the two modalities based on the principle of 'gaze
               selects, touch manipulates'. Gaze is used to select a target, and
               coupled with multi-touch gestures that the user can perform
               anywhere on the surface. Gaze-touch enables users to manipulate
               any target from the same touch position, for whole-surface
               reachability and rapid context switching. Conversely, gaze-touch
               enables manipulation of the same target from any touch position
               on the surface, for example to avoid occlusion. Gaze-touch is
               designed to complement direct-touch as the default interaction on
               multi-touch surfaces. We provide a design space analysis of the
               properties of gaze-touch versus direct-touch, and present four
               applications that explore how gaze-touch can be used alongside
               direct-touch. The applications demonstrate use cases for
               interchangeable, complementary and alternative use of the two
               modes of interaction, and introduce novel techniques arising from
               the combination of gaze-touch and conventional multi-touch.",
  series    = "UIST '14",
  month     =  oct,
  year      =  2014,
  url       = "http://dx.doi.org/10.1145/2642918.2647397",
  keywords  = "interactive surface, multimodal ui, multi-touch, gaze
               input;prj-gaze-shorthand",
  doi       = "10.1145/2642918.2647397",
  isbn      =  9781450330695
}

@INPROCEEDINGS{Pfeuffer2014-di,
  title     = "{Gaze-touch: combining gaze with multi-touch for interaction on
               the same surface}",
  author    = "Pfeuffer, Ken and Alexander, Jason and Chong, Ming Ki and
               Gellersen, Hans",
  booktitle = "{Proceedings of the 27th annual ACM symposium on User interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "509–518",
  abstract  = "Gaze has the potential to complement multi-touch for interaction
               on the same surface. We present gaze-touch, a technique that
               combines the two modalities based on the principle of 'gaze
               selects, touch manipulates'. Gaze is used to select a target, and
               coupled with multi-touch gestures that the user can perform
               anywhere on the surface. Gaze-touch enables users to manipulate
               any target from the same touch position, for whole-surface
               reachability and rapid context switching. Conversely, gaze-touch
               enables manipulation of the same target from any touch position
               on the surface, for example to avoid occlusion. Gaze-touch is
               designed to complement direct-touch as the default interaction on
               multi-touch surfaces. We provide a design space analysis of the
               properties of gaze-touch versus direct-touch, and present four
               applications that explore how gaze-touch can be used alongside
               direct-touch. The applications demonstrate use cases for
               interchangeable, complementary and alternative use of the two
               modes of interaction, and introduce novel techniques arising from
               the combination of gaze-touch and conventional multi-touch.",
  series    = "UIST '14",
  month     =  "10~" # may,
  year      =  2014,
  url       = "https://doi.org/10.1145/2642918.2647397",
  file      = "All Papers/My Library/Pfeuffer et al. 2014 - Gaze-touch - combining gaze with multi-touch for interaction on the same surface.pdf",
  doi       = "10.1145/2642918.2647397",
  isbn      =  9781450330695
}

@INPROCEEDINGS{Rueben2015-db,
  title     = "{Context-aware assistive interfaces for persons with severe motor
               disabilities}",
  author    = "Rueben, Matthew",
  booktitle = "{Proceedings of the tenth annual ACM/IEEE international
               conference on human-robot interaction extended abstracts}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "217–218",
  abstract  = "Persons with severe motor disabilities have a great need for
               assistive robots, but also struggle to communicate these needs in
               ways that a robot can understand. I propose an interface that
               will make it possible to communicate with robots using limited
               movements. This will be done using contextual information from
               the robot's semantic model of the world. I also describe the
               state-of-the-art hardware and personal collaborations that equip
               our lab for this research. Assistive robotic interfaces also
               evoke concerns that a robot could violate personal privacy
               expectations, particularly if a remote operator can see the
               robot's video stream. This is especially important for persons
               with disabilities because it may be harder for them to monitor
               the robot's whereabouts. I describe ongoing work on two
               interfaces that help make it possible for robots to be privacy
               conscious. Answers for privacy concerns need to be developed
               alongside the new interface technologies prior to in-home
               deployment.",
  series    = "HRI'15 extended abstracts",
  month     =  "3~" # feb,
  year      =  2015,
  url       = "https://doi.org/10.1145/2701973.2702707",
  doi       = "10.1145/2701973.2702707",
  isbn      =  9781450333184
}

@INPROCEEDINGS{Lopes2015-dy,
  title     = "{Affordance}",
  author    = "Lopes, Pedro and Jonell, Patrik and Baudisch, Patrick",
  booktitle = "{Proceedings of the 33rd annual ACM conference on human factors
               in computing systems}",
  publisher = "ACM",
  month     =  apr,
  year      =  2015,
  url       = "https://doi.org/10.1145%2F2702123.2702128",
  doi       = "10.1145/2702123.2702128"
}

@INPROCEEDINGS{Turner2015-ml,
  title     = "{Gaze+RST: Integrating Gaze and Multitouch for Remote
               Rotate-Scale-Translate Tasks}",
  author    = "Turner, Jayson and Alexander, Jason and Bulling, Andreas and
               Gellersen, Hans",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "4179–4188",
  abstract  = "Our work investigates the use of gaze and multitouch to fluidly
               perform rotate-scale-translate (RST) tasks on large displays. The
               work specifically aims to understand if gaze can provide benefit
               in such a task, how task complexity affects performance, and how
               gaze and multitouch can be combined to create an integral input
               structure suited to the task of RST. We present four techniques
               that individually strike a different balance between gaze-based
               and touch-based translation while maintaining concurrent rotation
               and scaling operations. A 16 participant empirical evaluation
               revealed that three of our four techniques present viable options
               for this scenario, and that larger distances and rotation/scaling
               operations can significantly affect a gaze-based translation
               configuration. Furthermore we uncover new insights regarding
               multimodal integrality, finding that gaze and touch can be
               combined into configurations that pertain to integral or
               separable input structures.",
  series    = "CHI '15",
  month     =  "18~" # apr,
  year      =  2015,
  url       = "https://doi.org/10.1145/2702123.2702355",
  file      = "All Papers/My Library/Turner et al. 2015 - Gaze+RST - Integrating Gaze and Multitouch for Remote Rotate-Scale-Translate Tasks.pdf",
  doi       = "10.1145/2702123.2702355",
  isbn      =  9781450331456
}

@INPROCEEDINGS{Simeone2015-yv,
  title     = "{Substitutional Reality: Using the Physical Environment to Design
               Virtual Reality Experiences}",
  author    = "Simeone, Adalberto L and Velloso, Eduardo and Gellersen, Hans",
  booktitle = "{Proceedings of the 33rd Annual ACM Conference on Human Factors
               in Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "3307–3316",
  abstract  = "Experiencing Virtual Reality in domestic and other uncontrolled
               settings is challenging due to the presence of physical objects
               and furniture that are not usually defined in the Virtual
               Environment. To address this challenge, we explore the concept of
               Substitutional Reality in the context of Virtual Reality: a class
               of Virtual Environments where every physical object surrounding a
               user is paired, with some degree of discrepancy, to a virtual
               counterpart. We present a model of potential substitutions and
               validate it in two user studies. In the first study we
               investigated factors that affect participants' suspension of
               disbelief and ease of use. We systematically altered the virtual
               representation of a physical object and recorded responses from
               20 participants. The second study investigated users' levels of
               engagement as the physical proxy for a virtual object varied.
               From the results, we derive a set of guidelines for the design of
               future Substitutional Reality experiences.",
  series    = "CHI '15",
  year      =  2015,
  url       = "https://doi.org/10.1145/2702123.2702389",
  file      = "All Papers/My Library/Simeone et al. 2015 - Substitutional Reality - Using the Physical Environment to Design Virtual Reality Experiences.pdf",
  doi       = "10.1145/2702123.2702389",
  isbn      =  9781450331456
}

@INPROCEEDINGS{Pedrosa2015-yl,
  title     = "{Filteryedping: A dwell-free eye typing technique}",
  author    = "Pedrosa, Diogo and Pimentel, Maria da Graça and Truong, Khai N",
  booktitle = "{Proceedings of the 33rd annual ACM conference extended abstracts
               on human factors in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "303–306",
  abstract  = "The ability to type using eye gaze only is extremely important
               for individuals with a severe motor disability. To eye type, the
               user currently must sequentially gaze at letters in a virtual
               keyboard and dwell on each desired letter for a specific amount
               of time to input that key. Dwell-based eye typing has two
               possible drawbacks: unwanted input if the dwell threshold is too
               short or slow typing rates if the threshold is long. We
               demonstrate an eye typing technique, which does not require the
               user to dwell on the letters that she wants to input. Our method
               automatically filters out unwanted letters from the sequence of
               letters gazed at while typing a word. It ranks candidate words
               based on their length and frequency and presents them to the user
               for confirmation. Spell correction and support for typing words
               not in the corpus are also included.",
  series    = "CHI EA '15",
  month     =  "18~" # apr,
  year      =  2015,
  url       = "https://doi.org/10.1145/2702613.2725458",
  doi       = "10.1145/2702613.2725458",
  isbn      =  9781450331463
}

@INPROCEEDINGS{Akkil2015-ik,
  title     = "{Glance awareness and gaze interaction in smartwatches}",
  author    = "Akkil, Deepak and Kangas, Jari and Rantala, Jussi and Isokoski,
               Poika and Spakov, Oleg and Raisamo, Roope",
  booktitle = "{Proceedings of the 33rd annual ACM conference extended abstracts
               on human factors in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1271–1276",
  abstract  = "Smartwatches are widely available and increasingly adopted by
               consumers. The most common way of interacting with smartwatches
               is either touching a screen or pressing buttons on the sides.
               However, such techniques require using both hands. We propose
               glance awareness and active gaze interaction as alternative
               techniques to interact with smartwatches. We will describe an
               experiment conducted to understand the user preferences for
               visual and haptic feedback on a “glance” at the wristwatch.
               Following the glance, the users interacted with the watch using
               gaze gestures. Our results showed that user preferences differed
               depending on the complexity of the interaction. No clear
               preference emerged for complex interaction. For simple
               interaction, haptics was the preferred glance feedback modality.",
  series    = "CHI EA '15",
  month     =  "18~" # apr,
  year      =  2015,
  url       = "https://doi.org/10.1145/2702613.2732816",
  file      = "All Papers/My Library/Akkil et al. 2015 - Glance awareness and gaze interaction in smartwatches.pdf",
  doi       = "10.1145/2702613.2732816",
  isbn      =  9781450331463
}

@INPROCEEDINGS{Amores2015-xh,
  title     = "{ShowMe}",
  author    = "Amores, Judith and Benavides, Xavier and Maes, Pattie",
  booktitle = "{Proceedings of the 33rd annual ACM conference extended abstracts
               on human factors in computing systems}",
  publisher = "ACM",
  month     =  apr,
  year      =  2015,
  url       = "https://doi.org/10.1145%2F2702613.2732927",
  doi       = "10.1145/2702613.2732927"
}

@ARTICLE{Adams2015-cn,
  title     = "{Mindless Computing: Designing Technologies to Subtly Influence
               Behavior}",
  author    = "Adams, Alexander T and Costa, Jean and Jung, Malte F and
               Choudhury, Tanzeem",
  journal   = "Proc ACM Int Conf Ubiquitous Comput",
  publisher = "dl.acm.org",
  volume    =  2015,
  pages     = "719--730",
  abstract  = "Persuasive technologies aim to influence user's behaviors. In
               order to be effective, many of the persuasive technologies
               de-veloped so far relies on user's motivation and ability, which
               is highly variable and often the reason behind the failure of
               such technology. In this paper, we present the concept of
               Mindless Computing, which is a new approach to persuasive
               technology design. Mindless Computing leverages theories and
               concepts from psychology and behavioral economics into the design
               of technologies for behavior change. We show through a systematic
               review that most of the current persuasive technologies do not
               utilize the fast and automatic mental processes for behavioral
               change and there is an opportunity for persuasive technology
               designers to develop systems that are less reliant on user's
               motivation and ability. We describe two examples of mindless
               technologies and present pilot studies with encouraging results.
               Finally, we discuss design guidelines and considerations for
               developing this type of persuasive technology.",
  month     =  sep,
  year      =  2015,
  url       = "http://dx.doi.org/10.1145/2750858.2805843",
  file      = "All Papers/Other/Adams et al. 2015 - Mindless Computing - Designing Technologies to Subtly Influence Behavior.pdf",
  keywords  = "Behavior Change; H.5.m Information Interfaces; Mindless; Nudging;
               Persuasive Technology; Presentation: Misc; Subconscious;
               Subliminal; System 1; System 2",
  doi       = "10.1145/2750858.2805843",
  pmc       = "PMC6169779",
  pmid      =  30294729,
  language  = "en"
}

@ARTICLE{Adams2015-tk,
  title    = "{Mindless computing: Designing technologies to subtly influence
              behavior}",
  author   = "Adams, Alexander T and Costa, Jean and Jung, Malte F and
              Choudhury, Tanzeem",
  journal  = "Proc ACM Int Conf Ubiquitous Comput",
  volume   =  2015,
  pages    = "719–730",
  abstract = "Persuasive technologies aim to influence user's behaviors. In
              order to be effective, many of the persuasive technologies
              de-veloped so far relies on user's motivation and ability, which
              is highly variable and often the reason behind the failure of such
              technology. In this paper, we present the concept of Mindless
              Computing, which is a new approach to persuasive technology
              design. Mindless Computing leverages theories and concepts from
              psychology and behavioral economics into the design of
              technologies for behavior change. We show through a systematic
              review that most of the current persuasive technologies do not
              utilize the fast and automatic mental processes for behavioral
              change and there is an opportunity for persuasive technology
              designers to develop systems that are less reliant on user's
              motivation and ability. We describe two examples of mindless
              technologies and present pilot studies with encouraging results.
              Finally, we discuss design guidelines and considerations for
              developing this type of persuasive technology.",
  month    =  sep,
  year     =  2015,
  url      = "http://dx.doi.org/10.1145/2750858.2805843",
  file     = "All Papers/My Library/Adams et al. 2015 - Mindless computing - Designing technologies to subtly influence behavior.pdf",
  doi      = "10.1145/2750858.2805843",
  language = "en"
}

@ARTICLE{Shekhar2015-fa,
  title     = "{Spatial computing}",
  author    = "Shekhar, Shashi and Feiner, Steven K and Aref, Walid G",
  journal   = "Communications of the ACM",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  59,
  number    =  1,
  pages     = "72--81",
  abstract  = "Knowing where you are in space and time promises a deeper
               understanding of neighbors, ecosystems, and the environment.",
  month     =  dec,
  year      =  2015,
  url       = "http://dx.doi.org/10.1145/2756547",
  file      = "All Papers/Other/Shekhar et al. 2015 - Spatial computing.pdf",
  doi       = "10.1145/2756547",
  issn      = "0001-0782"
}

@ARTICLE{Shekhar2015-gg,
  title    = "{Spatial computing}",
  author   = "Shekhar, Shashi and Feiner, Steven K and Aref, Walid G",
  journal  = "Communications of the ACM",
  volume   =  59,
  number   =  1,
  pages    = "72–81",
  abstract = "Knowing where you are in space and time promises a deeper
              understanding of neighbors, ecosystems, and the environment.",
  month    =  dec,
  year     =  2015,
  url      = "http://dx.doi.org/10.1145/2756547",
  file     = "All Papers/My Library/Shekhar et al. 2015 - Spatial computing.pdf",
  doi      = "10.1145/2756547",
  issn     = "0001-0782"
}

@INPROCEEDINGS{Kurauchi2015-dl,
  title     = "{HMAGIC: head movement and gaze input cascaded pointing}",
  author    = "Kurauchi, Andrew and Feng, Wenxin and Morimoto, Carlos and Betke,
               Margrit",
  booktitle = "{Proceedings of the 8th ACM international conference on PErvasive
               technologies related to assistive environments}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–4",
  abstract  = "Augmentative and alternative communication tools allow people
               with severe motor disabilities to interact with computers. Two
               commonly used tools are video-based interfaces and eye trackers.
               Video-based interfaces map head movements captured by a camera to
               mouse pointer movements. Alternatively, eye trackers place the
               mouse pointer at the estimated position of the user's gaze. Eye
               tracking based interfaces have been shown to even outperform
               traditional mice in terms of speed, however the accuracy of
               current eye trackers is not enough for fine mouse pointer
               placement. In this paper we propose the Head Movement And Gaze
               Input Cascaded (HMAGIC) pointing technique that combines head
               movement and gaze-based inputs in a fast and accurate
               mouse-replacement interface. The interface initially places the
               pointer at the estimated gaze position and then the user makes
               fine adjustments with their head movements. We conducted a user
               experiment to compare HMAGIC with a mouse-replacement interface
               that uses only head movements to control the pointer.
               Experimental results indicate that HMAGIC is significantly faster
               than the head-only interface while still providing accurate mouse
               pointer positioning.",
  series    = "PETRA '15",
  month     =  "7~" # jan,
  year      =  2015,
  url       = "https://doi.org/10.1145/2769493.2769550",
  doi       = "10.1145/2769493.2769550",
  isbn      =  9781450334525
}

@INPROCEEDINGS{Nishida2015-yd,
  title     = "{CHILDHOOD}",
  author    = "Nishida, Jun and Takatori, Hikaru and Sato, Kosuke and Suzuki,
               Kenji",
  booktitle = "{ACM SIGGRAPH 2015 emerging technologies}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "31~" # jul,
  year      =  2015,
  url       = "https://doi.org/10.1145%2F2782782.2792501",
  doi       = "10.1145/2782782.2792501",
  isbn      =  9781450336352
}

@INPROCEEDINGS{Kevic2015-ll,
  title     = "{Tracing software developers' eyes and interactions for change
               tasks}",
  author    = "Kevic, Katja and Walters, Braden M and Shaffer, Timothy R and
               Sharif, Bonita and Shepherd, David C and Fritz, Thomas",
  booktitle = "{Proceedings of the 2015 10th Joint Meeting on Foundations of
               Software Engineering}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "202–213",
  abstract  = "What are software developers doing during a change task? While an
               answer to this question opens countless opportunities to support
               developers in their work, only little is known about developers'
               detailed navigation behavior for realistic change tasks. Most
               empirical studies on developers performing change tasks are
               limited to very small code snippets or are limited by the
               granularity or the detail of the data collected for the study. In
               our research, we try to overcome these limitations by combining
               user interaction monitoring with very fine granular eye-tracking
               data that is automatically linked to the underlying source code
               entities in the IDE. In a study with 12 professional and 10
               student developers working on three change tasks from an open
               source system, we used our approach to investigate the detailed
               navigation of developers for realistic change tasks. The results
               of our study show, amongst others, that the eye tracking data
               does indeed capture different aspects than user interaction data
               and that developers focus on only small parts of methods that are
               often related by data flow. We discuss our findings and their
               implications for better developer tool support.",
  series    = "ESEC/FSE 2015",
  year      =  2015,
  url       = "https://dl.acm.org/doi/10.1145/2786805.2786864",
  file      = "All Papers/My Library/Kevic et al. 2015 - Tracing software developers' eyes and interactions for change tasks.pdf",
  doi       = "10.1145/2786805.2786864",
  isbn      =  9781450336758
}

@INPROCEEDINGS{Ochiai2015-gx,
  title     = "{Fairy lights in femtoseconds: aerial and volumetric graphics
               rendered by focused femtosecond laser combined with computational
               holographic fields}",
  author    = "Ochiai, Yoichi and Kumagai, Kota and Hoshi, Takayuki and
               Rekimoto, Jun and Hasegawa, Satoshi and Hayasaki, Yoshio",
  booktitle = "{ACM SIGGRAPH 2015 Posters}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     =  1,
  abstract  = "We envision a laser-induced plasma technology in general
               applications for public use. If laser-induced plasma aerial
               images were made available, many useful applications such as
               spatial aerial AR, aerial user interfaces, volumetric images
               could be produced. This would be a highly effective display for
               the expression of three-dimensional information. Volumetric
               expression has considerable merit because the content scale
               corresponds to the human body; therefore, this technology could
               be usefully applied to wearable materials and spatial user
               interactions. Further, laser focusing technology can add an
               additional dimension to conventional projection technology, which
               is designed for surface mapping, while laser focusing technology
               is capable of volumetric mapping. This technology can be
               effectively used in real-world-oriented user interfaces.",
  series    = "SIGGRAPH '15",
  year      =  2015,
  url       = "https://doi.org/10.1145/2787626.2792630",
  doi       = "10.1145/2787626.2792630",
  isbn      =  9781450336321
}

@INPROCEEDINGS{Nakagaki2015-dz,
  title     = "{LineFORM}",
  author    = "Nakagaki, Ken and Follmer, Sean and Ishii, Hiroshi",
  booktitle = "{Proceedings of the 28th annual ACM symposium on user interface
               software \& technology}",
  publisher = "ACM",
  month     =  nov,
  year      =  2015,
  url       = "https://doi.org/10.1145%2F2807442.2807452",
  file      = "All Papers/My Library/Nakagaki et al. 2015 - LineFORM.pdf",
  doi       = "10.1145/2807442.2807452"
}

@INPROCEEDINGS{Lutteroth2015-dc,
  title     = "{Gaze vs. Mouse: A Fast and Accurate Gaze-Only Click Alternative}",
  author    = "Lutteroth, Christof and Penkar, Moiz and Weber, Gerald",
  booktitle = "{Proceedings of the 28th Annual ACM Symposium on User Interface
               Software \& Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "385–394",
  abstract  = "Eye gaze tracking is a promising input method which is gradually
               finding its way into the mainstream. An obvious question to arise
               is whether it can be used for point-and-click tasks, as an
               alternative for mouse or touch. Pointing with gaze is both fast
               and natural, although its accuracy is limited. There are still
               technical challenges with gaze tracking, as well as inherent
               physiological limitations. Furthermore, providing an alternative
               to clicking is challenging. We are considering use cases where
               input based purely on gaze is desired, and the click targets are
               discrete user interface (UI) elements which are too small to be
               reliably resolved by gaze alone, e.g., links in hypertext. We
               present Actigaze, a new gaze-only click alternative which is fast
               and accurate for this scenario. A clickable user interface
               element is selected by dwelling on one of a set of confirm
               buttons, based on two main design contributions: First, the
               confirm buttons stay on fixed positions with easily
               distinguishable visual identifiers such as colors, enabling
               procedural learning of the confirm button position. Secondly, UI
               elements are associated with confirm buttons through the visual
               identifiers in a way which minimizes the likelihood of
               inadvertent clicks. We evaluate two variants of the proposed
               click alternative, comparing them against the mouse and another
               gaze-only click alternative.",
  series    = "UIST '15",
  year      =  2015,
  url       = "https://dl.acm.org/doi/10.1145/2807442.2807461",
  doi       = "10.1145/2807442.2807461",
  isbn      =  9781450337793
}

@INPROCEEDINGS{Lander2015-vk,
  title     = "{GazeProjector: Accurate gaze estimation and seamless gaze
               interaction across multiple displays}",
  author    = "Lander, Christian and Gehring, Sven and Krüger, Antonio and
               Boring, Sebastian and Bulling, Andreas",
  booktitle = "{Proceedings of the 28th annual ACM symposium on user interface
               software \& technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "395–404",
  abstract  = "Mobile gaze-based interaction with multiple displays may occur
               from arbitrary positions and orientations. However, maintaining
               high gaze estimation accuracy in such situa-tions remains a
               significant challenge. In this paper, we present GazeProjector, a
               system that combines (1) natural feature tracking on displays to
               determine the mobile eye tracker's position relative to a display
               with (2) accurate point-of-gaze estimation. GazeProjector allows
               for seam-less gaze estimation and interaction on multiple
               displays of arbitrary sizes independently of the user's position
               and orientation to the display. In a user study with 12
               partici-pants we compare GazeProjector to established methods
               (here: visual on-screen markers and a state-of-the-art
               video-based motion capture system). We show that our approach is
               robust to varying head poses, orientations, and distances to the
               display, while still providing high gaze estimation accuracy
               across multiple displays without re-calibration for each
               variation. Our system represents an important step towards the
               vision of pervasive gaze-based interfaces.",
  series    = "UIST '15",
  month     =  "11~" # may,
  year      =  2015,
  url       = "https://doi.org/10.1145/2807442.2807479",
  file      = "All Papers/My Library/Lander et al. 2015 - GazeProjector - Accurate gaze estimation and seamless gaze interaction across multiple displays.pdf",
  doi       = "10.1145/2807442.2807479",
  isbn      =  9781450337793
}

@INPROCEEDINGS{Esteves2015-qn,
  title     = "{Orbits: Gaze interaction for smart watches using smooth pursuit
               eye movements}",
  author    = "Esteves, Augusto and Velloso, Eduardo and Bulling, Andreas and
               Gellersen, Hans",
  booktitle = "{Proceedings of the 28th annual ACM symposium on user interface
               software \& technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "457–466",
  abstract  = "We introduce Orbits, a novel gaze interaction technique that
               enables hands-free input on smart watches. The technique relies
               on moving controls to leverage the smooth pursuit movements of
               the eyes and detect whether and at which control the user is
               looking at. In Orbits, controls include targets that move in a
               circular trajectory in the face of the watch, and can be selected
               by following the desired one for a small amount of time. We
               conducted two user studies to assess the technique's recognition
               and robustness, which demonstrated how Orbits is robust against
               false positives triggered by natural eye movements and how it
               presents a hands-free, high accuracy way of interacting with
               smart watches using off-the-shelf devices. Finally, we developed
               three example interfaces built with Orbits: a music player, a
               notifications face plate and a missed call menu. Despite relying
               on moving controls – very unusual in current HCI interfaces –
               these were generally well received by participants in a third and
               final study.",
  series    = "UIST '15",
  month     =  "11~" # may,
  year      =  2015,
  url       = "https://doi.org/10.1145/2807442.2807499",
  doi       = "10.1145/2807442.2807499",
  isbn      =  9781450337793
}

@INPROCEEDINGS{Liu2015-ui,
  title     = "{GazeTry: Swipe text typing using gaze}",
  author    = "Liu, Yi and Zhang, Chi and Lee, Chonho and Lee, Bu-Sung and Chen,
               Alex Qiang",
  booktitle = "{Proceedings of the annual meeting of the australian special
               interest group for computer human interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "192–196",
  abstract  = "Over the last decades, eye gaze has become an alternative form of
               text entry by some physically challenged people. Recently a
               dwell-free system has been proposed, which has been proven to be
               much faster compared to other existing dwell-free systems.
               However, it is vulnerable to some common text entry problems. In
               the paper, we propose GazeTry, a dwell-free gaze-based text entry
               system which allows people to type a word by gazing sequentially
               at the letters of the word. Simulation and experiments results
               show that our proposed new dwell-free system, GazeTry with the
               Moving Window String Matching (MoWing) algorithm has better
               accuracy and more resilience to text entry errors.",
  series    = "OzCHI '15",
  month     =  "12~" # jul,
  year      =  2015,
  url       = "https://doi.org/10.1145/2838739.2838804",
  doi       = "10.1145/2838739.2838804",
  isbn      =  9781450336734
}

@INPROCEEDINGS{Avrahami2016-yd,
  title     = "{Supporting multitasking in video conferencing using gaze
               tracking and on-screen activity detection}",
  author    = "Avrahami, Daniel and van Everdingen, Eveline and Marlow, Jennifer",
  booktitle = "{Proceedings of the 21st International Conference on Intelligent
               User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "The use of videoconferencing in the workplace has been steadily
               growing. While multitasking during video conferencing is often
               necessary, it is also viewed as impolite and sometimes
               unacceptable. One potential contributor to negative attitudes
               towards such multitasking is the disrupted sense of eye contact
               that occurs when an individual shifts their gaze away to another
               screen, for example, in a dual-monitor setup, common in office
               settings. We present an approach to improve a sense of eye
               contact over videoconferencing in dual-monitor setups. Our
               approach uses computer vision and desktop activity detection to
               dynamically choose the camera with the best view of a user's
               face. We describe two alternative implementations of our solution
               (RGB-only, and a combination of RGB and RGB-D cameras). We then
               describe results from an online experiment that shows the
               potential of our approach to significantly improve perceptions of
               a person's politeness and engagement in the meeting.",
  month     =  "7~" # mar,
  year      =  2016,
  url       = "https://dl.acm.org/doi/10.1145/2856767.2856801",
  file      = "All Papers/Other/Avrahami et al. 2016 - Supporting multitasking in video conferencing using gaze tracking and on-screen activity detection.pdf",
  doi       = "10.1145/2856767.2856801",
  isbn      =  9781450341370
}

@INPROCEEDINGS{Ward2016-ta,
  title     = "{On the possibility of predicting gaze aversion to improve
               video-chat efficiency}",
  author    = "Ward, Nigel G and Jurado, Chelsey N and Garcia, Ricardo A and
               Ramos, Florencia A",
  booktitle = "{Proceedings of the ninth biennial ACM symposium on eye tracking
               research \& applications}",
  publisher = "ACM",
  month     =  mar,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F2857491.2857497",
  doi       = "10.1145/2857491.2857497"
}

@INPROCEEDINGS{Iwatsuki2016-oz,
  title     = "{Skilled gaze behavior extraction based on dependency analysis of
               gaze patterns on video scenes}",
  author    = "Iwatsuki, Atsushi and Hirayama, Takatsugu and Morita, Junya and
               Mase, Kenji",
  booktitle = "{Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking
               Research \& Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "299–302",
  abstract  = "The eye gaze behavior of individuals changes depending on their
               knowledge and experience of the event occurring in their field of
               view. In past studies, researchers formulated a hypothesis
               concerning this dependency on a specific scene and then analyzed
               the gaze behavior of viewers observing the scene. We depart from
               this hypothesis-testing paradigm. In this paper, we propose a
               data-mining framework for extracting skilled gaze behaviors of
               experts while watching a video based on a comprehensive
               comparison of viewers in terms of the dependency of their gaze
               patterns on video scenes. To quantitatively analyze the changes
               in the gaze behavior of experts according to the events in the
               scene, video and eye movement sequences are classified into video
               scenes and gaze patterns, respectively, by using an unsupervised
               clustering method focusing on short-time dynamics. Then, we
               analyze the dependency based on the distinctiveness and
               occurrence frequency of gaze patterns for each video scene.",
  series    = "ETRA '16",
  year      =  2016,
  url       = "https://doi.org/10.1145/2857491.2857531",
  doi       = "10.1145/2857491.2857531",
  isbn      =  9781450341257
}

@INPROCEEDINGS{Pfeiffer2016-uc,
  title     = "{EyeSee3D 2.0}",
  author    = "Pfeiffer, Thies and Renner, Patrick and Pfeiffer-Leßmann, Nadine",
  booktitle = "{Proceedings of the ninth biennial ACM symposium on eye tracking
               research \& applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "189–196",
  abstract  = "With the launch of ultra-portable systems, mobile eye tracking
               finally has the potential to become mainstream. While eye
               movements on their own can already be used to identify human
               activities, such as reading or walking, linking eye movements to
               objects in the environment provides even deeper insights into
               human cognitive processing.We present a model-based approach for
               the identification of fixated objects in three-dimensional
               environments. For evaluation, we compare the automatic labelling
               of fixations with those performed by human annotators. In
               addition to that, we show how the approach can be extended to
               support moving targets, such as individual limbs or faces of
               human interaction partners. The approach also scales to studies
               using multiple mobile eye-tracking systems in parallel.The
               developed system supports real-time attentive systems that make
               use of eye tracking as means for indirect or direct
               human-computer interaction as well as off-line analysis for basic
               research purposes and usability studies.",
  series    = "ETRA '16",
  month     =  "14~" # mar,
  year      =  2016,
  url       = "https://doi.org/10.1145/2857491.2857532",
  doi       = "10.1145/2857491.2857532",
  isbn      =  9781450341257
}

@INPROCEEDINGS{Pieszala2016-zd,
  title     = "{3D gaze point localization and visualization using LiDAR-based
               3D reconstructions}",
  author    = "Pieszala, James and Diaz, Gabriel and Pelz, Jeff and Speir,
               Jacqueline and Bailey, Reynold",
  booktitle = "{Proceedings of the ninth biennial ACM symposium on eye tracking
               research \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "201–204",
  abstract  = "We present a novel pipeline for localizing a free roaming eye
               tracker within a LiDAR-based 3D reconstructed scene with high
               levels of accuracy. By utilizing a combination of reconstruction
               algorithms that leverage the strengths of global versus local
               capture methods and user-assisted refinement, we reduce drift
               errors associated with Dense-SLAM techniques. Our framework
               supports region-of-interest (ROI) annotation and gaze statistics
               generation and the ability to visualize gaze in 3D from an
               immersive first person or third person perspective. This approach
               gives unique insights into viewers' problem solving and search
               task strategies and has high applicability in complex static
               environments such as crime scenes.",
  series    = "ETRA '16",
  month     =  "14~" # mar,
  year      =  2016,
  url       = "https://doi.org/10.1145/2857491.2857545",
  file      = "All Papers/My Library/Pieszala et al. 2016 - 3D gaze point localization and visualization using LiDAR-based 3D reconstructions.pdf",
  doi       = "10.1145/2857491.2857545",
  isbn      =  9781450341257
}

@INPROCEEDINGS{Robinson2016-vb,
  title     = "{Emergeables}",
  author    = "Robinson, Simon and Coutrix, Céline and Pearson, Jennifer and
               Rosso, Juan and Torquato, Matheus Fernandes and Nigay, Laurence
               and Jones, Matt",
  booktitle = "{Proceedings of the 2016 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  may,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F2858036.2858097",
  file      = "All Papers/My Library/Robinson et al. 2016 - Emergeables.pdf",
  doi       = "10.1145/2858036.2858097"
}

@INPROCEEDINGS{Akkil2016-de,
  title     = "{Gaze Augmentation in Egocentric Video Improves Awareness of
               Intention}",
  author    = "Akkil, Deepak and Isokoski, Poika",
  booktitle = "{Proceedings of the 2016 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1573--1584",
  abstract  = "Video communication using head-mounted cameras could be useful to
               mediate shared activities and support collaboration. Growing
               popularity of wearable gaze trackers presents an opportunity to
               add gaze information on the egocentric video. We hypothesized
               three potential benefits of gaze-augmented egocentric video to
               support collaborative scenarios: support deictic referencing,
               enable grounding in communication, and enable better awareness of
               the collaborator's intentions. Previous research on using
               egocentric videos for real-world collaborative tasks has failed
               to show clear benefits of gaze point visualization. We designed a
               study, deconstructing a collaborative car navigation scenario, to
               specifically target the value of gaze-augmented video for
               intention prediction. Our results show that viewers of
               gaze-augmented video could predict the direction taken by a
               driver at a four-way intersection more accurately and more
               confidently than a viewer of the same video without the
               superimposed gaze point. Our study demonstrates that gaze
               augmentation can be useful and encourages further study in
               real-world collaborative scenarios.",
  series    = "CHI '16",
  month     =  may,
  year      =  2016,
  url       = "http://dx.doi.org/10.1145/2858036.2858127",
  keywords  = "gaze tracking, video-based collaboration, wearable computing,
               intention prediction",
  doi       = "10.1145/2858036.2858127",
  isbn      =  9781450333627
}

@INPROCEEDINGS{Akkil2016-yf,
  title     = "{Gaze augmentation in egocentric video improves awareness of
               intention}",
  author    = "Akkil, Deepak and Isokoski, Poika",
  booktitle = "{Proceedings of the 2016 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1573–1584",
  abstract  = "Video communication using head-mounted cameras could be useful to
               mediate shared activities and support collaboration. Growing
               popularity of wearable gaze trackers presents an opportunity to
               add gaze information on the egocentric video. We hypothesized
               three potential benefits of gaze-augmented egocentric video to
               support collaborative scenarios: support deictic referencing,
               enable grounding in communication, and enable better awareness of
               the collaborator's intentions. Previous research on using
               egocentric videos for real-world collaborative tasks has failed
               to show clear benefits of gaze point visualization. We designed a
               study, deconstructing a collaborative car navigation scenario, to
               specifically target the value of gaze-augmented video for
               intention prediction. Our results show that viewers of
               gaze-augmented video could predict the direction taken by a
               driver at a four-way intersection more accurately and more
               confidently than a viewer of the same video without the
               superimposed gaze point. Our study demonstrates that gaze
               augmentation can be useful and encourages further study in
               real-world collaborative scenarios.",
  series    = "CHI '16",
  month     =  may,
  year      =  2016,
  url       = "http://dx.doi.org/10.1145/2858036.2858127",
  doi       = "10.1145/2858036.2858127",
  isbn      =  9781450333627
}

@INPROCEEDINGS{Bailly2016-ak,
  title     = "{LivingDesktop}",
  author    = "Bailly, Gilles and Sahdev, Sidharth and Malacria, Sylvain and
               Pietrzak, Thomas",
  booktitle = "{Proceedings of the 2016 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  may,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F2858036.2858208",
  file      = "All Papers/My Library/Bailly et al. 2016 - LivingDesktop.pdf",
  doi       = "10.1145/2858036.2858208"
}

@INPROCEEDINGS{Feit2016-tn,
  title     = "{How we type: Movement strategies and performance in everyday
               typing}",
  author    = "Feit, Anna Maria and Weir, Daryl and Oulasvirta, Antti",
  booktitle = "{Proceedings of the 2016 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "4262–4273",
  abstract  = "This paper revisits the present understanding of typing, which
               originates mostly from studies of trained typists using the
               ten-finger touch typing system. Our goal is to characterise the
               majority of present-day users who are untrained and employ
               diverse, self-taught techniques. In a transcription task, we
               compare self-taught typists and those that took a touch typing
               course. We report several differences in performance, gaze
               deployment and movement strategies. The most surprising finding
               is that self-taught typists can achieve performance levels
               comparable with touch typists, even when using fewer fingers.
               Motion capture data exposes 3 predictors of high performance: 1)
               unambiguous mapping (a letter is consistently pressed by the same
               finger), 2) active preparation of upcoming keystrokes, and 3)
               minimal global hand motion. We release an extensive dataset on
               everyday typing behavior.",
  series    = "CHI '16",
  month     =  "5~" # jul,
  year      =  2016,
  url       = "https://doi.org/10.1145/2858036.2858233",
  file      = "All Papers/My Library/Feit et al. 2016 - How we type - Movement strategies and performance in everyday typing.pdf",
  doi       = "10.1145/2858036.2858233",
  isbn      =  9781450333627
}

@INPROCEEDINGS{Seyed2016-xc,
  title     = "{Doppio}",
  author    = "Seyed, Teddy and Yang, Xing-Dong and Vogel, Daniel",
  booktitle = "{Proceedings of the 2016 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # jul,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F2858036.2858256",
  doi       = "10.1145/2858036.2858256",
  isbn      =  9781450333627
}

@INPROCEEDINGS{Kurauchi2016-hg,
  title     = "{{EyeSwipe}: Dwell-free Text Entry Using Gaze Paths}",
  author    = "Kurauchi, Andrew and Feng, Wenxin and Joshi, Ajjen and Morimoto,
               Carlos and Betke, Margrit",
  booktitle = "{Proceedings of the 2016 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1952--1956",
  abstract  = "Text entry using gaze-based interaction is a vital communication
               tool for people with motor impairments. Most solutions require
               the user to fixate on a key for a given dwell time to select it,
               thus limiting the typing speed. In this paper we introduce
               EyeSwipe, a dwell-time-free gaze-typing method. With EyeSwipe,
               the user gaze-types the first and last characters of a word using
               the novel selection mechanism ``reverse crossing.'' To gaze-type
               the characters in the middle of the word, the user only needs to
               glance at the vicinity of the respective keys. We compared the
               performance of EyeSwipe with that of a dwell-time-based virtual
               keyboard. EyeSwipe afforded statistically significantly higher
               typing rates and more comfortable interaction in experiments with
               ten participants who reached 11.7 words per minute (wpm) after 30
               min typing with EyeSwipe.",
  series    = "CHI '16",
  month     =  may,
  year      =  2016,
  url       = "http://dx.doi.org/10.1145/2858036.2858335",
  file      = "All Papers/Other/Kurauchi et al. 2016 - EyeSwipe - Dwell-free Text Entry Using Gaze Paths.pdf",
  keywords  = "target selection, text entry, eye typing, eye tracking,
               dwell-free typing",
  doi       = "10.1145/2858036.2858335",
  isbn      =  9781450333627
}

@INPROCEEDINGS{Kurauchi2016-sp,
  title     = "{EyeSwipe: Dwell-free text entry using gaze paths}",
  author    = "Kurauchi, Andrew and Feng, Wenxin and Joshi, Ajjen and Morimoto,
               Carlos and Betke, Margrit",
  booktitle = "{Proceedings of the 2016 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1952–1956",
  abstract  = "Text entry using gaze-based interaction is a vital communication
               tool for people with motor impairments. Most solutions require
               the user to fixate on a key for a given dwell time to select it,
               thus limiting the typing speed. In this paper we introduce
               EyeSwipe, a dwell-time-free gaze-typing method. With EyeSwipe,
               the user gaze-types the first and last characters of a word using
               the novel selection mechanism “reverse crossing.” To gaze-type
               the characters in the middle of the word, the user only needs to
               glance at the vicinity of the respective keys. We compared the
               performance of EyeSwipe with that of a dwell-time-based virtual
               keyboard. EyeSwipe afforded statistically significantly higher
               typing rates and more comfortable interaction in experiments with
               ten participants who reached 11.7 words per minute (wpm) after 30
               min typing with EyeSwipe.",
  series    = "CHI '16",
  month     =  may,
  year      =  2016,
  url       = "http://dx.doi.org/10.1145/2858036.2858335",
  file      = "All Papers/My Library/Kurauchi et al. 2016 - EyeSwipe - Dwell-free text entry using gaze paths.pdf",
  doi       = "10.1145/2858036.2858335",
  isbn      =  9781450333627
}

@INPROCEEDINGS{Kangas2016-yz,
  title     = "{Feedback for smooth pursuit gaze tracking based control}",
  author    = "Kangas, Jari and Špakov, Oleg and Isokoski, Poika and Akkil,
               Deepak and Rantala, Jussi and Raisamo, Roope",
  booktitle = "{Proceedings of the 7th augmented human international conference
               2016}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Smart glasses, like Google Glass or Microsoft HoloLens, can be
               used as interfaces that expand human perceptual, cognitive, and
               actuation capabilities in many everyday situations. Conventional
               manual interaction techniques, however, are not convenient with
               smart glasses whereas eye trackers can be built into the frames.
               This makes gaze tracking a natural input technology for smart
               glasses. Not much is known about interaction techniques for
               gaze-aware smart glasses. This paper adds to this knowledge, by
               comparing feedback modalities (visual, auditory, haptic, none) in
               a continuous adjustment technique for smooth pursuit gaze
               tracking. Smooth pursuit based gaze tracking has been shown to
               enable flexible and calibration free method for spontaneous
               interaction situations. Continuous adjustment, on the other hand,
               is a technique that is needed in many everyday situations such as
               adjusting the volume of a sound system or the intensity of a
               light source. We measured user performance and preference in a
               task where participants matched the shades of two gray
               rectangles. The results showed no statistically significant
               differences in performance, but clear user preference and
               acceptability for haptic and audio feedback.",
  series    = "AH '16",
  month     =  "25~" # feb,
  year      =  2016,
  url       = "https://doi.org/10.1145/2875194.2875209",
  doi       = "10.1145/2875194.2875209",
  isbn      =  9781450336802
}

@INPROCEEDINGS{Shimonishi2016-mn,
  title     = "{Tracing temporal changes of selection criteria from gaze
               information}",
  author    = "Shimonishi, Kei and Kawashima, Hiroaki and Schaffer, Erina and
               Matsuyama, Takashi",
  booktitle = "{Companion Publication of the 21st International Conference on
               Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "To design interactive systems that proactively assist users'
               decision making, the users' gaze information is an important cue
               for the system to estimate users' selection criteria. Users
               sometimes change selection criteria while browsing content.
               Therefore, temporal changes of those criteria need to be traced
               from gaze data in short time scales. In this paper, we propose an
               approach to detecting users' distinctive browsing periods with
               its appropriate time-scale by leveraging multiscale exact tests
               so that the system can trace temporal changes of selection
               criteria. We demonstrate the applicability of the proposed method
               through a toy example and experiments.",
  month     =  "7~" # mar,
  year      =  2016,
  url       = "https://dl.acm.org/doi/10.1145/2876456.2879475",
  file      = "All Papers/Other/Shimonishi et al. 2016 - Tracing temporal changes of selection criteria from gaze information.pdf",
  doi       = "10.1145/2876456.2879475",
  isbn      =  9781450341400
}

@ARTICLE{Xu2016-hd,
  title     = "{See You See Me: The Role of Eye Contact in Multimodal
               Human-Robot Interaction}",
  author    = "Xu, Tian (linger) and Zhang, Hui and Yu, Chen",
  journal   = "ACM Transactions on Interactive Intelligent Systems",
  publisher = "dl.acm.org",
  volume    =  6,
  number    =  1,
  pages     = "1--22",
  month     =  "5~" # may,
  year      =  2016,
  url       = "https://dl.acm.org/doi/abs/10.1145/2882970?casa_token=sptWTHdkxMgAAAAA:tEeyfP_5rqAIDcOWsX9lCFMA4e1FOfp8MX0aaUi-tdI2QoH_y_3o9Wu02A6Jn4LKfYxx5pNgRp2BcFw",
  file      = "All Papers/Other/Xu et al. 2016 - See You See Me - The Role of Eye Contact in Multimodal Human-Robot Interaction.pdf",
  doi       = "10.1145/2882970",
  issn      = "2160-6455,2160-6463"
}

@ARTICLE{Lien2016-zy,
  title    = "{Soli}",
  author   = "Lien, Jaime and Gillian, Nicholas and Karagozler, M Emre and
              Amihood, Patrick and Schwesig, Carsten and Olson, Erik and Raja,
              Hakim and Poupyrev, Ivan",
  journal  = "ACM transactions on graphics",
  volume   =  35,
  number   =  4,
  pages    = "1–19",
  abstract = "This paper presents Soli , a new, robust, high-resolution,
              low-power, miniature gesture sensing technology for human-computer
              interaction based on millimeter-wave radar. We describe a new
              approach to developing a radar-based sensor optimized for
              human-computer interaction, building the sensor architecture from
              the ground up with the inclusion of radar design principles, high
              temporal resolution gesture tracking, a hardware abstraction layer
              (HAL), a solid-state radar chip and system architecture,
              interaction models and gesture vocabularies, and gesture
              recognition. We demonstrate that Soli can be used for robust
              gesture recognition and can track gestures with sub-millimeter
              accuracy, running at over 10,000 frames per second on embedded
              hardware.",
  month    =  "7~" # nov,
  year     =  2016,
  url      = "https://doi.org/10.1145%2F2897824.2925953",
  file     = "All Papers/My Library/Lien et al. 2016 - Soli.pdf",
  doi      = "10.1145/2897824.2925953",
  issn     = "0730-0301,1557-7368",
  language = "en"
}

@INPROCEEDINGS{Velloso2016-eb,
  title     = "{AmbiGaze: Direct control of ambient devices by gaze}",
  author    = "Velloso, Eduardo and Wirth, Markus and Weichel, Christian and
               Esteves, Augusto and Gellersen, Hans",
  booktitle = "{Proceedings of the 2016 ACM conference on designing interactive
               systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "812–817",
  abstract  = "Eye tracking offers many opportunities for direct device control
               in smart environments, but issues such as the need for
               calibration and the Midas touch problem make it impractical. In
               this paper, we propose AmbiGaze, a smart environment that employs
               the animation of targets to provide users with direct control of
               devices by gaze only through smooth pursuit tracking. We propose
               a design space of means of exposing functionality through
               movement and illustrate the concept through four prototypes. We
               evaluated the system in a user study and found that AmbiGaze
               enables robust gaze-only interaction with many devices, from
               multiple positions in the environment, in a spontaneous and
               comfortable manner.",
  series    = "DIS '16",
  month     =  "6~" # apr,
  year      =  2016,
  url       = "https://doi.org/10.1145/2901790.2901867",
  doi       = "10.1145/2901790.2901867",
  isbn      =  9781450340311
}

@INPROCEEDINGS{Simeone2016-jo,
  title     = "{Three-Point Interaction: Combining Bi-manual Direct Touch with
               Gaze}",
  author    = "Simeone, Adalberto L and Bulling, Andreas and Alexander, Jason
               and Gellersen, Hans",
  booktitle = "{Proceedings of the International Working Conference on Advanced
               Visual Interfaces}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "168–175",
  abstract  = "The benefits of two-point interaction for tasks that require
               users to simultaneously manipulate multiple entities or
               dimensions are widely known. Two-point interaction has become
               common, e.g., when zooming or pinching using two fingers on a
               smartphone. We propose a novel interaction technique that
               implements three-point interaction by augmenting two-finger
               direct touch with gaze as a third input channel. We evaluate two
               key characteristics of our technique in two multi-participant
               user studies. In the first, we used the technique for object
               selection. In the second, we evaluate it in a 3D matching task
               that requires simultaneous continuous input from fingers and the
               eyes. Our results show that in both cases participants learned to
               interact with three input channels without cognitive or mental
               overload. Participants' performance tended towards fast selection
               times in the first study and exhibited parallel interaction in
               the second. These results are promising and show that there is
               scope for additional input channels beyond two-point interaction.",
  series    = "AVI '16",
  year      =  2016,
  url       = "https://dl.acm.org/doi/10.1145/2909132.2909251",
  file      = "All Papers/My Library/Simeone et al. 2016 - Three-Point Interaction - Combining Bi-manual Direct Touch with Gaze.pdf",
  doi       = "10.1145/2909132.2909251",
  isbn      =  9781450341318
}

@INPROCEEDINGS{Ozgur2017-yb,
  title     = "{Cellulo}",
  author    = "Özgür, Ayberk and Lemaignan, Séverin and Johal, Wafa and Beltran,
               Maria and Briod, Manon and Pereyre, Léa and Mondada, Francesco
               and Dillenbourg, Pierre",
  booktitle = "{Proceedings of the 2017 ACM/IEEE international conference on
               human-robot interaction}",
  publisher = "ACM",
  month     =  mar,
  year      =  2017,
  url       = "https://doi.org/10.1145%2F2909824.3020247",
  file      = "All Papers/My Library/Özgür et al. 2017 - Cellulo.pdf",
  doi       = "10.1145/2909824.3020247"
}

@INPROCEEDINGS{Yamaoka2016-tt,
  title     = "{MiragePrinter: interactive fabrication on a 3D printer with a
               mid-air display}",
  author    = "Yamaoka, Junichi and Kakehi, Yasuaki",
  booktitle = "{ACM SIGGRAPH 2016 Studio}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "The rapid proliferation of digital fabrication machines has
               resulted in creating an environment that enables more people to
               make various creations. From a viewpoint of Human Computer
               Interaction, it is often pointed out that interfaces bridging
               between works in the digital environment and the physical
               environment are necessary to support design for personal
               fabrication [WILLIS 2011] [WEICHEL 2014].",
  series    = "SIGGRAPH '16",
  year      =  2016,
  url       = "https://doi.org/10.1145/2929484.2929489",
  doi       = "10.1145/2929484.2929489",
  isbn      =  9781450343732
}

@INPROCEEDINGS{Newn2016-bl,
  title     = "{Exploring the Effects of Gaze Awareness on Multiplayer Gameplay}",
  author    = "Newn, Joshua and Velloso, Eduardo and Carter, Marcus and Vetere,
               Frank",
  booktitle = "{Proceedings of the 2016 Annual Symposium on Computer-Human
               Interaction in Play Companion Extended Abstracts}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "239–244",
  abstract  = "During tabletop gameplay, players monitor each other's gaze
               throughout, providing a form of implicit nonverbal communication.
               A player can infer the intention and potential strategies by
               estimating the gaze of another, especially in co-located
               gameplay. This early work-in-progress paper details an
               exploratory study design that explores the effects of gaze
               awareness on gameplay, in particular when the gaze of one or more
               players is augmented over the game and revealed to others. We
               believe that players may very well change their strategies when
               gaze becomes `common knowledge', but can also be used as a form
               of deception. We will explore these effects with the same game in
               two settings: a screen-based digital version with players in
               separate rooms and a co-located tabletop board game version
               augmented with a projector. This work evaluates the effects of
               gaze awareness in both settings, providing new insights towards
               the emerging research of EyePlay.",
  series    = "CHI PLAY Companion '16",
  year      =  2016,
  url       = "https://dl.acm.org/doi/10.1145/2968120.2987740",
  doi       = "10.1145/2968120.2987740",
  isbn      =  9781450344586
}

@INPROCEEDINGS{Menges2016-we,
  title     = "{eyeGUI: A Novel Framework for Eye-Controlled User Interfaces}",
  author    = "Menges, Raphael and Kumar, Chandan and Sengupta, Korok and Staab,
               Steffen",
  booktitle = "{Proceedings of the 9th Nordic Conference on Human-Computer
               Interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–6",
  abstract  = "The user interfaces and input events are typically composed of
               mouse and keyboard interactions in generic applications.
               Eye-controlled applications need to revise these interactions to
               eye gestures, and hence design and optimization of interface
               elements becomes a substantial feature. In this work, we propose
               a novel eyeGUI framework, to support the development of such
               interactive eye-controlled applications with many significant
               aspects, like rendering, layout, dynamic modification of content,
               support of graphics and animation.",
  series    = "NordiCHI '16",
  year      =  2016,
  url       = "https://dl.acm.org/doi/10.1145/2971485.2996756",
  doi       = "10.1145/2971485.2996756",
  isbn      =  9781450347631
}

@ARTICLE{Jacob2016-iu,
  title    = "{What you look at is what you get: gaze-based user interfaces}",
  author   = "Jacob, Rob and Stellmach, Sophie",
  journal  = "Interactions",
  volume   =  23,
  number   =  5,
  pages    = "62–65",
  abstract = "Envisioning, designing, and implementing the user interface
              require a comprehensive understanding of interaction technologies.
              In this forum we scout trends and discuss new technologies with
              the potential to influence interaction design. --- Albrecht
              Schmidt, Editor",
  year     =  2016,
  url      = "https://dl.acm.org/doi/10.1145/2978577",
  doi      = "10.1145/2978577",
  issn     = "1072-5520"
}

@INPROCEEDINGS{Chen2016-yz,
  title     = "{Reprise: A design tool for specifying, generating, and
               customizing 3D printable adaptations on everyday objects}",
  author    = "Chen, Xiang 'anthony' and Kim, Jeeeun and Mankoff, Jennifer and
               Grossman, Tovi and Coros, Stelian and Hudson, Scott E",
  booktitle = "{Proceedings of the 29th annual symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "29–39",
  abstract  = "Everyday tools and objects often need to be customized for an
               unplanned use or adapted for specific user, such as adding a
               bigger pull to a zipper or a larger grip for a pen. The advent of
               low-cost 3D printing offers the possibility to rapidly construct
               a wide range of such adaptations. However, while 3D printers are
               now affordable enough for even home use, the tools needed to
               design custom adaptations normally require skills that are beyond
               users with limited 3D modeling experience.In this paper, we
               describe Reprise–a design tool for specifying, generating,
               customizing and fitting adaptations onto existing household
               objects. Reprise allows users to express at a high level what
               type of action is applied to an object. Based on this high level
               specification, Reprise automatically generates adaptations. Users
               can use simple sliders to customize the adaptations to better
               suit their particular needs and preferences, such as increasing
               the tightness for gripping, enhancing torque for rotation, or
               making a larger base for stability. Finally, Reprise provides a
               toolkit of fastening methods and support structures for fitting
               the adaptations onto existing objects.To validate our approach,
               we used Reprise to replicate 15 existing adaptation examples,
               each of which represents a specific category in a design space
               distilled from an analysis of over 3000 cases found in the
               literature and online communities. We believe this work would
               benefit makers and designers for prototyping lifehacking
               solutions and assistive technologies.",
  series    = "UIST '16",
  month     =  "16~" # oct,
  year      =  2016,
  url       = "https://doi.org/10.1145/2984511.2984512",
  file      = "All Papers/My Library/Chen et al. 2016 - Reprise - A design tool for specifying, generating, and customizing 3D printable adaptations on everyday objects.pdf",
  doi       = "10.1145/2984511.2984512",
  isbn      =  9781450341899
}

@INPROCEEDINGS{Pfeuffer2016-ta,
  title     = "{Gaze and touch interaction on tablets}",
  author    = "Pfeuffer, Ken and Gellersen, Hans",
  booktitle = "{Proceedings of the 29th annual symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "301–311",
  abstract  = "We explore how gaze can support touch interaction on tablets.
               When holding the device, the free thumb is normally limited in
               reach, but can provide an opportunity for indirect touch input.
               Here we propose gaze and touch input, where touches redirect to
               the gaze target. This provides whole-screen reachability while
               only using a single hand for both holding and input. We present a
               user study comparing this technique to direct-touch, showing that
               users are slightly slower but can utilise one-handed use with
               less physical effort. To enable interaction with small targets,
               we introduce CursorShift, a method that uses gaze to provide
               users temporal control over cursors during direct-touch
               interactions. Taken together, users can employ three techniques
               on tablets: direct-touch, gaze and touch, and cursor input. In
               three applications, we explore how these techniques can coexist
               in the same UI and demonstrate how tablet tasks can be performed
               with thumb-only input of the holding hand, and with it describe
               novel interaction techniques for gaze based tablet interaction.",
  series    = "UIST '16",
  month     =  "16~" # oct,
  year      =  2016,
  url       = "https://doi.org/10.1145/2984511.2984514",
  file      = "All Papers/My Library/Pfeuffer and Gellersen 2016 - Gaze and touch interaction on tablets.pdf",
  doi       = "10.1145/2984511.2984514",
  isbn      =  9781450341899
}

@INPROCEEDINGS{Goc2016-ks,
  title     = "{Zooids}",
  author    = "Goc, Mathieu Le and Kim, Lawrence H and Parsaei, Ali and Fekete,
               Jean-Daniel and Dragicevic, Pierre and Follmer, Sean",
  booktitle = "{Proceedings of the 29th annual symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F2984511.2984547",
  file      = "All Papers/My Library/Goc et al. 2016 - Zooids.pdf",
  doi       = "10.1145/2984511.2984547"
}

@INPROCEEDINGS{Wang2016-bq,
  title     = "{Interacting with soli}",
  author    = "Wang, Saiwen and Song, Jie and Lien, Jaime and Poupyrev, Ivan and
               Hilliges, Otmar",
  booktitle = "{Proceedings of the 29th annual symposium on user interface
               software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "16~" # oct,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F2984511.2984565",
  doi       = "10.1145/2984511.2984565",
  isbn      =  9781450341899
}

@INPROCEEDINGS{Hiraki2016-go,
  title     = "{Phones on wheels}",
  author    = "Hiraki, Takefumi and Narumi, Koya and Yatani, Koji and Kawahara,
               Yoshihiro",
  booktitle = "{Proceedings of the 29th annual symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F2984751.2985727",
  doi       = "10.1145/2984751.2985727"
}

@INPROCEEDINGS{Lankes2016-vm,
  title     = "{An eye for an eye}",
  author    = "Lankes, Michael and Maurer, Bernhard and Stiglbauer, Barbara",
  booktitle = "{Proceedings of the 13th international conference on advances in
               computer entertainment technology}",
  publisher = "ACM",
  month     =  nov,
  year      =  2016,
  url       = "https://doi.org/10.1145%2F3001773.3001774",
  doi       = "10.1145/3001773.3001774"
}

@INPROCEEDINGS{Ducasse2016-me,
  title     = "{Designing spatial tangible interfaces for visually impaired
               users}",
  author    = "Ducasse, Julie and Oriola, Bernard and Macé, Marc and Jouffrais,
               Christophe",
  booktitle = "{Actes de la 28ième conférence francophone sur l'Interaction
               Homme-Machine on - IHM '16}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  year      =  2016,
  url       = "https://doi.org/10.1145%2F3004107.3004128",
  file      = "All Papers/My Library/Ducasse et al. 2016 - Designing spatial tangible interfaces for visually impaired users.pdf",
  doi       = "10.1145/3004107.3004128",
  isbn      =  9781450342438
}

@INPROCEEDINGS{Schenk2017-vb,
  title     = "{GazeEverywhere: Enabling gaze-only user interaction on an
               unmodified desktop PC in everyday scenarios}",
  author    = "Schenk, Simon and Dreiser, Marc and Rigoll, Gerhard and Dorr,
               Michael",
  booktitle = "{Proceedings of the 2017 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "3034–3044",
  abstract  = "Eye tracking is becoming more and more affordable, and thus gaze
               has the potential to become a viable input modality for
               human-computer interaction. We present the GazeEverywhere
               solution that can replace the mouse with gaze control by adding a
               transparent layer on top of the system GUI. It comprises three
               parts: i) the SPOCK interaction method that is based on smooth
               pursuit eye movements and does not suffer from the Midas touch
               problem; ii) an online recalibration algorithm that continuously
               improves gaze-tracking accuracy using the SPOCK target
               projections as reference points; and iii) an optional hardware
               setup utilizing head-up display technology to project
               superimposed dynamic stimuli onto the PC screen where a software
               modification of the system is not feasible. In validation
               experiments, we show that GazeEverywhere's throughput according
               to ISO 9241-9 was improved over dwell time based interaction
               methods and nearly reached trackpad level. Online recalibration
               reduced interaction target ('button') size by about 25\%.
               Finally, a case study showed that users were able to browse the
               internet and successfully run Wikirace using gaze only, without
               any plug-ins or other modifications.",
  series    = "CHI '17",
  month     =  "5~" # feb,
  year      =  2017,
  url       = "https://doi.org/10.1145/3025453.3025455",
  file      = "All Papers/My Library/Schenk et al. 2017 - GazeEverywhere - Enabling gaze-only user interaction on an unmodified desktop PC in everyday scenarios.pdf",
  doi       = "10.1145/3025453.3025455",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Gronbaek2017-za,
  title     = "{Proxemic transitions}",
  author    = "Grønbæk, Jens Emil and Korsgaard, Henrik and Petersen, Marianne
               Graves and Birk, Morten Henriksen and Krogh, Peter Gall",
  booktitle = "{Proceedings of the 2017 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # feb,
  year      =  2017,
  url       = "https://doi.org/10.1145%2F3025453.3025487",
  doi       = "10.1145/3025453.3025487",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Mott2017-uf,
  title     = "{Improving {Dwell-Based} Gaze Typing with Dynamic, Cascading
               Dwell Times}",
  author    = "Mott, Martez E and Williams, Shane and Wobbrock, Jacob O and
               Morris, Meredith Ringel",
  booktitle = "{Proceedings of the 2017 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2558--2570",
  abstract  = "We present cascading dwell gaze typing, a novel approach to
               dwell-based eye typing that dynamically adjusts the dwell time of
               keys in an on-screen keyboard based on the likelihood that a key
               will be selected next, and the location of the key on the
               keyboard. Our approach makes unlikely keys more difficult to
               select and likely keys easier to select by increasing and
               decreasing their required dwell times, respectively. To maintain
               a smooth typing rhythm for the user, we cascade the dwell time of
               likely keys, slowly decreasing the minimum allowable dwell time
               as a user enters text. Cascading the dwell time affords users the
               benefits of faster dwell times while causing little disruption to
               users' typing cadence. Results from a longitudinal study with 17
               non-disabled participants show that our dynamic cascading dwell
               technique was significantly faster than a static dwell approach.
               Participants were able to achieve typing speeds of 12.39 WPM on
               average with our cascading technique, whereas participants were
               able to achieve typing speeds of 10.62 WPM on average with a
               static dwell time approach. In a small evaluation conducted with
               five people with ALS, participants achieved average typing speeds
               of 9.51 WPM with our cascading dwell approach. These results show
               that our dynamic cascading dwell technique has the potential to
               improve gaze typing for users with and without disabilities.",
  series    = "CHI '17",
  month     =  may,
  year      =  2017,
  url       = "http://dx.doi.org/10.1145/3025453.3025517",
  keywords  = "eye typing, text entry, gaze typing,
               accessibility;prj-gaze-shorthand",
  doi       = "10.1145/3025453.3025517",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Mott2017-jb,
  title     = "{Improving Dwell-Based gaze typing with dynamic, cascading dwell
               times}",
  author    = "Mott, Martez E and Williams, Shane and Wobbrock, Jacob O and
               Morris, Meredith Ringel",
  booktitle = "{Proceedings of the 2017 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2558–2570",
  abstract  = "We present cascading dwell gaze typing, a novel approach to
               dwell-based eye typing that dynamically adjusts the dwell time of
               keys in an on-screen keyboard based on the likelihood that a key
               will be selected next, and the location of the key on the
               keyboard. Our approach makes unlikely keys more difficult to
               select and likely keys easier to select by increasing and
               decreasing their required dwell times, respectively. To maintain
               a smooth typing rhythm for the user, we cascade the dwell time of
               likely keys, slowly decreasing the minimum allowable dwell time
               as a user enters text. Cascading the dwell time affords users the
               benefits of faster dwell times while causing little disruption to
               users' typing cadence. Results from a longitudinal study with 17
               non-disabled participants show that our dynamic cascading dwell
               technique was significantly faster than a static dwell approach.
               Participants were able to achieve typing speeds of 12.39 WPM on
               average with our cascading technique, whereas participants were
               able to achieve typing speeds of 10.62 WPM on average with a
               static dwell time approach. In a small evaluation conducted with
               five people with ALS, participants achieved average typing speeds
               of 9.51 WPM with our cascading dwell approach. These results show
               that our dynamic cascading dwell technique has the potential to
               improve gaze typing for users with and without disabilities.",
  series    = "CHI '17",
  month     =  "5~" # feb,
  year      =  2017,
  url       = "https://doi.org/10.1145/3025453.3025517",
  doi       = "10.1145/3025453.3025517",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Miller2017-mw,
  title     = "{Through the looking glass}",
  author    = "Miller, Matthew K and Mandryk, Regan L and Birk, Max V and
               Depping, Ansgar E and Patel, Tushita",
  booktitle = "{Proceedings of the 2017 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # feb,
  year      =  2017,
  url       = "https://doi.org/10.1145%2F3025453.3025548",
  doi       = "10.1145/3025453.3025548",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Gong2017-bj,
  title     = "{Cito}",
  author    = "Gong, Jun and Li, Lan and Vogel, Daniel and Yang, Xing-Dong",
  booktitle = "{Proceedings of the 2017 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # feb,
  year      =  2017,
  url       = "https://doi.org/10.1145%2F3025453.3025568",
  doi       = "10.1145/3025453.3025568",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Zhang2017-fk,
  title     = "{Smartphone-based gaze gesture communication for people with
               motor disabilities}",
  author    = "Zhang, Xiaoyi and Kulkarni, Harish and Morris, Meredith Ringel",
  booktitle = "{Proceedings of the 2017 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2878–2889",
  abstract  = "Current eye-tracking input systems for people with ALS or other
               motor impairments are expensive, not robust under sunlight, and
               require frequent re-calibration and substantial, relatively
               immobile setups. Eye-gaze transfer (e-tran) boards, a low-tech
               alternative, are challenging to master and offer slow
               communication rates. To mitigate the drawbacks of these two
               status quo approaches, we created GazeSpeak, an eye gesture
               communication system that runs on a smartphone, and is designed
               to be low-cost, robust, portable, and easy-to-learn, with a
               higher communication bandwidth than an e-tran board. GazeSpeak
               can interpret eye gestures in real time, decode these gestures
               into predicted utterances, and facilitate communication, with
               different user interfaces for speakers and interpreters. Our
               evaluations demonstrate that GazeSpeak is robust, has good user
               satisfaction, and provides a speed improvement with respect to an
               e-tran board; we also identify avenues for further improvement to
               low-cost, low-effort gaze-based communication technologies.",
  series    = "CHI '17",
  month     =  "5~" # feb,
  year      =  2017,
  url       = "https://doi.org/10.1145/3025453.3025790",
  doi       = "10.1145/3025453.3025790",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Higuchi2017-dl,
  title     = "{EgoScanning: Quickly Scanning First-Person Videos with
               Egocentric Elastic Timelines}",
  author    = "Higuchi, Keita and Yonetani, Ryo and Sato, Yoichi",
  booktitle = "{Proceedings of the 2017 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "6536–6546",
  abstract  = "This work presents EgoScanning, a novel video fast-forwarding
               interface that helps users to find important events from lengthy
               first-person videos recorded with wearable cameras continuously.
               This interface is featured by an elastic timeline that adaptively
               changes playback speeds and emphasizes egocentric cues specific
               to first-person videos, such as hand manipulations, moving, and
               conversations with people, based on computer-vision techniques.
               The interface also allows users to input which of such cues are
               relevant to events of their interests. Through our user study, we
               confirm that users can find events of interests quickly from
               first-person videos thanks to the following benefits of using the
               EgoScanning interface: 1) adaptive changes of playback speeds
               allow users to watch fast-forwarded videos more easily; 2)
               Emphasized parts of videos can act as candidates of events
               actually significant to users; 3) Users are able to select
               relevant egocentric cues depending on events of their interests.",
  series    = "CHI '17",
  year      =  2017,
  url       = "https://dl.acm.org/doi/10.1145/3025453.3025821",
  doi       = "10.1145/3025453.3025821",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Yu2017-yb,
  title     = "{Tap, dwell or gesture? Exploring head-based text entry
               techniques for {HMDs}}",
  author    = "Yu, Chun and Gu, Yizheng and Yang, Zhican and Yi, Xin and Luo,
               Hengliang and Shi, Yuanchun",
  booktitle = "{Proceedings of the 2017 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "4479–4488",
  abstract  = "Despite the increasing popularity of head mounted displays
               (HMDs), development of efficient text entry methods on these
               devices has remained under explored. In this paper, we
               investigate the feasibility of head-based text entry for HMDs, by
               which, the user controls a pointer on a virtual keyboard using
               head rotation. Specifically, we investigate three techniques:
               TapType, DwellType, and GestureType. Users of TapType select a
               letter by pointing to it and tapping a button. Users of DwellType
               select a letter by pointing to it and dwelling over it for a
               period of time. Users of GestureType perform word-level input
               using a gesture typing style. Two lab studies were conducted. In
               the first study, users typed 10.59 WPM, 15.58 WPM, and 19.04 WPM
               with DwellType, TapType, and GestureType, respectively. Users
               subjectively felt that all three of the techniques were easy to
               learn and considered the induced fatigue to be acceptable. In the
               second study, we further investigated GestureType. We improved
               its gesture-word recognition algorithm by incorporating the head
               movement pattern obtained from the first study. This resulted in
               users reaching 24.73 WPM after 60 minutes of training. Based on
               these results, we argue that head-based text entry is feasible
               and practical on HMDs, and deserves more attention.",
  series    = "CHI '17",
  month     =  "5~" # feb,
  year      =  2017,
  url       = "https://doi.org/10.1145/3025453.3025964",
  file      = "All Papers/My Library/Yu et al. 2017 - Tap, dwell or gesture - Exploring head-based text entry techniques for HMDs.pdf",
  doi       = "10.1145/3025453.3025964",
  isbn      =  9781450346559
}

@INPROCEEDINGS{Mott2017-hn,
  title     = "{Accessible touch input for people with motor impairments}",
  author    = "Mott, Martez E",
  booktitle = "{Proceedings of the 2017 CHI conference extended abstracts on
               human factors in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "307–311",
  abstract  = "Touch input has emerged as a dominant form of interaction for
               billions of interactive computing devices like smartphones,
               tablets, and interactive surfaces. Although touch input is widely
               used, it remains largely inaccessible for many people with motor
               impairments such as cerebral palsy, muscular dystrophy, and
               multiple sclerosis. For my thesis, I will employ advanced pattern
               matching techniques to create intelligent touch interaction
               techniques that allow users with motor impairments to touch the
               screen in whichever way is most comfortable and natural to them,
               and for the system to respond as if the screen was touched
               precisely. My thesis research aims to improve the accessibility
               of touch-enabled devices for users with motor impairments,
               broadening access to these devices like never before.",
  series    = "CHI EA '17",
  month     =  "5~" # jun,
  year      =  2017,
  url       = "https://doi.org/10.1145/3027063.3027123",
  file      = "All Papers/My Library/Mott 2017 - Accessible touch input for people with motor impairments.pdf",
  doi       = "10.1145/3027063.3027123",
  isbn      =  9781450346566
}

@INPROCEEDINGS{Zhai1999-ep,
  title     = "{Manual and gaze input cascaded (MAGIC) pointing}",
  author    = "Zhai, Shumin and Morimoto, Carlos and Ihde, Steven",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "246–253",
  abstract  = "This work explores a new direction in utilizing eye gaze for
               computer input. Gaze tracking has long been considered as an
               alternative or potentially superior pointing method for computer
               input. We believe that many fundamental limitations exist with
               traditional gaze pointing. In particular, it is unnatural to
               overload a perceptual channel such as vision with a motor control
               task. We therefore propose an alternative approach, dubbed MAGIC
               (Manual And Gaze Input Cascaded) pointing. With such an approach,
               pointing appears to the user to be a manual task, used for fine
               manipulation and selection. However, a large portion of the
               cursor movement is eliminated by warping the cursor to the eye
               gaze area, which encompasses the target. Two specific MAGIC
               pointing techniques, one conservative and one liberal, were
               designed, analyzed, and implemented with an eye tracker we
               developed. They were then tested in a pilot study. This early-
               stage exploration showed that the MAGIC pointing techniques might
               offer many advantages, including reduced physical effort and
               fatigue as compared to traditional manual pointing, greater
               accuracy and naturalness than traditional gaze pointing, and
               possibly faster speed than manual pointing. The pros and cons of
               the two techniques are discussed in light of both performance
               data and subjective reports.",
  series    = "CHI '99",
  month     =  "5~" # jan,
  year      =  1999,
  url       = "https://doi.org/10.1145/302979.303053",
  file      = "All Papers/My Library/Zhai et al. 1999 - Manual and gaze input cascaded (MAGIC) pointing.pdf",
  doi       = "10.1145/302979.303053",
  isbn      =  9780201485592
}

@INPROCEEDINGS{Vertegaal1999-bh,
  title     = "{The {GAZE} groupware system: mediating joint attention in
               multiparty communication and collaboration}",
  author    = "Vertegaal, Roel",
  booktitle = "{Proceedings of the {SIGCHI} conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "294--301",
  abstract  = "In this paper, we discuss why, in designing multiparty mediated
               systems, we should focus first on providing non-verbal cues which
               are less redundantly coded in speech than those normally conveyed
               by video. We show how conveying one such cue, gaze direction, may
               solve two problems in multiparty mediated communication and
               collaboration: knowing who is talking to whom, and who is talking
               about what. As a candidate solution, we present the GAZE
               Groupware System, which combines support for gaze awareness in
               multiparty mediated communication and collaboration with small
               and linear bandwidth requirements. The system uses an advanced,
               desk- mounted eyetracker to metaphorically convey gaze awareness
               in a 3D virtual meeting room and within shared documents.",
  series    = "CHI '99",
  month     =  may,
  year      =  1999,
  url       = "http://dx.doi.org/10.1145/302979.303065",
  file      = "All Papers/Other/Vertegaal 1999 - The GAZE groupware system - mediating joint attention in multiparty communication and collaboration.pdf",
  keywords  = "attention, VRML 2, CSCW, multiparty videoconferencing,
               eyetracking, gaze direction, awareness",
  doi       = "10.1145/302979.303065",
  isbn      =  9780201485592
}

@INPROCEEDINGS{Vertegaal1999-zw,
  title     = "{The GAZE groupware system: mediating joint attention in
               multiparty communication and collaboration}",
  author    = "Vertegaal, Roel",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "294–301",
  abstract  = "In this paper, we discuss why, in designing multiparty mediated
               systems, we should focus first on providing non-verbal cues which
               are less redundantly coded in speech than those normally conveyed
               by video. We show how conveying one such cue, gaze direction, may
               solve two problems in multiparty mediated communication and
               collaboration: knowing who is talking to whom, and who is talking
               about what. As a candidate solution, we present the GAZE
               Groupware System, which combines support for gaze awareness in
               multiparty mediated communication and collaboration with small
               and linear bandwidth requirements. The system uses an advanced,
               desk- mounted eyetracker to metaphorically convey gaze awareness
               in a 3D virtual meeting room and within shared documents.",
  series    = "CHI '99",
  month     =  may,
  year      =  1999,
  url       = "http://dx.doi.org/10.1145/302979.303065",
  file      = "All Papers/My Library/Vertegaal 1999 - The GAZE groupware system - mediating joint attention in multiparty communication and collaboration.pdf",
  doi       = "10.1145/302979.303065",
  isbn      =  9780201485592
}

@INPROCEEDINGS{Ozgur2017-st,
  title     = "{Windfield}",
  author    = "Özgür, Ayberk and Johal, Wafa and Mondada, Francesco and
               Dillenbourg, Pierre",
  booktitle = "{Proceedings of the companion of the 2017 ACM/IEEE international
               conference on human-robot interaction}",
  publisher = "ACM",
  month     =  mar,
  year      =  2017,
  url       = "https://doi.org/10.1145%2F3029798.3036664",
  file      = "All Papers/My Library/Özgür et al. 2017 - Windfield.pdf",
  doi       = "10.1145/3029798.3036664"
}

@INPROCEEDINGS{Rodrigues2017-fe,
  title     = "{Evaluation of a head-tracking pointing device for users with
               motor disabilities}",
  author    = "Rodrigues, Andreia Sias and da Costa, Vinicius Kruger and
               Cardoso, Rafael Cunha and Machado, Marcio Bender and Machado,
               Marcelo Bender and Tavares, Tatiana Aires",
  booktitle = "{Proceedings of the 10th international conference on PErvasive
               technologies related to assistive environments}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "156–162",
  abstract  = "People with disabilities generally do not have the same access to
               health care, education and employment opportunities. It is known
               that, very often, they do not receive the support they need, and
               end up experiencing the taste of exclusion to perform their daily
               activities. Analyzing the overall rates of people with
               disabilities, it is possible to perceive that social inclusion of
               minorities is not a simple task. One of these daily activities is
               the use of information technology. Considering the cost of
               commercial Assistive Technology devices are very expensive for
               developing countries like Brazil, we proposed a comparative
               evaluation between the low-cost wearable head-based device and
               the performance values obtained in Kurauchi et al. [8] work, to
               analyses the viability of this low-cost device. This Experimental
               results indicate that IOM presented a better accuracy than the
               two other devices, providing accurate mouse pointer positioning.
               Even though the speed of the low-cost IOM device was very similar
               than the devices analyzed and compared, the most significant
               result was the low-cost device accuracy was 100\%, error rate was
               0\%.",
  series    = "PETRA '17",
  month     =  "21~" # jun,
  year      =  2017,
  url       = "https://doi.org/10.1145/3056540.3056552",
  doi       = "10.1145/3056540.3056552",
  isbn      =  9781450352277
}

@INPROCEEDINGS{Kane2017-sq,
  title     = "{Let's Talk About X: Combining Image Recognition and Eye Gaze to
               Support Conversation for People with {ALS}}",
  author    = "Kane, Shaun K and Morris, Meredith Ringel",
  booktitle = "{Proceedings of the 2017 Conference on Designing Interactive
               Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "129--134",
  abstract  = "Communicating at a natural speed is a significant challenge for
               users of augmentative and alternative communication (AAC)
               devices, especially when input is provided by eye gaze, as is
               common for people with ALS and similar conditions. One way to
               improve AAC throughput is by drawing on contextual information
               from the outside world. Toward this goal, we present SceneTalk, a
               prototype gaze-based AAC system that uses computer vision to
               identify objects in the user's field of view and suggests words
               and phrases related to the current scene. We conducted a
               formative evaluation of SceneTalk with six people with ALS, in
               which we evaluated their preference for user interface modes and
               output preferences. Participants agreed that integrating
               contextual awareness into their AAC device could be helpful
               across a diverse range of situations.",
  series    = "DIS '17",
  month     =  jun,
  year      =  2017,
  url       = "http://dx.doi.org/10.1145/3064663.3064762",
  keywords  = "eye gaze, augmentative and alternative communication, als,
               computer vision, assistive technology;prj-gaze-shorthand",
  doi       = "10.1145/3064663.3064762",
  isbn      =  9781450349222
}

@INPROCEEDINGS{Kane2017-my,
  title     = "{Let's talk about X: Combining image recognition and eye gaze to
               support conversation for people with {ALS}}",
  author    = "Kane, Shaun K and Morris, Meredith Ringel",
  booktitle = "{Proceedings of the 2017 conference on designing interactive
               systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "129–134",
  abstract  = "Communicating at a natural speed is a significant challenge for
               users of augmentative and alternative communication (AAC)
               devices, especially when input is provided by eye gaze, as is
               common for people with ALS and similar conditions. One way to
               improve AAC throughput is by drawing on contextual information
               from the outside world. Toward this goal, we present SceneTalk, a
               prototype gaze-based AAC system that uses computer vision to
               identify objects in the user's field of view and suggests words
               and phrases related to the current scene. We conducted a
               formative evaluation of SceneTalk with six people with ALS, in
               which we evaluated their preference for user interface modes and
               output preferences. Participants agreed that integrating
               contextual awareness into their AAC device could be helpful
               across a diverse range of situations.",
  series    = "DIS '17",
  month     =  "6~" # oct,
  year      =  2017,
  url       = "https://doi.org/10.1145/3064663.3064762",
  doi       = "10.1145/3064663.3064762",
  isbn      =  9781450349222
}

@INPROCEEDINGS{Zhang2017-oc,
  title     = "{Everyday eye contact detection using unsupervised gaze target
               discovery}",
  author    = "Zhang, Xucong and Sugano, Yusuke and Bulling, Andreas",
  booktitle = "{Proceedings of the 30th annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "193–203",
  abstract  = "Eye contact is an important non-verbal cue in social signal
               processing and promising as a measure of overt attention in
               human-object interactions and attentive user interfaces. However,
               robust detection of eye contact across different users, gaze
               targets, camera positions, and illumination conditions is
               notoriously challenging. We present a novel method for eye
               contact detection that combines a state-of-the-art
               appearance-based gaze estimator with a novel approach for
               unsupervised gaze target discovery, i.e. without the need for
               tedious and time-consuming manual data annotation. We evaluate
               our method in two real-world scenarios: detecting eye contact at
               the workplace, including on the main work display, from cameras
               mounted to target objects, as well as during everyday social
               interactions with the wearer of a head-mounted egocentric camera.
               We empirically evaluate the performance of our method in both
               scenarios and demonstrate its effectiveness for detecting eye
               contact independent of target object type and size, camera
               position, and user and recording environment.",
  series    = "UIST '17",
  month     =  "20~" # oct,
  year      =  2017,
  url       = "https://doi.org/10.1145/3126594.3126614",
  file      = "All Papers/My Library/Zhang et al. 2017 - Everyday eye contact detection using unsupervised gaze target discovery.pdf",
  doi       = "10.1145/3126594.3126614",
  isbn      =  9781450349819
}

@ARTICLE{Kim2017-zi,
  title    = "{UbiSwarm}",
  author   = "Kim, Lawrence H and Follmer, Sean",
  journal  = "Proceedings of the ACM on interactive, mobile, wearable and
              ubiquitous technologies",
  volume   =  1,
  number   =  3,
  pages    = "1–20",
  month    =  "9~" # nov,
  year     =  2017,
  url      = "https://doi.org/10.1145%2F3130931",
  doi      = "10.1145/3130931",
  issn     = "2474-9567",
  language = "en"
}

@INPROCEEDINGS{Suzuki2017-dc,
  title     = "{FluxMarker}",
  author    = "Suzuki, Ryo and Stangl, Abigale and Gross, Mark D and Yeh, Tom",
  booktitle = "{Proceedings of the 19th international ACM SIGACCESS conference
               on computers and accessibility}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "19~" # oct,
  year      =  2017,
  url       = "https://doi.org/10.1145%2F3132525.3132548",
  file      = "All Papers/My Library/Suzuki et al. 2017 - FluxMarker.pdf",
  doi       = "10.1145/3132525.3132548",
  isbn      =  9781450349260
}

@ARTICLE{Sturdee2018-ie,
  title    = "{Analysis and classification of shape-changing interfaces for
              design and application-based research}",
  author   = "Sturdee, Miriam and Alexander, Jason",
  journal  = "ACM computing surveys",
  volume   =  51,
  number   =  1,
  pages    = "1–32",
  abstract = "Shape-changing interfaces are physically tangible, interactive
              devices, surfaces, or spaces that allow for rich, organic, and
              novel experiences with computational devices. Over the last 15
              years, research has produced functional prototypes over many use
              applications; reviews have identified themes and possible future
              directions but have not yet looked at possible design or
              application-based research. Here, we gather this information
              together to provide a reference for designers and researchers
              wishing to build upon existing prototyping work, using synthesis
              and discussion of existing shape-changing interface reviews and
              comprehensive analysis and classification of 84 shape-changing
              interfaces. Eight categories of prototype are identified alongside
              recommendations for the field.",
  month    =  "14~" # apr,
  year     =  2018,
  url      = "https://doi.org/10.1145%2F3143559",
  file     = "All Papers/My Library/Sturdee and Alexander 2018 - Analysis and classification of shape-changing interfaces for design and application-based research.pdf",
  doi      = "10.1145/3143559",
  issn     = "0360-0300,1557-7341",
  language = "en"
}

@INPROCEEDINGS{Ma2018-ee,
  title     = "{Combining brain-computer interface and eye tracking for
               high-speed text entry in virtual reality}",
  author    = "Ma, Xinyao and Yao, Zhaolin and Wang, Yijun and Pei, Weihua and
               Chen, Hongda",
  booktitle = "{23rd international conference on intelligent user interfaces}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "263–267",
  abstract  = "Gaze interaction provides an efficient way for users to
               communicate and control in virtual reality (VR) presented by
               head-mounted displays. In gaze-based text-entry systems, eye
               tracking and brain-computer interface (BCI) are the two most
               commonly used approaches. This paper presents a hybrid BCI system
               for text entry in VR by combining steady-state visual evoked
               potentials (SSVEP) and eye tracking. The user interface in VR
               designed a 40-target virtual keyboard using a joint
               frequency-phase modulation method for SSVEP. Eye position was
               measured by an eye-tracking accessory in the VR headset.
               Target-related gaze direction was detected by combining
               simultaneously recorded SSVEP and eye position data. Offline and
               online experiments indicate that the proposed system can type at
               a speed around 10 words per minute, leading to an information
               transfer rate (ITR) of 270 bits per minute. The results further
               demonstrate the superiority of the hybrid method over
               single-modality methods for VR applications.",
  series    = "IUI '18",
  month     =  "3~" # may,
  year      =  2018,
  url       = "https://doi.org/10.1145/3172944.3172988",
  doi       = "10.1145/3172944.3172988",
  isbn      =  9781450349451
}

@INPROCEEDINGS{Katsini2018-ze,
  title     = "{Eye gaze-driven prediction of cognitive differences during
               graphical password composition}",
  author    = "Katsini, Christina and Fidas, Christos and Raptis, George E and
               Belk, Marios and Samaras, George and Avouris, Nikolaos",
  booktitle = "{23rd International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # mar,
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3172944.3172996",
  file      = "All Papers/Other/Katsini et al. 2018 - Eye gaze-driven prediction of cognitive differences during graphical password composition.pdf",
  doi       = "10.1145/3172944.3172996",
  isbn      =  9781450349451
}

@INPROCEEDINGS{Malla2018-th,
  title     = "{Eye tracking- single technology to handle multiple domains}",
  author    = "Malla, Adil Hamid and Hammond, Tracy",
  booktitle = "{23rd International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # mar,
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3172944.3173152",
  file      = "All Papers/Other/Malla and Hammond 2018 - Eye tracking- single technology to handle multiple domains.pdf",
  doi       = "10.1145/3172944.3173152",
  isbn      =  9781450349451
}

@INPROCEEDINGS{Ikegawa2018-ag,
  title     = "{Investigation of touch interfaces using multilayered urushi
               circuit}",
  author    = "Ikegawa, Koshi and Aoyama, Shuhei and Tsuchikiri, Shogo and
               Nakamura, Takuto and Hashimoto, Yuki and Shizuki, Buntarou",
  booktitle = "{Proceedings of the twelfth international conference on tangible,
               embedded, and embodied interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "115–122",
  abstract  = "Urushi (Japanese lacquer) is a natural resin paint with
               electrical insulating capability. By using it as a base material
               and coating material for electronic circuits, it is possible to
               construct a circuit with an elegant appearance and feel. It is
               also possible to build a multilayered electronic circuit by using
               urushi as insulation layers. In this study, we investigate
               techniques to construct touch interfaces using a multilayered
               electronic circuit (urushi circuit). At first, we fabricated an
               urushi circuit with a touch electrode consisting of two layers.
               To improve its appearance, we fabricated urushi circuits in which
               all touch electrodes are arranged on the top layer and all wires
               are hidden in the bottom layer. Moreover, as an extension of the
               touch interface, we built a grid of touch electrodes that
               realizes two-dimensional touch sensing.",
  series    = "TEI '18",
  month     =  "18~" # mar,
  year      =  2018,
  url       = "https://doi.org/10.1145/3173225.3173285",
  doi       = "10.1145/3173225.3173285",
  isbn      =  9781450355681
}

@INPROCEEDINGS{Aiyer2018-xj,
  title     = "{ParticiPod: Portable telepresence for immersive activity
               sharing}",
  author    = "Aiyer, Sriraj and Arnott, Stuart and Sharbain, Raya and Wang, Qin
               and Wen, Heyi",
  booktitle = "{Companion of the 2018 ACM/IEEE international conference on
               human-robot interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "331–332",
  abstract  = "Varying factors prevent loved ones from being together
               physically. There are existing means of facilitating
               conversation, be it through text voice, video chat or
               telepresence products. However, current solutions do not have the
               portability and personification necessary to allow for shared
               activities with one another and a sense of presence. We present
               the design for ParticiPod, a portable device to enable
               telepresence in a more tangible way. We have identified a
               potential gap in research and present a physical prototype that
               demonstrates key features. We also detail the ways in which
               ParticiPod contributes to social good.",
  series    = "HRI '18",
  month     =  "3~" # jan,
  year      =  2018,
  url       = "https://doi.org/10.1145/3173386.3177825",
  doi       = "10.1145/3173386.3177825",
  isbn      =  9781450356152
}

@INPROCEEDINGS{Xiao2018-yi,
  title     = "{LumiWatch: On-arm projected graphics and touch input}",
  author    = "Xiao, Robert and Cao, Teng and Guo, Ning and Zhuo, Jun and Zhang,
               Yang and Harrison, Chris",
  booktitle = "{Proceedings of the 2018 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–11",
  abstract  = "Compact, worn computers with projected, on-skin touch interfaces
               have been a long-standing yet elusive goal, largely written off
               as science fiction. Such devices offer the potential to mitigate
               the significant human input/output bottleneck inherent in worn
               devices with small screens. In this work, we present the first
               fully functional and self-contained projection smartwatch
               implementation, containing the requisite compute, power,
               projection and touch-sensing capabilities. Our watch offers
               roughly 40 sq. cm of interactive surface area – more than five
               times that of a typical smartwatch display. We demonstrate
               continuous 2D finger tracking with interactive, rectified
               graphics, transforming the arm into a touchscreen. We discuss our
               hardware and software implementation, as well as evaluation
               results regarding touch accuracy and projection visibility.",
  series    = "CHI '18",
  month     =  "19~" # apr,
  year      =  2018,
  url       = "https://doi.org/10.1145/3173574.3173669",
  doi       = "10.1145/3173574.3173669",
  isbn      =  9781450356206
}

@INPROCEEDINGS{Suzuki2018-jg,
  title     = "{Reactile}",
  author    = "Suzuki, Ryo and Kato, Jun and Gross, Mark D and Yeh, Tom",
  booktitle = "{Proceedings of the 2018 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "19~" # apr,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3173574.3173773",
  doi       = "10.1145/3173574.3173773",
  isbn      =  9781450356206
}

@INCOLLECTION{Xia2018-ra,
  title     = "{{DataInk}: Direct and Creative {Data-Oriented} Drawing}",
  author    = "Xia, Haijun and Henry Riche, Nathalie and Chevalier, Fanny and De
               Araujo, Bruno and Wigdor, Daniel",
  booktitle = "{Proceedings of the 2018 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--13",
  abstract  = "Creating whimsical, personal data visualizations remains a
               challenge due to a lack of tools that enable for creative visual
               expression while providing support to bind graphical content to
               data. Many data analysis and visualization creation tools target
               the quick generation of visual representations, but lack the
               functionality necessary for graphics design. Toolkits and
               charting libraries offer more expressive power, but require
               expert programming skills to achieve custom designs. In contrast,
               sketching affords fluid experimentation with visual shapes and
               layouts in a free-form manner, but requires one to manually draw
               every single data point. We aim to bridge the gap between these
               extremes. We propose DataInk, a system supports the creation of
               expressive data visualizations with rigorous direct manipulation
               via direct pen and touch input. Leveraging our commonly held
               skills, coupled with a novel graphical user interface, DataInk
               enables direct, fluid, and flexible authoring of creative data
               visualizations.",
  month     =  apr,
  year      =  2018,
  url       = "http://dx.doi.org/10.1145/3173574.3173797",
  doi       = "10.1145/3173574.3173797",
  isbn      =  9781450356206
}

@INCOLLECTION{Xia2018-nz,
  title     = "{DataInk: Direct and creative Data-Oriented drawing}",
  author    = "Xia, Haijun and Henry Riche, Nathalie and Chevalier, Fanny and De
               Araujo, Bruno and Wigdor, Daniel",
  booktitle = "{Proceedings of the 2018 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "Creating whimsical, personal data visualizations remains a
               challenge due to a lack of tools that enable for creative visual
               expression while providing support to bind graphical content to
               data. Many data analysis and visualization creation tools target
               the quick generation of visual representations, but lack the
               functionality necessary for graphics design. Toolkits and
               charting libraries offer more expressive power, but require
               expert programming skills to achieve custom designs. In contrast,
               sketching affords fluid experimentation with visual shapes and
               layouts in a free-form manner, but requires one to manually draw
               every single data point. We aim to bridge the gap between these
               extremes. We propose DataInk, a system supports the creation of
               expressive data visualizations with rigorous direct manipulation
               via direct pen and touch input. Leveraging our commonly held
               skills, coupled with a novel graphical user interface, DataInk
               enables direct, fluid, and flexible authoring of creative data
               visualizations.",
  month     =  apr,
  year      =  2018,
  url       = "http://dx.doi.org/10.1145/3173574.3173797",
  doi       = "10.1145/3173574.3173797",
  isbn      =  9781450356206
}

@INPROCEEDINGS{Matulic2018-cx,
  title     = "{Multiray: Multi-finger raycasting for large displays}",
  author    = "Matulic, Fabrice and Vogel, Daniel",
  booktitle = "{Proceedings of the 2018 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "We explore and evaluate a multi-finger raycasting design space
               that we call “multiray”. Each finger projects a ray on to the
               display, so the user is interacting from a distance using a form
               of direct input. Specifically, we propose techniques, where
               patterns of ray intersections created by hand postures form 2D
               geometric shapes to trigger actions and perform direct
               manipulations that go beyond single-point selections. Two
               formative studies examine characteristics of multi-finger
               raycasting for different projection methods, shapes, and tasks.
               Based on the results of those investigations, we demonstrate a
               number of dynamic UI controls and operations that utilise
               multiray points and shapes.",
  series    = "CHI '18",
  month     =  "21~" # apr,
  year      =  2018,
  url       = "https://doi.org/10.1145/3173574.3173819",
  doi       = "10.1145/3173574.3173819",
  isbn      =  9781450356206
}

@INPROCEEDINGS{Fiannaca2018-pg,
  title     = "{Voicesetting: Voice authoring UIs for improved expressivity in
               augmentative communication}",
  author    = "Fiannaca, Alexander J and Paradiso, Ann and Campbell, Jon and
               Morris, Meredith Ringel",
  booktitle = "{Proceedings of the 2018 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "Alternative and augmentative communication (AAC) systems used by
               people with speech disabilities rely on text-to-speech (TTS)
               engines for synthesizing speech. Advances in TTS systems allowing
               for the rendering of speech with a range of emotions have yet to
               be incorporated into AAC systems, leaving AAC users with speech
               that is mostly devoid of emotion and expressivity. In this work,
               we describe voicesetting as the process of authoring the speech
               properties of text. We present the design and evaluation of two
               voicesetting user interfaces: the Expressive Keyboard, designed
               for rapid addition of expressivity to speech, and the
               Voicesetting Editor, designed for more careful crafting of the
               way text should be spoken. We evaluated the perceived output
               quality, requisite effort, and usability of both interfaces; the
               concept of voicesetting and our interfaces were highly valued by
               end-users as an enhancement to communication quality. We close by
               discussing design insights from our evaluations.",
  series    = "CHI '18",
  month     =  "21~" # apr,
  year      =  2018,
  url       = "https://doi.org/10.1145/3173574.3173857",
  doi       = "10.1145/3173574.3173857",
  isbn      =  9781450356206
}

@INPROCEEDINGS{Alexander2018-cw,
  title     = "{Grand challenges in shape-changing interface research}",
  author    = "Alexander, Jason and Roudaut, Anne and Steimle, Jürgen and
               Hornbæk, Kasper and Alonso, Miguel Bruns and Follmer, Sean and
               Merritt, Timothy",
  booktitle = "{Proceedings of the 2018 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  apr,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3173574.3173873",
  file      = "All Papers/My Library/Alexander et al. 2018 - Grand challenges in shape-changing interface research.pdf",
  doi       = "10.1145/3173574.3173873"
}

@INPROCEEDINGS{Hagihara2018-dz,
  title     = "{Object-wise 3D gaze mapping in physical workspace}",
  author    = "Hagihara, Kakeru and Taniguchi, Keiichiro and Abibouraguimane,
               Irshad and Itoh, Yuta and Higuchi, Keita and Otsuka, Jiu and
               Sugimoto, Maki and Sato, Yoichi",
  booktitle = "{Proceedings of the 9th augmented human international conference}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1–5",
  abstract  = "Understanding the intention of other people is a fundamental
               social skill in human communication. Eye behavior is an
               important, yet implicit communication cue. In this work, we focus
               on enabling people to see the users' gaze associated with objects
               in the 3D space, namely, we present users the history of gaze
               linked to real 3D objects. Our 3D gaze visualization system
               automatically segments objects in the workspace and projects
               user's gaze trajectory onto the objects in 3D for visualizing
               user's intention. By combining automated object segmentation and
               head tracking via the first-person video from a wearable eye
               tracker, our system can visualize user's gaze behavior more
               intuitively and efficiently compared to 2D based methods and 3D
               methods with manual annotation. We performed an evaluation of the
               system to measure the accuracy of object-wise gaze mapping. In
               the evaluation, the system achieved 94\% accuracy of gaze mapping
               onto 40, 30, 20, 10-centimeter cubes. We also conducted a case
               study of through a case study where the user looks at food
               products, we showed that our system was able to predict products
               that the user is interested in.",
  series    = "AH '18",
  month     =  "2~" # jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3174910.3174921",
  doi       = "10.1145/3174910.3174921",
  isbn      =  9781450354158
}

@INPROCEEDINGS{Umezawa2018-mi,
  title     = "{Egocentric video multi-viewer for analyzing skilled behaviors
               based on gaze object}",
  author    = "Umezawa, Yuki and Hirayama, Takatsugu and Enokibori, Yu and Mase,
               Kenji",
  booktitle = "{Proceedings of the 23rd International Conference on Intelligent
               User Interfaces Companion}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "In many intellectual tasks, efficient succession of human
               physical and sensory skills is a long-standing issue. In order to
               analyze skilled behaviors, a useful approach is to compare same
               task scenes among workers or days and to understand these
               differences. In this paper, we propose an egocentric scene
               classification method based on objects which the worker turned
               the gaze to and a multi-viewer for egocentric videos comparison.
               We have experimented on proposed classification method with
               videos of painting watercolor.",
  month     =  "5~" # mar,
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3180308.3180342",
  file      = "All Papers/gaze-llm/Umezawa et al. 2018 - Egocentric video multi-viewer for analyzing skilled behaviors based on gaze object.pdf",
  doi       = "10.1145/3180308.3180342",
  isbn      =  9781450355711
}

@INPROCEEDINGS{Yoo2018-sw,
  title     = "{Gaze Data Clustering and Analysis}",
  author    = "Yoo, Sangbong and Jeong, Sujin and Jang, Yun",
  booktitle = "{Proceedings of the 23rd International Conference on Intelligent
               User Interfaces Companion}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # mar,
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3180308.3180359",
  file      = "All Papers/Other/Yoo et al. 2018 - Gaze Data Clustering and Analysis.pdf",
  doi       = "10.1145/3180308.3180359",
  isbn      =  9781450355711
}

@ARTICLE{Morrison2018-aa,
  title    = "{Visualizing ubiquitously sensed measures of motor ability in
              multiple sclerosis: Reflections on communicating machine learning
              in practice}",
  author   = "Morrison, Cecily and Huckvale, Kit and Corish, Bob and Banks,
              Richard and Grayson, Martin and Dorn, Jonas and Sellen, Abigail
              and Lindley, Sân",
  journal  = "ACM Transactions on Interactive Intelligent Systems",
  volume   =  8,
  number   =  2,
  pages    = "1–28",
  abstract = "Sophisticated ubiquitous sensing systems are being used to measure
              motor ability in clinical settings. Intended to augment clinical
              decision-making, the interpretability of the machine-learning
              measurements underneath becomes critical to their use. We explore
              how visualization can support the interpretability of
              machine-learning measures through the case of Assess MS, a system
              to support the clinical assessment of Multiple Sclerosis. A
              substantial design challenge is to make visible the algorithm's
              decision-making process in a way that allows clinicians to
              integrate the algorithm's result into their own decision process.
              To this end, we present a series of design iterations that probe
              the challenges in supporting interpretability in a real-world
              system. The key contribution of this article is to illustrate that
              simply making visible the algorithmic decision-making process is
              not helpful in supporting clinicians in their own decision-making
              process. It disregards that people and algorithms make decisions
              in different ways. Instead, we propose that visualisation can
              provide context to algorithmic decision-making, rendering
              observable a range of internal workings of the algorithm from data
              quality issues to the web of relationships generated in the
              machine-learning process.",
  month    =  "14~" # jul,
  year     =  2018,
  url      = "https://doi.org/10.1145/3181670",
  doi      = "10.1145/3181670",
  issn     = "2160-6455"
}

@ARTICLE{Gotzelmann2018-oc,
  title   = "{Visually augmented audio-tactile graphics for visually impaired
             people}",
  author  = "Götzelmann, T",
  journal = "ACM transactions on accessible computing",
  volume  =  11,
  number  =  2,
  pages   = "1–31",
  month   =  jun,
  year    =  2018,
  url     = "https://doi.org/10.1145%2F3186894",
  file    = "All Papers/My Library/Götzelmann 2018 - Visually augmented audio-tactile graphics for visually impaired people.pdf",
  doi     = "10.1145/3186894",
  issn    = "1936-7228"
}

@INPROCEEDINGS{Asselborn2018-ug,
  title     = "{Bringing letters to life}",
  author    = "Asselborn, Thibault and Guneysu, Arzu and Mrini, Khalil and
               Yadollahi, Elmira and Ozgur, Ayberk and Johal, Wafa and
               Dillenbourg, Pierre",
  booktitle = "{Proceedings of the 17th ACM conference on interaction design and
               children}",
  publisher = "ACM",
  month     =  jun,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3202185.3202747",
  file      = "All Papers/My Library/Asselborn et al. 2018 - Bringing letters to life.pdf",
  doi       = "10.1145/3202185.3202747"
}

@ARTICLE{Ducasse2018-au,
  title   = "{BotMap}",
  author  = "Ducasse, Julie and Macé, Marc and Oriola, Bernard and Jouffrais,
             Christophe",
  journal = "ACM Trans. Comput. -Hum. Interact.",
  volume  =  25,
  number  =  4,
  pages   = "1–42",
  month   =  aug,
  year    =  2018,
  url     = "https://doi.org/10.1145%2F3204460",
  file    = "All Papers/My Library/Ducasse et al. 2018 - BotMap.pdf",
  doi     = "10.1145/3204460"
}

@INPROCEEDINGS{Isomoto2018-rg,
  title     = "{Dwell time reduction technique using Fitts' law for gaze-based
               target acquisition}",
  author    = "Isomoto, Toshiya and Ando, Toshiyuki and Shizuki, Buntarou and
               Takahashi, Shin",
  booktitle = "{Proceedings of the 2018 ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "We present a dwell time reduction technique for gaze-based target
               acquisition. We adopt Fitts' Law to achieve the dwell time
               reduction. Our technique uses both the eye movement time for
               target acquisition estimated using Fitts' Law (Te) and the actual
               eye movement time (Ta) for target acquisition; a target is
               acquired when the difference between Te and Ta is small. First,
               we investigated the relation between the eye movement for target
               acquisition and Fitts' Law; the result indicated a correlation of
               0.90 after error correction. Then we designed and implemented our
               technique. Finally, we conducted a user study to investigate the
               performance of our technique; an average dwell time of 86.7 ms
               was achieved, with a 10.0\% Midas-touch rate.",
  series    = "ETRA '18",
  month     =  "14~" # jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3204493.3204532",
  doi       = "10.1145/3204493.3204532",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Barz2018-wq,
  title     = "{Error-aware gaze-based interfaces for robust mobile gaze
               interaction}",
  author    = "Barz, Michael and Daiber, Florian and Sonntag, Daniel and
               Bulling, Andreas",
  booktitle = "{Proceedings of the 2018 ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Gaze estimation error can severely hamper usability and
               performance of mobile gaze-based interfaces given that the error
               varies constantly for different interaction positions. In this
               work, we explore error-aware gaze-based interfaces that estimate
               and adapt to gaze estimation error on-the-fly. We implement a
               sample error-aware user interface for gaze-based selection and
               different error compensation methods: a naive approach that
               increases component size directly proportional to the absolute
               error, a recent model by Feit et al. that is based on the
               two-dimensional error distribution, and a novel predictive model
               that shifts gaze by a directional error estimate. We evaluate
               these models in a 12-participant user study and show that our
               predictive model significantly outperforms the others in terms of
               selection rate, particularly for small gaze targets. These
               results underline both the feasibility and potential of next
               generation error-aware gaze-based user interfaces.",
  month     =  "14~" # jun,
  year      =  2018,
  url       = "https://www.semanticscholar.org/paper/29bd6bd0565391db118440181f9402699adcfc10",
  file      = "All Papers/Other/Barz et al. 2018 - Error-aware gaze-based interfaces for robust mobile gaze interaction.pdf",
  doi       = "10.1145/3204493.3204536",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Rajanna2018-mg,
  title     = "{Gaze typing in virtual reality}",
  author    = "Rajanna, Vijay and Hansen, John Paulin",
  booktitle = "{Proceedings of the 2018 ACM symposium on eye tracking research
               \& applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1–10",
  abstract  = "Gaze tracking in virtual reality (VR) allows for hands-free text
               entry, but it has not yet been explored. We investigate how the
               keyboard design, selection method, and motion in the field of
               view may impact typing performance and user experience. We
               present two studies of people (n = 32) typing with gaze+dwell and
               gaze+click inputs in VR. In study 1, the typing keyboard was flat
               and within-view; in study 2, it was larger-than-view but curved.
               Both studies included a stationary and a dynamic motion
               conditions in the user's field of view.Our findings suggest that
               1) gaze typing in VR is viable but constrained, 2) the users
               perform best (10.15 WPM) when the entire keyboard is within-view;
               the larger-than-view keyboard (9.15 WPM) induces physical strain
               due to increased head movements, 3) motion in the field of view
               impacts the user's performance: users perform better while
               stationary than when in motion, and 4) gaze+click is better than
               dwell only (fixed at 550 ms) interaction.",
  series    = "ETRA '18",
  month     =  "14~" # jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3204493.3204541",
  doi       = "10.1145/3204493.3204541",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Akkil2018-un,
  title     = "{{I} see what you see: gaze awareness in mobile video
               collaboration}",
  author    = "Akkil, Deepak and Thankachan, Biju and Isokoski, Poika",
  booktitle = "{Proceedings of the 2018 {ACM} Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--9",
  abstract  = "An emerging use of mobile video telephony is to enable joint
               activities and collaboration on physical tasks. We conducted a
               controlled user study to understand if seeing the gaze of a
               remote instructor is beneficial for mobile video collaboration
               and if it is valuable that the instructor is aware of sharing of
               the gaze. We compared three gaze sharing configurations, (a)
               Gaze\_Visible where the instructor is aware and can view own gaze
               point that is being shared, (b) Gaze\_Invisible where the
               instructor is aware of the shared gaze but cannot view her own
               gaze point and (c) Gaze\_Unaware where the instructor is unaware
               about the gaze sharing, with a baseline of shared-mouse pointer.
               Our results suggests that naturally occurring gaze may not be as
               useful as explicitly produced eye movements. Further, instructors
               prefer using mouse rather than gaze for remote gesturing, while
               the workers also find value in transferring the gaze information.",
  series    = "ETRA '18",
  month     =  jun,
  year      =  2018,
  url       = "http://dx.doi.org/10.1145/3204493.3204542",
  keywords  = "physical task, explicit, gaze awareness, implicit, mobile phone,
               collaboration, video, video conferencing, communication",
  doi       = "10.1145/3204493.3204542",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Akkil2018-oc,
  title     = "{I see what you see: gaze awareness in mobile video
               collaboration}",
  author    = "Akkil, Deepak and Thankachan, Biju and Isokoski, Poika",
  booktitle = "{Proceedings of the 2018 ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "An emerging use of mobile video telephony is to enable joint
               activities and collaboration on physical tasks. We conducted a
               controlled user study to understand if seeing the gaze of a
               remote instructor is beneficial for mobile video collaboration
               and if it is valuable that the instructor is aware of sharing of
               the gaze. We compared three gaze sharing configurations, (a)
               Gaze\_Visible where the instructor is aware and can view own gaze
               point that is being shared, (b) Gaze\_Invisible where the
               instructor is aware of the shared gaze but cannot view her own
               gaze point and (c) Gaze\_Unaware where the instructor is unaware
               about the gaze sharing, with a baseline of shared-mouse pointer.
               Our results suggests that naturally occurring gaze may not be as
               useful as explicitly produced eye movements. Further, instructors
               prefer using mouse rather than gaze for remote gesturing, while
               the workers also find value in transferring the gaze information.",
  series    = "ETRA '18",
  month     =  jun,
  year      =  2018,
  url       = "http://dx.doi.org/10.1145/3204493.3204542",
  doi       = "10.1145/3204493.3204542",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Silva2018-ou,
  title     = "{Leveraging eye-gaze and time-series features to predict user
               interests and build a recommendation model for visual analysis}",
  author    = "Silva, Nelson and Schreck, Tobias and Veas, Eduardo and Sabol,
               Vedran and Eggeling, Eva and Fellner, Dieter W",
  booktitle = "{Proceedings of the 2018 ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "ACM",
  address   = "Warsaw Poland",
  pages     = "1--9",
  month     =  "14~" # jun,
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3204493.3204546",
  file      = "All Papers/My Library/Silva et al. 2018 - Leveraging eye-gaze and time-series features to predict user interests and build a recommendation model for visual analysis.pdf",
  doi       = "10.1145/3204493.3204546",
  isbn      =  9781450357067,
  language  = "en"
}

@INPROCEEDINGS{Papoutsaki2018-pn,
  title     = "{The eye of the typer: a benchmark and analysis of gaze behavior
               during typing}",
  author    = "Papoutsaki, Alexandra and Gokaslan, Aaron and Tompkin, James and
               He, Yuze and Huang, Jeff",
  booktitle = "{Proceedings of the 2018 ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "We examine the relationship between eye gaze and typing, focusing
               on the differences between touch and non-touch typists. To enable
               typing-based research, we created a 51-participant benchmark
               dataset for user input across multiple tasks, including user
               input data, screen recordings, webcam video of the participant's
               face, and eye tracking positions. There are patterns of eye
               movements that differ between the two types of typists,
               representing glances at the keyboard, which can be used to
               identify touch-.typed strokes with 92\% accuracy. Then, we relate
               eye gaze with cursor activity, aligning both pointing and typing
               to eye gaze. One demonstrative application of the work is in
               extending WebGazer, a real-time web-browser-based webcam eye
               tracker. We show that incorporating typing behavior as a
               secondary signal improves eye tracking accuracy by 16\% for touch
               typists, and 8\% for non-touch typists.",
  series    = "ETRA '18",
  month     =  "14~" # jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3204493.3204552",
  file      = "All Papers/My Library/Papoutsaki et al. 2018 - The eye of the typer - a benchmark and analysis of gaze behavior during typing.pdf",
  doi       = "10.1145/3204493.3204552",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Wang2018-vg,
  title     = "{SLAM-based localization of 3D gaze using a mobile eye tracker}",
  author    = "Wang, Haofei and Pi, Jimin and Qin, Tong and Shen, Shaojie and
               Shi, Bertram E",
  booktitle = "{Proceedings of the 2018 ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–5",
  abstract  = "Past work in eye tracking has focused on estimating gaze targets
               in two dimensions (2D), e.g. on a computer screen or scene camera
               image. Three-dimensional (3D) gaze estimates would be extremely
               useful when humans are mobile and interacting with the real 3D
               environment. We describe a system for estimating the 3D locations
               of gaze using a mobile eye tracker. The system integrates
               estimates of the user's gaze vector from a mobile eye tracker,
               estimates of the eye tracker pose from a visual-inertial
               simultaneous localization and mapping (SLAM) algorithm, a 3D
               point cloud map of the environment from a RGB-D sensor.
               Experimental results indicate that our system produces accurate
               estimates of 3D gaze over a much larger range than remote eye
               trackers. Our system will enable applications, such as the
               analysis of 3D human attention and more anticipative human robot
               interfaces.",
  series    = "ETRA '18",
  month     =  "14~" # jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3204493.3204584",
  doi       = "10.1145/3204493.3204584",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Gomez2018-ad,
  title     = "{Smooth-i: smart re-calibration using smooth pursuit eye
               movements}",
  author    = "Gomez, Argenis Ramirez and Gellersen, Hans",
  booktitle = "{Proceedings of the 2018 ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–5",
  abstract  = "Eye gaze for interaction is dependent on calibration. However,
               gaze calibration can deteriorate over time affecting the
               usability of the system. We propose to use motion matching of
               smooth pursuit eye movements and known motion on the display to
               determine when there is a drift in accuracy and use it as input
               for re-calibration. To explore this idea we developed Smooth-i,
               an algorithm that stores calibration points and updates them
               incrementally when inaccuracies are identified. To validate the
               accuracy of Smooth-i, we conducted a study with five participants
               and a remote eye tracker. A baseline calibration profile was used
               by all participants to test the accuracy of the Smooth-i
               re-calibration following interaction with moving targets. Results
               show that Smooth-i is able to manage re-calibration efficiently,
               updating the calibration profile only when inaccurate data
               samples are detected.",
  series    = "ETRA '18",
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3204493.3204585",
  doi       = "10.1145/3204493.3204585",
  isbn      =  9781450357067
}

@INPROCEEDINGS{Morimoto2018-ta,
  title     = "{Context switching eye typing using dynamic expanding targets}",
  author    = "Morimoto, Carlos H and Leyva, Jose A T and Diaz-Tula, Antonio",
  booktitle = "{Proceedings of the workshop on communication by gaze
               interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "Text entry by gazing on a virtual keyboard (also known as eye
               typing) is an important component of any gaze communication
               system. One of the main challenges for efficient communication is
               how to avoid unintended key selections due to the Midas' touch
               problem. The most common selection technique by gaze is dwelling.
               Though easy to learn, long dwell-times slows down the
               communication, and short dwells are prone to error. Context
               switching (CS) is a faster and more comfortable alternative, but
               the duplication of contexts takes a lot of screen space. In this
               paper we introduce two new CS designs using dynamic expanding
               targets that are more appropriate when a reduced interaction
               window is required. We compare the performance of the two new
               designs with the original CS design using QWERTY layouts as
               contexts. Our results with 6 participants typing with the 3
               keyboards show that the use of smaller size layouts with dynamic
               expanding targets are as accurate and comfortable as the larger
               QWERTY layout, though providing lower typing speeds.",
  series    = "COGAIN '18",
  month     =  "15~" # jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3206343.3206347",
  doi       = "10.1145/3206343.3206347",
  isbn      =  9781450357906
}

@INPROCEEDINGS{He2018-fs,
  title     = "{Real-Time Eye-Gaze Based Interaction for Human Intention
               Prediction and Emotion Analysis}",
  author    = "He, Hao and She, Yingying and Xiahou, Jianbing and Yao, Junfeng
               and Li, Jun and Hong, Qingqi and Ji, Yingxuan",
  booktitle = "{Proceedings of Computer Graphics International 2018}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "185–194",
  abstract  = "The human eye's state of motion and content of interest can
               express people's cognitive status and emotional status based on
               their situation. When observing the surrounding things, the human
               eyes make different eye movements according to the observed
               objects which reflects human's attention and interest. In this
               paper, we capture and analyze patterns of human eye-gaze behavior
               and head motion and classify them into different categories.
               Besides, we compute and train the eye-object movement attention
               model and eye-object feature preference model based on different
               peoples' eye-gaze behaviors by using machine learning algorithms.
               These models are used to predict humans' object of interest and
               the interaction intention according to people's real-time
               situation. Furthermore, the eye-gaze behavior and head motion
               patterns can be used as a modality of non-verbal information in
               the computing of human emotional states based on the PAD
               affective computing model. Our methodology analyzes human emotion
               and cognition status from the aspect of eye-gaze behavior and
               head motion, understands the cognitive information that human
               eyes can express, and effectively improves the efficiency of
               human-computer interaction in different circumstances.",
  series    = "CGI 2018",
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3208159.3208180",
  doi       = "10.1145/3208159.3208180",
  isbn      =  9781450364010
}

@INPROCEEDINGS{Teyssier2018-ez,
  title     = "{MobiLimb}",
  author    = "Teyssier, Marc and Bailly, Gilles and Pelachaud, Catherine and
               Lecolinet, Eric",
  booktitle = "{Proceedings of the 31st annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3242587.3242626",
  file      = "All Papers/My Library/Teyssier et al. 2018 - MobiLimb.pdf",
  doi       = "10.1145/3242587.3242626"
}

@INPROCEEDINGS{Aslan2018-zy,
  title     = "{Gazeover -- Exploring the UX of Gaze-triggered Affordance
               Communication for GUI Elements}",
  author    = "Aslan, Ilhan and Dietz, Michael and André, Elisabeth",
  booktitle = "{Proceedings of the 20th ACM International Conference on
               Multimodal Interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "253–257",
  abstract  = "The user experience (UX) of graphical user interfaces (GUIs)
               often depends on how clearly visual designs communicate/signify
               ``affordances'', such as if an element on the screen can be
               pushed, dragged, or rotated. Especially for novice users figuring
               out the complexity of a new interface can be cumbersome. In the
               ``past'' era of mouse-based interaction mouseover effects were
               successfully utilized to trigger a variety of assistance, and
               help users in exploring interface elements without causing
               unintended interactions and associated negative experiences.
               Today's GUIs are increasingly designed for touch and lack a
               method similiar to mouseover to help (novice) users to get
               acquainted with interface elements. In order to address this
               issue, we have studied gazeover, as a technique for triggering
               ``help or guidance'' when a user's gaze is over an interactive
               element, which we believe is suitable for today's touch
               interfaces. We report on a user study comparing pragmatic and
               hedonic qualities of gazeover and mouseover, which showed
               significant higher ratings in hedonic quality for the gazeover
               technique. We conclude by discussing limitations and implications
               of our findings.",
  series    = "ICMI '18",
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3242969.3242987",
  file      = "All Papers/My Library/Aslan et al. 2018 - Gazeover - Exploring the UX of Gaze-triggered Affordance Communication for GUI Elements.pdf",
  doi       = "10.1145/3242969.3242987",
  isbn      =  9781450356923
}

@INPROCEEDINGS{Ishibashi2018-xn,
  title     = "{Gaze-guided image classification for reflecting perceptual class
               ambiguity}",
  author    = "Ishibashi, Tatsuya and Sugano, Yusuke and Matsushita, Yasuyuki",
  booktitle = "{The 31st annual ACM symposium on user interface software and
               technology adjunct proceedings}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "26–28",
  abstract  = "Despite advances in machine learning and deep neural networks,
               there is still a huge gap between machine and human image
               understanding. One of the causes is the annotation process used
               to label training images. In most image categorization tasks,
               there is a fundamental ambiguity between some image categories
               and the underlying class probability differs from very obvious
               cases to ambiguous ones. However, current machine learning
               systems and applications usually work with discrete annotation
               processes and the training labels do not reflect this ambiguity.
               To address this issue, we propose an new image annotation
               framework where labeling incorporates human gaze behavior. In
               this framework, gaze behavior is used to predict image labeling
               difficulty. The image classifier is then trained with sample
               weights defined by the predicted difficulty. We demonstrate our
               approach's effectiveness on four-class image classification
               tasks.",
  series    = "UIST '18 adjunct",
  month     =  "10~" # nov,
  year      =  2018,
  url       = "https://doi.org/10.1145/3266037.3266090",
  file      = "All Papers/My Library/Ishibashi et al. 2018 - Gaze-guided image classification for reflecting perceptual class ambiguity.pdf",
  doi       = "10.1145/3266037.3266090",
  isbn      =  9781450359498
}

@INPROCEEDINGS{Hashizume2018-me,
  title     = "{Trans-scale playground}",
  author    = "Hashizume, Satoshi and Ishii, Akira and Suzuki, Kenta and
               Takazawa, Kazuki and Ochiai, Yoichi",
  booktitle = "{The 31st annual ACM symposium on user interface software and
               technology adjunct proceedings}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "10~" # nov,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3266037.3266103",
  doi       = "10.1145/3266037.3266103",
  isbn      =  9781450359498
}

@INPROCEEDINGS{Bhattacharya2018-mw,
  title     = "{Monitoring daily activities of multiple sclerosis patients with
               connected health devices}",
  author    = "Bhattacharya, Sourav and Ramos, Alberto Gil C P and Kawsar, Fahim
               and Lane, Nicholas D and Gionta, Lynn M and Manidis, Joanne and
               Silvesti, Greg and Vegreville, Mathieu",
  booktitle = "{Proceedings of the 2018 ACM international joint conference and
               2018 international symposium on pervasive and ubiquitous
               computing and wearable computers}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "666–669",
  abstract  = "We report results from a pilot study that focuses mainly on
               understanding the everyday life quality of patients suffering
               from multiple sclerosis through the lens of connected Nokia
               Health devices. Our dataset comprises of 198 individuals (184
               females and 14 males) and the study lasted over six months. By
               analyzing carefully crafted user-studies and correlating with
               personal sensor data collected with Nokia devices, we found that
               the level of fatigue is one of the main sources of discomfort
               across the majority of the patients. We further perform an
               exploratory analysis, which provides an early indication that by
               actively monitoring and perturbing users' daily activity levels,
               such as increasing daily step-counts, sleep duration and
               decreasing body weight, patients can potentially reduce their
               daily fatigue level.",
  series    = "UbiComp '18",
  month     =  "10~" # aug,
  year      =  2018,
  url       = "https://doi.org/10.1145/3267305.3267682",
  doi       = "10.1145/3267305.3267682",
  isbn      =  9781450359665
}

@INPROCEEDINGS{Sefidgar2018-lo,
  title     = "{RobotIST}",
  author    = "Sefidgar, Yasaman S and Weng, Thomas and Harvey, Heather and
               Elliott, Sarah and Cakmak, Maya",
  booktitle = "{Proceedings of the symposium on spatial user interaction}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "13~" # oct,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3267782.3267921",
  doi       = "10.1145/3267782.3267921",
  isbn      =  9781450357081
}

@ARTICLE{Yang2018-ur,
  title    = "{Shopping over distance through a telepresence robot}",
  author   = "Yang, Lillian and Jones, Brennan and Neustaedter, Carman and
              Singhal, Samarth",
  journal  = "Proceedings of the ACM on human-computer interaction",
  volume   =  2,
  number   = "CSCW",
  pages    = "1–18",
  abstract = "Computer mediated-communication tools (CMC) support loved ones in
              maintaining connections with one another over distance, yet it can
              be difficult to do activities together. We studied the use of
              telepresence robots for supporting distance-separated loved ones
              in engaging in the joint activity of shopping over distance. One
              partner shopped in person while the other used either a
              telepresence robot or a tablet from a remote location. Compared to
              the tablet group, we found that when partners communicated through
              a telepresence robot, the remote partner's personality and
              presence were expressed through the movements and physicality of
              the medium. However, the use of the telepresence robot introduced
              tension between partners regarding responsibility, dependency, and
              contribution to the act of shopping. These results demonstrate the
              benefits of a mobile embodiment for remote partners, as well as
              the need for greater physical capabilities to support both
              physical connection and remote contribution to leisure activities.",
  month    =  nov,
  year     =  2018,
  url      = "https://doi.org/10.1145%2F3274460",
  doi      = "10.1145/3274460",
  issn     = "2573-0142",
  language = "en"
}

@ARTICLE{Harrison2018-et,
  title    = "{The HCI innovator's dilemma}",
  author   = "Harrison, Chris",
  journal  = "Interactions",
  volume   =  25,
  number   =  6,
  pages    = "26--33",
  month    =  "25~" # oct,
  year     =  2018,
  url      = "https://dl.acm.org/doi/10.1145/3274564",
  doi      = "10.1145/3274564",
  issn     = "1072-5520,1558-3449",
  language = "en"
}

@INPROCEEDINGS{Matsuda2018-js,
  title     = "{JackIn neck}",
  author    = "Matsuda, Akira and Nozawa, Kazunori and Rekimoto, Jun",
  booktitle = "{Proceedings of the 2018 ACM international conference on
               interactive surfaces and spaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "19~" # nov,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3279778.3279917",
  doi       = "10.1145/3279778.3279917",
  isbn      =  9781450356947
}

@INPROCEEDINGS{Zhang2018-im,
  title     = "{EXController}",
  author    = "Zhang, Junjian and Chen, Yaohao and Hashizume, Satoshi and
               Muramatsu, Naoya and Omomo, Kotaro and Iwasaki, Riku and Wataru,
               Kaji and Ochiai, Yoichi",
  booktitle = "{Proceedings of the 24th ACM symposium on virtual reality
               software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "28~" # nov,
  year      =  2018,
  url       = "https://doi.org/10.1145%2F3281505.3283385",
  doi       = "10.1145/3281505.3283385",
  isbn      =  9781450360869
}

@INPROCEEDINGS{Nakazawa2018-ac,
  title     = "{Development of communication aid device for disabled persons
               using corneal surface reflection image}",
  author    = "Nakazawa, Nobuaki and Aikawa, Shinnosuke and Matsui, Toshikazu",
  booktitle = "{Proceedings of the 2nd international conference on graphics and
               signal processing}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "16–20",
  abstract  = "Improving quality of life (QOL) has become an important issue for
               patients with neurological diseases such as ALS (amyotrophic
               lateral sclerosis) and SMA (spinal atrophy). At the same time, it
               is indispensable to secure a communication device that displays
               its own intention, and various interfaces have been proposed. In
               this research, we developed a glasses-type switching wearable
               device focusing on eyeball movement. Here, images around the
               eyeball were obtained by USB-camera equipped with infrared LEDs,
               and pupils were extracted by Hough transform procedure.
               Furthermore, in order to apply a communication aid operation, a
               detection model of the line-of-sight direction was constructed by
               acquiring the coordinates of the cornea surface reflection image.",
  series    = "ICGSP'18",
  month     =  "10~" # jun,
  year      =  2018,
  url       = "https://doi.org/10.1145/3282286.3282298",
  doi       = "10.1145/3282286.3282298",
  isbn      =  9781450363860
}

@INPROCEEDINGS{Baschieri2018-rs,
  title     = "{A planning-based serious game for cognitive rehabilitation in
               multiple sclerosis}",
  author    = "Baschieri, Daniele and Gaspari, Mauro and Zini, Floriano",
  booktitle = "{Proceedings of the 4th EAI international conference on smart
               objects and technologies for social good}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "214–219",
  abstract  = "Many patients suffering from multiple sclerosis encounter
               cognitive impairment. The use of computerized serious games could
               play an essential role in their cognitive rehabilitation,
               especially when these games are designed to train the patient
               with intellectual disability on tasks typically required in their
               daily activities. In this paper, we present a serious game for
               the rehabilitation of executive functions, high-level cognitive
               processes that people activate to achieve their everyday goals.
               The game consists in executing, via a dedicated graphical
               interface, the actions that are necessary to complete a set of
               common tasks. Automated planning techniques are used to generate
               a potentially unlimited number of game scenarios with
               parametrized difficulty, and to verify if the patient's game
               strategy is leading to the correct completion of the tasks. An
               evaluation of the game usability is also presented.",
  series    = "Goodtechs '18",
  month     =  "28~" # nov,
  year      =  2018,
  url       = "https://doi.org/10.1145/3284869.3284916",
  doi       = "10.1145/3284869.3284916",
  isbn      =  9781450365819
}

@ARTICLE{Srivastava2018-wt,
  title    = "{Combining Low and Mid-Level Gaze Features for Desktop Activity
              Recognition}",
  author   = "Srivastava, Namrata and Newn, Joshua and Velloso, Eduardo",
  journal  = "Proceedings of the ACM on Interactive, Mobile, Wearable and
              Ubiquitous Technologies",
  volume   =  2,
  number   =  4,
  pages    = "189:1–189:27",
  abstract = "Human activity recognition (HAR) is an important research area due
              to its potential for building context-aware interactive systems.
              Though movement-based activity recognition is an established area
              of research, recognising sedentary activities remains an open
              research question. Previous works have explored eye-based activity
              recognition as a potential approach for this challenge, focusing
              on statistical measures derived from eye movement
              properties---low-level gaze features---or some knowledge of the
              Areas-of-Interest (AOI) of the stimulus---high-level gaze
              features. In this paper, we extend this body of work by employing
              the addition of mid-level gaze features; features that add a level
              of abstraction over low-level features with some knowledge of the
              activity, but not of the stimulus. We evaluated our approach on a
              dataset collected from 24 participants performing eight desktop
              computing activities. We trained a classifier extending 26
              low-level features derived from existing literature with the
              addition of 24 novel candidate mid-level gaze features. Our
              results show an overall classification performance of 0.72
              (F1-Score), with up to 4\% increase in accuracy when adding our
              mid-level gaze features. Finally, we discuss the implications of
              combining low- and mid-level gaze features, as well as the future
              directions for eye-based activity recognition.",
  year     =  2018,
  url      = "https://dl.acm.org/doi/10.1145/3287067",
  doi      = "10.1145/3287067"
}

@INPROCEEDINGS{Kim2019-dd,
  title     = "{Comparing Data from Chatbot and Web Surveys: Effects of Platform
               and Conversational Style on Survey Response Quality}",
  author    = "Kim, Soomin and Lee, Joonhwan and Gweon, Gahgene",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "This study aims to explore the feasibility of a text-based
               virtual agent as a new survey method to overcome the web survey's
               common response quality problems, which are caused by
               respondents' inattention. To this end, we conducted a 2
               (platform: web vs. chatbot) × 2 (conversational style: formal vs.
               casual) experiment. We used satisficing theory to compare the
               responses' data quality. We found that the participants in the
               chatbot survey, as compared to those in the web survey, were more
               likely to produce differentiated responses and were less likely
               to satisfice; the chatbot survey thus resulted in higher-quality
               data. Moreover, when a casual conversational style is used, the
               participants were less likely to satisfice-although such effects
               were only found in the chatbot condition. These results imply
               that conversational interactivity occurs when a chat interface is
               accompanied by messages with effective tone. Based on an analysis
               of the qualitative responses, we also showed that a chatbot could
               perform part of a human interviewer's role by applying effective
               communication strategies.",
  series    = "CHI '19",
  month     =  "2~" # may,
  year      =  2019,
  url       = "https://doi.org/10.1145/3290605.3300316",
  doi       = "10.1145/3290605.3300316",
  isbn      =  9781450359702
}

@INPROCEEDINGS{Sindhwani2019-hf,
  title     = "{ReType: Quick Text Editing with Keyboard and Gaze}",
  author    = "Sindhwani, Shyamli and Lutteroth, Christof and Weber, Gerald",
  booktitle = "{Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "When a user needs to reposition the cursor during text editing,
               this is often done using the mouse. For experienced typists
               especially, the switch between keyboard and mouse can slow down
               the keyboard editing workflow considerably. To address this we
               propose ReType, a new gaze-assisted positioning technique
               combining keyboard with gaze input based on a new 'patching'
               metaphor. ReType allows users to perform some common editing
               operations while keeping their hands on the keyboard. We present
               the result of two studies. A free-use study indicated that ReType
               enhances the user experience of text editing. ReType was liked by
               many participants, regardless of their typing skills. A
               comparative user study showed that ReType is able to match or
               even beat the speed of mouse-based interaction for small text
               edits. We conclude that the gaze-augmented user interface can
               make common interactions more fluent, especially for professional
               keyboard users.",
  series    = "CHI '19",
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3290605.3300433",
  file      = "All Papers/My Library/Sindhwani et al. 2019 - ReType - Quick Text Editing with Keyboard and Gaze.pdf",
  doi       = "10.1145/3290605.3300433",
  isbn      =  9781450359702
}

@INPROCEEDINGS{Vatavu2019-oy,
  title     = "{Stroke-gesture input for people with motor impairments:
               Empirical results \& research roadmap}",
  author    = "Vatavu, Radu-Daniel and Ungurean, Ovidiu-Ciprian",
  booktitle = "{Proceedings of the 2019 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–14",
  abstract  = "We examine the articulation characteristics of stroke-gestures
               produced by people with upper body motor impairments on
               touchscreens as well as the accuracy rates of popular
               classification techniques, such as the \$-family, to recognize
               those gestures. Our results on a dataset of 9,681 gestures
               collected from 70 participants reveal that stroke-gestures
               produced by people with motor impairments are recognized less
               accurately than the same gesture types produced by people without
               impairments, yet still accurately enough (93.0\%) for practical
               purposes; are similar in terms of geometrical criteria to the
               gestures produced by people without impairments; but take
               considerably more time to produce (3.4s vs. 1.7s) and exhibit
               lower consistency (-49.7\%). We outline a research roadmap for
               accessible gesture input on touchscreens for users with upper
               body motor impairments, and we make our large gesture dataset
               publicly available in the community.",
  series    = "CHI '19",
  month     =  "5~" # feb,
  year      =  2019,
  url       = "https://doi.org/10.1145/3290605.3300445",
  doi       = "10.1145/3290605.3300445",
  isbn      =  9781450359702
}

@INPROCEEDINGS{Kwok2019-ou,
  title     = "{Gaze-guided narratives: Adapting audio guide content to gaze in
               virtual and real environments}",
  author    = "Kwok, Tiffany C K and Kiefer, Peter and Schinazi, Victor R and
               Adams, Benjamin and Raubal, Martin",
  booktitle = "{Proceedings of the 2019 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "Exploring a city panorama from a vantage point is a popular
               tourist activity. Typical audio guides that support this activity
               are limited by their lack of responsiveness to user behavior and
               by the difficulty of matching audio descriptions to the panorama.
               These limitations can inhibit the acquisition of information and
               negatively affect user experience. This paper proposes
               Gaze-Guided Narratives as a novel interaction concept that helps
               tourists find specific features in the panorama (gaze guidance)
               while adapting the audio content to what has been previously
               looked at (content adaptation). Results from a controlled study
               in a virtual environment (n=60) revealed that a system featuring
               both gaze guidance and content adaptation obtained better user
               experience, lower cognitive load, and led to better performance
               in a mapping task compared to a classic audio guide. A second
               study with tourists situated at a vantage point (n=16) further
               demonstrated the feasibility of this approach in the real world.",
  series    = "CHI '19",
  month     =  "5~" # feb,
  year      =  2019,
  url       = "https://doi.org/10.1145/3290605.3300721",
  file      = "All Papers/My Library/Kwok et al. 2019 - Gaze-guided narratives - Adapting audio guide content to gaze in virtual and real environments.pdf",
  doi       = "10.1145/3290605.3300721",
  isbn      =  9781450359702,
  annote    = "<p xmlns=``http://www.w3.org/1999/xhtml''
               id=``title''><strong>Contents</strong></p><ul
               xmlns=``http://www.w3.org/1999/xhtml'' style=``list-style-type:
               none; padding-left:0px'' id=``toc''><li><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/1''>Abstract</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/2''>1
               Introduction</a></li><li style=``padding-top:8px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/2''>2 Related Work</a><ul
               style=``list-style-type: none; padding-left:12px''><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/2''>Tourist
               Guides</a></li><li style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/2''>Pervasive Eye Tracking
               and Eye-Based Interaction</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/3''>Gaze
               Guidance</a></li><li style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/3''>Interactive Narratives
               and Spatial Narratives</a></li></ul></li><li
               style=``padding-top:8px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/3''>3 Gaze-Guided
               Narratives</a><ul style=``list-style-type: none;
               padding-left:12px''><li style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/3''>Concept</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/3''>Implementation</a></li></ul></li><li
               style=``padding-top:8px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/4''>4 Study 1: Controlled
               Lab Study</a><ul style=``list-style-type: none;
               padding-left:12px''><li style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/4''>Methodology</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/6''>Results</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/7''>Discussion</a></li></ul></li><li
               style=``padding-top:8px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/8''>5 Study 2: Real World
               Study</a><ul style=``list-style-type: none;
               padding-left:12px''><li style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/8''>Methodology</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/9''>Results</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/10''>Discussion</a></li></ul></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/10''>6 Conclusion and
               Outlook</a></li><li style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/10''>Acknowledgments</a></li><li
               style=``padding-top:4px''><a
               href=``zotero://open-pdf/0\_Y8T4PYEN/11''>References</a></li></ul>"
}

@INPROCEEDINGS{Radu2019-fy,
  title     = "{What can we learn from augmented reality (AR)?}",
  author    = "Radu, Iulian and Schneider, Bertrand",
  booktitle = "{Proceedings of the 2019 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # feb,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3290605.3300774",
  file      = "All Papers/My Library/Radu and Schneider 2019 - What can we learn from augmented reality (AR).pdf",
  doi       = "10.1145/3290605.3300774",
  isbn      =  9781450359702
}

@INPROCEEDINGS{Hirzle2019-sa,
  title     = "{A Design Space for Gaze Interaction on Head-mounted Displays}",
  author    = "Hirzle, Teresa and Gugenheimer, Jan and Geiselhart, Florian and
               Bulling, Andreas and Rukzio, Enrico",
  booktitle = "{Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "Augmented and virtual reality (AR/VR) has entered the mass market
               and, with it, will soon eye tracking as a core technology for
               next generation head-mounted displays (HMDs). In contrast to
               existing gaze interfaces, the 3D nature of AR and VR requires
               estimating a user's gaze in 3D. While first applications, such as
               foveated rendering, hint at the compelling potential of combining
               HMDs and gaze, a systematic analysis is missing. To fill this
               gap, we present the first design space for gaze interaction on
               HMDs. Our design space covers human depth perception and
               technical requirements in two dimensions aiming to identify
               challenges and opportunities for interaction design. As such, our
               design space provides a comprehensive overview and serves as an
               important guideline for researchers and practitioners working on
               gaze interaction on HMDs. We further demonstrate how our design
               space is used in practice by presenting two interactive
               applications: EyeHealth and XRay-Vision.",
  series    = "CHI '19",
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3290605.3300855",
  doi       = "10.1145/3290605.3300855",
  isbn      =  9781450359702
}

@INPROCEEDINGS{Kim2019-bx,
  title     = "{SwarmHaptics}",
  author    = "Kim, Lawrence H and Follmer, Sean",
  booktitle = "{Proceedings of the 2019 CHI conference on human factors in
               computing systems - CHI '19}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3290605.3300918",
  doi       = "10.1145/3290605.3300918",
  isbn      =  9781450359702
}

@INPROCEEDINGS{Cave2019-xq,
  title     = "{PALS: Patching ALS through crowdsourced advice, social links \&
               public awareness}",
  author    = "Cave, Richard and Kocemba, Karina and Dajic, Svjetlana and
               Bostock, Holloe and Cook, Alastair",
  booktitle = "{Extended abstracts of the 2019 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–6",
  abstract  = "Amyotrophic Lateral Sclerosis (ALS) is a serious and poorly
               understood disease, impacting 50,000 people a year globally. Our
               research found that people with ALS express a lack of connection
               with other people with the disease, and that the general public
               lacks awareness about ALS. We also identified an engagement
               problem with the currently available resources to connect and
               support people with ALS. To address these issues, we introduce
               'PALS' - an accessible crowdsourcing and connection quilt, first
               hung like a tapestry in the ALS clinic, then later used as an
               interactive public display. The quilt offers the opportunity to
               access crowdsourced information concerning individual experiences
               of ALS. Our work offers three primary contributions: 1) adding to
               limited HCI research concerning the ALS community by establishing
               the needs, 2) applying the 'PALS' quilt design solution to these
               needs, and 3) combining three modalities: crowdsourcing, tangible
               tapestry displays, and interactive waiting education in a unique
               way.",
  series    = "CHI EA '19",
  month     =  "5~" # feb,
  year      =  2019,
  url       = "https://doi.org/10.1145/3290607.3309690",
  doi       = "10.1145/3290607.3309690",
  isbn      =  9781450359719
}

@INPROCEEDINGS{He2019-en,
  title     = "{Exploring configuration of mixed reality spaces for
               communication}",
  author    = "He, Zhenyi and Rosenberg, Karl Toby and Perlin, Ken",
  booktitle = "{Extended Abstracts of the 2019 {CHI} Conference on Human Factors
               in Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This work designed and implemented side-by-side, mirrored
               face-to-face and eyes-free configurations in a multi-user MR
               environment, and conducted a preliminary user study for the
               authors' mirrored face to-face configuration. Mixed Reality (MR)
               enables users to explore scenarios not realizable in the physical
               world. This allows users to communicate with the help of digital
               content. We investigate how different configurations of
               participants and content affect communication in a shared
               immersive environment. We designed and implemented side-by-side,
               mirrored face-to-face and eyes-free configurations in our
               multi-user MR environment and conducted a preliminary user study
               for our mirrored face-to-face configuration, evaluating with
               respect to one-to-one interaction, smooth focus shifts and eye
               contact within a 3D presentation using the interactive Chalktalk
               system. We provide experimental results and interview responses.",
  month     =  may,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3290607.3312761",
  doi       = "10.1145/3290607.3312761",
  isbn      =  9781450359719,
  language  = "en"
}

@INPROCEEDINGS{He2019-gz,
  title     = "{Exploring configuration of mixed reality spaces for
               communication}",
  author    = "He, Zhenyi and Rosenberg, Karl Toby and Perlin, Ken",
  booktitle = "{Extended abstracts of the 2019 CHI conference on human factors
               in computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This work designed and implemented side-by-side, mirrored
               face-to-face and eyes-free configurations in a multi-user MR
               environment, and conducted a preliminary user study for the
               authors' mirrored face to-face configuration. Mixed Reality (MR)
               enables users to explore scenarios not realizable in the physical
               world. This allows users to communicate with the help of digital
               content. We investigate how different configurations of
               participants and content affect communication in a shared
               immersive environment. We designed and implemented side-by-side,
               mirrored face-to-face and eyes-free configurations in our
               multi-user MR environment and conducted a preliminary user study
               for our mirrored face-to-face configuration, evaluating with
               respect to one-to-one interaction, smooth focus shifts and eye
               contact within a 3D presentation using the interactive Chalktalk
               system. We provide experimental results and interview responses.",
  month     =  may,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3290607.3312761",
  doi       = "10.1145/3290607.3312761",
  isbn      =  9781450359719,
  language  = "en"
}

@INPROCEEDINGS{Cao2019-fx,
  title     = "{V.Ra}",
  author    = "Cao, Yuanzhi and Xu, Zhuangying and Li, Fan and Zhong, Wentao and
               Huo, Ke and Ramani, Karthik",
  booktitle = "{Extended abstracts of the 2019 CHI conference on human factors
               in computing systems - CHI EA '19}",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3290607.3312797",
  doi       = "10.1145/3290607.3312797",
  isbn      =  9781450359719
}

@INPROCEEDINGS{Setlur2019-ai,
  title     = "{Inferencing underspecified natural language utterances in visual
               analysis}",
  author    = "Setlur, Vidya and Tory, Melanie and Djalali, Alex",
  booktitle = "{Proceedings of the 24th International Conference on Intelligent
               User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Handling ambiguity and underspecification of users' utterances is
               challenging, particularly for natural language interfaces that
               help with visual analytical tasks. Constraints in the underlying
               analytical platform and the users' expectations of high precision
               and recall require thoughtful inferencing to help generate useful
               responses. In this paper, we introduce a system to resolve
               partial utterances based on syntactic and semantic constraints of
               the underlying analytical expressions. We extend inferencing
               based on best practices in information visualization to generate
               useful visualization responses. We employ heuristics to help
               constrain the solution space of possible inferences, and apply
               ranking logic to the interpretations based on relevancy. We
               evaluate the quality of inferred interpretations based on
               relevancy and analytical usefulness.",
  month     =  "17~" # mar,
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3301275.3302270",
  file      = "All Papers/Other/Setlur et al. 2019 - Inferencing underspecified natural language utterances in visual analysis.pdf",
  doi       = "10.1145/3301275.3302270",
  isbn      =  9781450362726
}

@INPROCEEDINGS{Dijkstra-Soudarissanane2019-qr,
  title     = "{Multi-sensor capture and network processing for virtual reality
               conferencing}",
  author    = "Dijkstra-Soudarissanane, Sylvie and Assal, Karim El and Gunkel,
               Simon and ter Haar, Frank and Hindriks, Rick and Kleinrouweler,
               Jan Willem and Niamut, Omar",
  booktitle = "{Proceedings of the 10th ACM multimedia systems conference}",
  publisher = "ACM",
  month     =  jun,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3304109.3323838",
  file      = "All Papers/My Library/Dijkstra-Soudarissanane et al. 2019 - Multi-sensor capture and network processing for virtual reality conferencing.pdf",
  doi       = "10.1145/3304109.3323838"
}

@INPROCEEDINGS{Bafna2019-dn,
  title     = "{Cognitive load during gaze typing}",
  author    = "Bafna, Tanya and Paulin, John",
  booktitle = "{Proceedings of the 24th international conference on intelligent
               user interfaces: Companion}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "83–84",
  abstract  = "Cognitive load derived from pupillary measures has been studied
               extensively in the human-computer interaction community, using
               various kinds of tasks and measures. In this paper, we present
               preliminary results on the cognitive load due to a gaze typing
               task of varying complexities. A pilot study was conducted to
               observe if a task on gaze typing sentences, of varying
               complexities, could generate different cognitive load in the
               participants. The data obtained was from pupils, blink data,
               pupil oscillation measured in form of Index of Pupillary Activity
               and subjective scores. Logistic regression was performed to
               observe the difference between easy and difficult tasks and they
               were successfully differentiated, with 75\% accuracy, using the
               measures of absolute pupil size, relative pupil size and
               subjective scores given by the participants.",
  series    = "IUI '19",
  month     =  "16~" # mar,
  year      =  2019,
  url       = "https://doi.org/10.1145/3308557.3308663",
  doi       = "10.1145/3308557.3308663",
  isbn      =  9781450366731
}

@INPROCEEDINGS{Guinness2019-vm,
  title     = "{RoboGraphics}",
  author    = "Guinness, Darren and Muehlbradt, Annika and Szafir, Daniel and
               Kane, Shaun K",
  booktitle = "{The 21st international ACM SIGACCESS conference on computers and
               accessibility}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "24~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3308561.3353804",
  file      = "All Papers/My Library/Guinness et al. 2019 - RoboGraphics.pdf",
  doi       = "10.1145/3308561.3353804",
  isbn      =  9781450366762
}

@INPROCEEDINGS{Yumura2020-fc,
  title     = "{Augmented typing: augmentation of keyboard typing experience by
               adding visual and sound effects}",
  author    = "Yumura, Tsubasa and Nakamura, Satoshi",
  booktitle = "{Proceedings of asian CHI symposium 2019: Emerging HCI research
               collection}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "53–59",
  abstract  = "People make choice of a keyboard for various features such as key
               arrangement, repulsion strength, stroke depth, and stroke sound.
               Currently these features depend on hardware; however, if a
               physical keyboard can be customized with software, users will be
               able to arrange it as their preference. In this paper, we propose
               augmented typing that augments typing experience by adding a
               visual and sound effect to a physical keyboard. We implemented a
               prototype system of the proposed system using projection mapping.
               To determine the effect and the usefulness of the system, in
               addition, we conducted evaluation experiments. Results of the
               experiments showed that the evaluation of sound effect was more
               variable than visual effect; users found that visual effects are
               beautiful, whereas the sound effects to be annoying.",
  series    = "AsianHCI '19",
  month     =  "22~" # may,
  year      =  2020,
  url       = "https://doi.org/10.1145/3309700.3338436",
  file      = "All Papers/My Library/Yumura and Nakamura 2020 - Augmented typing - augmentation of keyboard typing experience by adding visual and sound effects.pdf",
  doi       = "10.1145/3309700.3338436",
  isbn      =  9781450366793
}

@ARTICLE{Hsu2019-tc,
  title     = "{Look at Me! Correcting Eye Gaze in Live Video Communication}",
  author    = "Hsu, Chih-Fan and Wang, Yu-Shuen and Lei, Chin-Laung and Chen,
               Kuan-Ta",
  journal   = "ACM Trans. Multimedia Comput. Commun. Appl.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  15,
  number    =  2,
  pages     = "1--21",
  abstract  = "Although live video communication is widely used, it is generally
               less engaging than face-to-face communication because of
               limitations on social, emotional, and haptic feedback. Missing
               eye contact is one such problem caused by the physical deviation
               between the screen and camera on a device. Manipulating video
               frames to correct eye gaze is a solution to this problem. In this
               article, we introduce a system to rotate the eyeball of a local
               participant before the video frame is sent to the remote side. It
               adopts a warping-based convolutional neural network to relocate
               pixels in eye regions. To improve visual quality, we minimize the
               L2 distance between the ground truths and warped eyes. We also
               present several newly designed loss functions to help network
               training. These new loss functions are designed to preserve the
               shape of eye structures and minimize color changes around the
               periphery of eye regions. To evaluate the presented network and
               loss functions, we objectively and subjectively compared results
               generated by our system and the state-of-the-art, DeepWarp, in
               relation to two datasets. The experimental results demonstrated
               the effectiveness of our system. In addition, we showed that our
               system can perform eye-gaze correction in real time on a
               consumer-level laptop. Because of the quality and efficiency of
               the system, gaze correction by postprocessing through this system
               is a feasible solution to the problem of missing eye contact in
               video communication.",
  month     =  jun,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3311784",
  keywords  = "Eye contact, convolutional neural network, live video
               communication, gaze correction, image processing;eye
               contact;telepresence;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/3311784",
  issn      = "1551-6857"
}

@ARTICLE{Hsu2019-zi,
  title    = "{Look at me! Correcting eye gaze in live video communication}",
  author   = "Hsu, Chih-Fan and Wang, Yu-Shuen and Lei, Chin-Laung and Chen,
              Kuan-Ta",
  journal  = "ACM transactions on multimedia computing communications and
              applications",
  volume   =  15,
  number   =  2,
  pages    = "1–21",
  abstract = "Although live video communication is widely used, it is generally
              less engaging than face-to-face communication because of
              limitations on social, emotional, and haptic feedback. Missing eye
              contact is one such problem caused by the physical deviation
              between the screen and camera on a device. Manipulating video
              frames to correct eye gaze is a solution to this problem. In this
              article, we introduce a system to rotate the eyeball of a local
              participant before the video frame is sent to the remote side. It
              adopts a warping-based convolutional neural network to relocate
              pixels in eye regions. To improve visual quality, we minimize the
              L2 distance between the ground truths and warped eyes. We also
              present several newly designed loss functions to help network
              training. These new loss functions are designed to preserve the
              shape of eye structures and minimize color changes around the
              periphery of eye regions. To evaluate the presented network and
              loss functions, we objectively and subjectively compared results
              generated by our system and the state-of-the-art, DeepWarp, in
              relation to two datasets. The experimental results demonstrated
              the effectiveness of our system. In addition, we showed that our
              system can perform eye-gaze correction in real time on a
              consumer-level laptop. Because of the quality and efficiency of
              the system, gaze correction by postprocessing through this system
              is a feasible solution to the problem of missing eye contact in
              video communication.",
  month    =  "14~" # jun,
  year     =  2019,
  url      = "https://doi.org/10.1145%2F3311784",
  doi      = "10.1145/3311784",
  issn     = "1551-6857,1551-6865",
  language = "en"
}

@INPROCEEDINGS{Ishii2019-et,
  title     = "{Let your world open}",
  author    = "Ishii, Akira and Tsuruta, Masaya and Suzuki, Ippei and Nakamae,
               Shuta and Suzuki, Junichi and Ochiai, Yoichi",
  booktitle = "{Proceedings of the 10th augmented human international conference
               2019}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "3~" # nov,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3311823.3311860",
  doi       = "10.1145/3311823.3311860",
  isbn      =  9781450365475
}

@INCOLLECTION{Matulic2020-yj,
  title     = "{{PenSight}: Enhanced Interaction with a {Pen-Top} Camera}",
  author    = "Matulic, Fabrice and Arakawa, Riku and Vogel, Brian and Vogel,
               Daniel",
  booktitle = "{Proceedings of the 2020 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--14",
  abstract  = "We propose mounting a downward-facing camera above the top end of
               a digital tablet pen. This creates a unique and practical viewing
               angle for capturing the pen-holding hand and the immediate
               surroundings which can include the other hand. The fabrication of
               a prototype device is described and the enabled interaction
               design space is explored, including dominant and non-dominant
               hand pose recognition, tablet grip detection, hand gestures,
               capturing physical content in the environment, and detecting
               users and pens. A deep learning computer vision pipeline is
               developed for classification, regression, and keypoint detection
               to enable these interactions. Example applications demonstrate
               usage scenarios and a qualitative user evaluation confirms the
               potential of the approach.",
  month     =  apr,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3313831.3376147",
  keywords  = "world model",
  doi       = "10.1145/3313831.3376147",
  isbn      =  9781450367080
}

@INPROCEEDINGS{Matulic2020-ms,
  title     = "{PenSight: Enhanced interaction with a pen-top camera}",
  author    = "Matulic, Fabrice and Arakawa, Riku and Vogel, Brian and Vogel,
               Daniel",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "21~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3313831.3376147",
  doi       = "10.1145/3313831.3376147",
  isbn      =  9781450367080
}

@INCOLLECTION{Creed2020-me,
  title     = "{Multimodal Gaze Interaction for Creative Design}",
  author    = "Creed, Chris and Frutos-Pascual, Maite and Williams, Ian",
  booktitle = "{Proceedings of the 2020 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--13",
  abstract  = "We present a new application (``Sakura'') that enables people
               with physical impairments to produce creative visual design work
               using a multimodal gaze approach. The system integrates multiple
               features tailored for gaze interaction including the selection of
               design artefacts via a novel grid approach, control methods for
               manipulating canvas objects, creative typography, a new color
               selection approach, and a customizable guide technique
               facilitating the alignment of design elements. A user evaluation
               (N=24) found that non-disabled users were able to utilize the
               application to complete common design activities and that they
               rated the system positively in terms of usability. A follow-up
               study with physically impaired participants (N=6) demonstrated
               they were able to control the system when working towards a
               website design, rating the application as having a good level of
               usability. Our research highlights new directions in making
               creative activities more accessible for people with physical
               impairments.",
  month     =  apr,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3313831.3376196",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1145/3313831.3376196",
  isbn      =  9781450367080
}

@INPROCEEDINGS{Creed2020-ps,
  title     = "{Multimodal gaze interaction for creative design}",
  author    = "Creed, Chris and Frutos-Pascual, Maite and Williams, Ian",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "We present a new application (“Sakura”) that enables people with
               physical impairments to produce creative visual design work using
               a multimodal gaze approach. The system integrates multiple
               features tailored for gaze interaction including the selection of
               design artefacts via a novel grid approach, control methods for
               manipulating canvas objects, creative typography, a new color
               selection approach, and a customizable guide technique
               facilitating the alignment of design elements. A user evaluation
               (N=24) found that non-disabled users were able to utilize the
               application to complete common design activities and that they
               rated the system positively in terms of usability. A follow-up
               study with physically impaired participants (N=6) demonstrated
               they were able to control the system when working towards a
               website design, rating the application as having a good level of
               usability. Our research highlights new directions in making
               creative activities more accessible for people with physical
               impairments.",
  series    = "CHI '20",
  month     =  "23~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145/3313831.3376196",
  file      = "All Papers/My Library/Creed et al. 2020 - Multimodal gaze interaction for creative design.pdf",
  doi       = "10.1145/3313831.3376196",
  isbn      =  9781450367080
}

@INPROCEEDINGS{Mueller2020-jr,
  title     = "{Next steps for human-computer integration}",
  author    = "Mueller, Florian Floyd and Lopes, Pedro and Strohmeier, Paul and
               Ju, Wendy and Seim, Caitlyn and Weigel, Martin and Nanayakkara,
               Suranga and Obrist, Marianna and Li, Zhuying and Delfa, Joseph
               and Nishida, Jun and Gerber, Elizabeth M and Svanaes, Dag and
               Grudin, Jonathan and Greuter, Stefan and Kunze, Kai and Erickson,
               Thomas and Greenspan, Steven and Inami, Masahiko and Marshall,
               Joe and Reiterer, Harald and Wolf, Katrin and Meyer, Jochen and
               Schiphorst, Thecla and Wang, Dakuo and Maes, Pattie",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  apr,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3313831.3376242",
  file      = "All Papers/My Library/Mueller et al. 2020 - Next steps for human-computer integration.pdf",
  doi       = "10.1145/3313831.3376242"
}

@INPROCEEDINGS{Kumar2020-ds,
  title     = "{TAGSwipe: Touch assisted gaze swipe for text entry}",
  author    = "Kumar, Chandan and Hedeshy, Ramin and MacKenzie, I Scott and
               Staab, Steffen",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "The conventional dwell-based methods for text entry by gaze are
               typically slow and uncomfortable. A swipe-based method that maps
               gaze path into words offers an alternative. However, it requires
               the user to explicitly indicate the beginning and ending of a
               word, which is typically achieved by tedious gaze-only selection.
               This paper introduces TAGSwipe, a bi-modal method that combines
               the simplicity of touch with the speed of gaze for swiping
               through a word. The result is an efficient and comfortable
               dwell-free text entry method. In the lab study TAGSwipe achieved
               an average text entry rate of 15.46 wpm and significantly
               outperformed conventional swipe-based and dwell-based methods in
               efficacy and user satisfaction.",
  series    = "CHI '20",
  month     =  "21~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145/3313831.3376317",
  file      = "All Papers/My Library/Kumar et al. 2020 - TAGSwipe - Touch assisted gaze swipe for text entry.pdf",
  doi       = "10.1145/3313831.3376317",
  isbn      =  9781450367080
}

@INPROCEEDINGS{Wells2020-uv,
  title     = "{CollabAR  investigating the mediating role of mobile AR
               interfaces on co-located group collaboration}",
  author    = "Wells, Thomas and Houben, Steven",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  apr,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3313831.3376541",
  doi       = "10.1145/3313831.3376541"
}

@INPROCEEDINGS{Jiang2020-px,
  title     = "{How we type: Eye and finger movement strategies in mobile
               typing}",
  author    = "Jiang, Xinhui and Li, Yang and Jokinen, Jussi P P and Hirvola,
               Viet Ba and Oulasvirta, Antti and Ren, Xiangshi",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–14",
  abstract  = "Relatively little is known about eye and finger movement in
               typing with mobile devices. Most prior studies of mobile typing
               rely on log data, while data on finger and eye movements in
               typing come from studies with physical keyboards. This paper
               presents new findings from a transcription task with mobile
               touchscreen devices. Movement strategies were found to emerge in
               response to sharing of visual attention: attention is needed for
               guiding finger movements and detecting typing errors. In contrast
               to typing on physical keyboards, visual attention is kept mostly
               on the virtual keyboard, and glances at the text display are
               associated with performance. When typing with two fingers,
               although users make more errors, they manage to detect and
               correct them more quickly. This explains part of the known
               superiority of two-thumb typing over one-finger typing. We
               release the extensive dataset on everyday typing on smartphones.",
  series    = "CHI '20",
  month     =  "23~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145/3313831.3376711",
  file      = "All Papers/My Library/Jiang et al. 2020 - How we type - Eye and finger movement strategies in mobile typing.pdf",
  doi       = "10.1145/3313831.3376711",
  isbn      =  9781450367080
}

@INPROCEEDINGS{Kim2020-pp,
  title     = "{User-defined swarm robot control}",
  author    = "Kim, Lawrence H and Drew, Daniel S and Domova, Veronika and
               Follmer, Sean",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "21~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3313831.3376814",
  doi       = "10.1145/3313831.3376814",
  isbn      =  9781450367080
}

@INPROCEEDINGS{Stemasov2020-dv,
  title     = "{Mix\&Match: Towards omitting modelling through in-situ remixing
               of model repository artifacts in mixed reality}",
  author    = "Stemasov, Evgeny and Wagner, Tobias and Gugenheimer, Jan and
               Rukzio, Enrico",
  booktitle = "{Proceedings of the 2020 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  apr,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3313831.3376839",
  file      = "All Papers/My Library/Stemasov et al. 2020 - Mix&Match - Towards omitting modelling through in-situ remixing of model repository artifacts in mixed reality.pdf",
  doi       = "10.1145/3313831.3376839"
}

@INPROCEEDINGS{Katsini2020-ny,
  title     = "{The role of eye gaze in security and privacy applications:
               Survey and future HCI research directions}",
  author    = "Katsini, Christina and Abdrabou, Yasmeen and Raptis, George E and
               Khamis, Mohamed and Alt, Florian",
  booktitle = "{Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "21~" # apr,
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3313831.3376840",
  file      = "All Papers/Other/Katsini et al. 2020 - The role of eye gaze in security and privacy applications - Survey and future HCI research directions.pdf",
  doi       = "10.1145/3313831.3376840",
  isbn      =  9781450367080
}

@INPROCEEDINGS{Sasaki2019-ij,
  title     = "{Screen corner detection using polarization camera for
               cross-ratio based gaze estimation}",
  author    = "Sasaki, Masato and Nagamatsu, Takashi and Takemura, Kentaro",
  booktitle = "{Proceedings of the 11th ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "Eye tracking, which measures line of sight, is expected to
               advance as an intuitive and rapid input method for user
               interfaces, and a cross-ratio based method that calculates the
               point-of-gaze using homography matrices has attracted attention
               because it does not require hardware calibration to determine the
               geometric relationship between an eye camera and a screen.
               However, this method requires near-infrared (NIR) light-emitting
               diodes (LEDs) attached to the display in order to detect screen
               corners. Consequently, LEDs must be installed around the display
               to estimate the point-of-gaze. Without these requirements,
               cross-ratio based gaze estimation can be distributed smoothly.
               Therefore, we propose the use of a polarization camera for
               detecting the screen area reflected on a corneal surface. The
               reflection area of display light is easily detected by the
               polarized image because the light radiated from the display is
               polarized linearly by the internal polarization filter. With the
               proposed method, the screen corners can be determined without
               using NIR LEDs, and the point-of-gaze can be estimated using the
               detected corners on the corneal surface. We investigated the
               accuracy of the estimated point-of-gaze based on a cross-ratio
               method under various illumination and display conditions.
               Cross-ratio based gaze estimation is expected to be utilized
               widely in commercial products because the proposed method does
               not require infrared light sources at display corners.",
  series    = "ETRA '19",
  month     =  "25~" # jun,
  year      =  2019,
  url       = "https://doi.org/10.1145/3314111.3319814",
  file      = "All Papers/My Library/Sasaki et al. 2019 - Screen corner detection using polarization camera for cross-ratio based gaze estimation.pdf",
  doi       = "10.1145/3314111.3319814",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Drewes2019-ys,
  title     = "{Time- and space-efficient eye tracker calibration}",
  author    = "Drewes, Heiko and Pfeuffer, Ken and Alt, Florian",
  booktitle = "{Proceedings of the 11th ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--8",
  month     =  "25~" # jun,
  year      =  2019,
  url       = "https://dl.acm.org/doi/abs/10.1145/3314111.3319818?casa_token=hmCdWCRjjxUAAAAA:CyY-FVVaUb0ElqT-DVLM3uAOgIsHfzKCJrQZkpeFC51Jiyl5yALYTr8i7YrQikukF8f8ZCoL8uCSkOk",
  file      = "All Papers/Other/Drewes et al. 2019 - Time- and space-efficient eye tracker calibration.pdf",
  doi       = "10.1145/3314111.3319818",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Hassoumi2019-fq,
  title     = "{EyeFlow: pursuit interactions using an unmodified camera}",
  author    = "Hassoumi, Almoctar and Peysakhovich, Vsevolod and Hurter,
               Christophe",
  booktitle = "{Proceedings of the 11th ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "S2 TL;DR: The smooth pursuit eye movement based interaction using
               an unmodified off-the-shelf RGB camera is investigated using
               EyeFlow, which does not call for either a 3D pupil model or 2D
               pupil detection to track the pupil center location.",
  month     =  "25~" # jun,
  year      =  2019,
  url       = "https://www.semanticscholar.org/paper/9f4738fd376d281a1d6bb14e888ba93892a23ef7",
  file      = "All Papers/Other/Hassoumi et al. 2019 - EyeFlow - pursuit interactions using an unmodified camera.pdf",
  doi       = "10.1145/3314111.3319820",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Abdrabou2019-jw,
  title     = "{Calibration-free text entry using smooth pursuit eye movements}",
  author    = "Abdrabou, Yasmeen and Mostafa, Mariam and Khamis, Mohamed and
               Elmougy, Amr",
  booktitle = "{Proceedings of the 11th ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–5",
  abstract  = "In this paper, we propose a calibration-free gaze-based text
               entry system that uses smooth pursuit eye movements. We report on
               our implementation, which improves over prior work on smooth
               pursuit text entry by 1) eliminating the need of calibration
               using motion correlation, 2) increasing input rate from 3.34 to
               3.41 words per minute, 3) featuring text suggestions that were
               trained on 10,000 lexicon sentences recommended in the
               literature. We report on a user study (N=26) which shows that
               users are able to eye type at 3.41 words per minutes without
               calibration and without user training. Qualitative feedback also
               indicates that users positively perceive the system. Our work is
               of particular benefit for disabled users and for situations when
               voice and tactile input are not feasible (e.g., in noisy
               environments or when the hands are occupied).",
  series    = "ETRA '19",
  month     =  "25~" # jun,
  year      =  2019,
  url       = "https://doi.org/10.1145/3314111.3319838",
  file      = "All Papers/My Library/Abdrabou et al. 2019 - Calibration-free text entry using smooth pursuit eye movements.pdf",
  doi       = "10.1145/3314111.3319838",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Venuprasad2019-gp,
  title     = "{Characterizing joint attention behavior during real world
               interactions using automated object and gaze detection}",
  author    = "Venuprasad, Pranav and Dobhal, Tushal and Paul, Anurag and
               Nguyen, Tu N M and Gilman, Andrew and Cosman, Pamela and
               Chukoskie, Leanne",
  booktitle = "{Proceedings of the 11th ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Joint attention is an essential part of the development process
               of children, and impairments in joint attention are considered as
               one of the first symptoms of autism. In this paper, we develop a
               novel technique to characterize joint attention in real time, by
               studying the interaction of two human subjects with each other
               and with multiple objects present in the room. This is done by
               capturing the subjects' gaze through eye-tracking glasses and
               detecting their looks on predefined indicator objects. A deep
               learning network is trained and deployed to detect the objects in
               the field of vision of the subject by processing the video feed
               of the world view camera mounted on the eye-tracking glasses. The
               looking patterns of the subjects are determined and a real-time
               audio response is provided when a joint attention is detected,
               i.e., when their looks coincide. Our findings suggest a trade-off
               between the accuracy measure (Look Positive Predictive Value) and
               the latency of joint look detection for various system
               parameters. For more accurate joint look detection, the system
               has higher latency, and for faster detection, the detection
               accuracy goes down.",
  series    = "ETRA '19",
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3314111.3319843",
  file      = "All Papers/My Library/Venuprasad et al. 2019 - Characterizing joint attention behavior during real world interactions using automated object and gaze detection.pdf",
  doi       = "10.1145/3314111.3319843",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Bafna2019-qy,
  title     = "{Eye-tracking based fatigue and cognitive assessment: doctoral
               symposium, extended abstract}",
  author    = "Bafna, Tanya and Hansen, John Paulin",
  booktitle = "{Proceedings of the 11th ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–3",
  abstract  = "Fatigue detection, monitoring and management is important and
               needs to be accommodated in the busy lifestyles that many people
               have these days. It may have an impact on the physical as well as
               the emotional health of the individuals. Detection of fatigue is
               the first step towards its management. With eye-tracking software
               using cameras, and being included in the laptops and smartphones,
               it now has the potential to become quite ubiquitous.This extended
               abstract describes my PhD project for fatigue detection using
               eye-tracking measures while gaze typing. The steps taken and
               experiments conducted upto now are presented, with an outline of
               the future plans. The principal use-case will be to provide the
               service of fatigue detection for people with neurological
               disorders, who use eye-tracking for alternative communications.",
  series    = "ETRA '19",
  month     =  "25~" # jun,
  year      =  2019,
  url       = "https://doi.org/10.1145/3314111.3322867",
  doi       = "10.1145/3314111.3322867",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Sengupta2019-bp,
  title     = "{Impact of variable positioning of text prediction in gaze-based
               text entry}",
  author    = "Sengupta, Korok and Menges, Raphael and Kumar, Chandan and Staab,
               Steffen",
  booktitle = "{Proceedings of the 11th {ACM} Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--9",
  abstract  = "Text predictions play an important role in improving the
               performance of gaze-based text entry systems. However, visual
               search, scanning, and selection of text predictions require a
               shift in the user's attention from the keyboard layout. Hence the
               spatial positioning of predictions becomes an imperative aspect
               of the end-user experience. In this work, we investigate the role
               of spatial positioning by comparing the performance of three
               different keyboards entailing variable positions for text
               predictions. The experiment result shows no significant
               differences in the text entry performance, i.e., displaying
               suggestions closer to visual fovea did not enhance the text entry
               rate of participants, however they used more keystrokes and
               backspace. This implies to the inessential usage of suggestions
               when it is in the constant visual attention of users, resulting
               in increased cost of correction. Furthermore, we argue that the
               fast saccadic eye movements undermines the spatial distance
               optimization in prediction positioning.",
  series    = "ETRA '19",
  month     =  jun,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3317956.3318152",
  keywords  = "text prediction, gaze input, text entry, interaction, variable
               position;prj-gaze-shorthand",
  doi       = "10.1145/3317956.3318152",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Sengupta2019-bf,
  title     = "{Impact of variable positioning of text prediction in gaze-based
               text entry}",
  author    = "Sengupta, Korok and Menges, Raphael and Kumar, Chandan and Staab,
               Steffen",
  booktitle = "{Proceedings of the 11th ACM symposium on eye tracking research
               \& applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "Text predictions play an important role in improving the
               performance of gaze-based text entry systems. However, visual
               search, scanning, and selection of text predictions require a
               shift in the user's attention from the keyboard layout. Hence the
               spatial positioning of predictions becomes an imperative aspect
               of the end-user experience. In this work, we investigate the role
               of spatial positioning by comparing the performance of three
               different keyboards entailing variable positions for text
               predictions. The experiment result shows no significant
               differences in the text entry performance, i.e., displaying
               suggestions closer to visual fovea did not enhance the text entry
               rate of participants, however they used more keystrokes and
               backspace. This implies to the inessential usage of suggestions
               when it is in the constant visual attention of users, resulting
               in increased cost of correction. Furthermore, we argue that the
               fast saccadic eye movements undermines the spatial distance
               optimization in prediction positioning.",
  series    = "ETRA '19",
  month     =  "25~" # jun,
  year      =  2019,
  url       = "https://doi.org/10.1145/3317956.3318152",
  doi       = "10.1145/3317956.3318152",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Rivu2019-vp,
  title     = "{GazeButton: enhancing buttons with eye gaze interactions}",
  author    = "Rivu, Sheikh and Abdrabou, Yasmeen and Mayer, Thomas and
               Pfeuffer, Ken and Alt, Florian",
  booktitle = "{Proceedings of the 11th ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "The button is an element of a user interface to trigger an
               action, traditionally using click or touch. We introduce
               GazeButton, a novel concept extending the default button mode
               with advanced gaze-based interactions. During normal interaction,
               users can utilise this button as a universal hub for gaze-based
               UI shortcuts. The advantages are: 1) easy to integrate in
               existing UIs, 2) complementary, as users choose either gaze or
               manual interaction, 3) straightforward, as all features are
               located in one button, and 4) one button to interact with the
               whole screen. We explore GazeButtons for a custom-made text
               reading, writing, and editing tool on a multitouch tablet device.
               For example, this allows the text cursor position to be set as
               users look at the position and tap on the GazeButton, avoiding
               costly physical movement. Or, users can simply gaze over a part
               of the text that should be selected, while holding the
               GazeButton. We present a design space, specific application
               examples, and point to future button designs that become highly
               expressive by unifying the user's visual and manual input.",
  series    = "ETRA '19",
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3317956.3318154",
  doi       = "10.1145/3317956.3318154",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Spakov2019-oa,
  title     = "{Eye gaze and head gaze in collaborative games}",
  author    = "Špakov, Oleg and Istance, Howell and Räihä, Kari-Jouko and
               Viitanen, Tiia and Siirtola, Harri",
  booktitle = "{Proceedings of the 11th ACM Symposium on Eye Tracking Research
               \& Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "We present an investigation of sharing the focus of visual
               attention between two players in a collaborative game, so that
               where one player was looking was visible to the other. The
               difference between using head-gaze and eye-gaze to estimate the
               point of regard was studied, the motive being that recording
               head-gaze is easier and cheaper than eye-gaze. Two experiments
               are reported, the first investigates the effect of a high
               immersion presentation of the game in VR Head Mounted Display
               compared with a lower immersion desktop presentation. The second
               examines the high immersion condition in more detail. The studies
               show that in spite of there being many factors that could affect
               the outcome of a relatively short period of game play, sharing
               eye-gaze in the high immersion condition produces shorter overall
               durations and better subjective ratings of team work than does
               sharing head-gaze. This difference is not apparent in the low
               immersion condition. The findings are a good argument for
               exploiting the opportunities for including and using eye tracking
               within head mounted displays in the context of collaborative
               games.",
  series    = "ETRA '19",
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3317959.3321489",
  file      = "All Papers/My Library/Špakov et al. 2019 - Eye gaze and head gaze in collaborative games.pdf",
  doi       = "10.1145/3317959.3321489",
  isbn      =  9781450367097
}

@INPROCEEDINGS{Sibert2000-on,
  title     = "{Evaluation of eye gaze interaction}",
  author    = "Sibert, Linda E and Jacob, Robert J K",
  booktitle = "{Proceedings of the SIGCHI conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "281–288",
  abstract  = "Eye gaze interaction can provide a convenient and natural
               addition to user-computer dialogues. We have previously reported
               on our interaction techniques using eye gaze [10]. While our
               techniques seemed useful in demonstration, we now investigate
               their strengths and weaknesses in a controlled setting. In this
               paper, we present two experiments that compare an interaction
               technique we developed for object selection based on a where a
               person is looking with the most commonly used selection method
               using a mouse. We find that our eye gaze interaction technique is
               faster than selection with a mouse. The results show that our
               algorithm, which makes use of knowledge about how the eyes
               behave, preserves the natural quickness of the eye. Eye gaze
               interaction is a reasonable addition to computer interaction and
               is convenient in situations where it is important to use the
               hands for other tasks. It is particularly beneficial for the
               larger screen workspaces and virtual environments of the future,
               and it will become increasingly practical as eye tracker
               technology matures.",
  series    = "CHI '00",
  year      =  2000,
  url       = "https://dl.acm.org/doi/10.1145/332040.332445",
  file      = "All Papers/My Library/Sibert and Jacob 2000 - Evaluation of eye gaze interaction.pdf",
  doi       = "10.1145/332040.332445",
  isbn      =  9781581132168
}

@INPROCEEDINGS{Hoggenmueller2019-jt,
  title     = "{Self-moving robots and pulverized urban displays}",
  author    = "Hoggenmueller, Marius and Hespanhol, Luke and Wiethoff, Alexander
               and Tomitsch, Martin",
  booktitle = "{Proceedings of the 8th ACM international symposium on pervasive
               displays}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "6~" # dec,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3321335.3324950",
  doi       = "10.1145/3321335.3324950",
  isbn      =  9781450367516
}

@INPROCEEDINGS{Thoravi_Kumaravel2019-lh,
  title     = "{Loki}",
  author    = "Thoravi Kumaravel, Balasaravanan and Anderson, Fraser and
               Fitzmaurice, George and Hartmann, Bjoern and Grossman, Tovi",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "17~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3332165.3347872",
  file      = "All Papers/My Library/Thoravi Kumaravel et al. 2019 - Loki.pdf",
  doi       = "10.1145/3332165.3347872",
  isbn      =  9781450368162
}

@INPROCEEDINGS{Ahn2019-cd,
  title     = "{Gaze-assisted typing for smart glasses}",
  author    = "Ahn, Sunggeun and Lee, Geehyuk",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "857–869",
  abstract  = "Text entry is expected to be a common task for smart glass users,
               which is generally performed using a touchpad on the temple or by
               a promising approach using eye tracking. However, each approach
               has its own limitations. For more efficient text entry, we
               present the concept of gaze-assisted typing (GAT), which uses
               both a touchpad and eye tracking. We initially examined GAT with
               a minimal eye input load, and demonstrated that the GAT
               technology was 51\% faster than a two-step touch input typing
               method (i.e.,M-SwipeBoard: 5.85 words per minute (wpm) and GAT:
               8.87 wpm). We also compared GAT methods with varying numbers of
               touch gestures. The results showed that a GAT requiring five
               different touch gestures was the most preferred, although all GAT
               techniques were equally efficient. Finally, we compared GAT with
               touch-only typing (SwipeZone) and eye-only typing (adjustable
               dwell) using an eye-trackable head-worn display. The results
               demonstrate that the most preferred technique, GAT, was 25.4\%
               faster than the eye-only typing and 29.4\% faster than the
               touch-only typing (GAT: 11.04 wpm, eye-only typing: 8.81 wpm, and
               touch-only typing: 8.53 wpm).",
  series    = "UIST '19",
  month     =  "17~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145/3332165.3347883",
  file      = "All Papers/My Library/Ahn and Lee 2019 - Gaze-assisted typing for smart glasses.pdf",
  doi       = "10.1145/3332165.3347883",
  isbn      =  9781450368162
}

@INPROCEEDINGS{Ahuja2019-qp,
  title     = "{MeCap: Whole-body digitization for low-cost VR/AR headsets}",
  author    = "Ahuja, Karan and Harrison, Chris and Goel, Mayank and Xiao,
               Robert",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3332165.3347889",
  file      = "All Papers/My Library/Ahuja et al. 2019 - MeCap - Whole-body digitization for low-cost VR - AR headsets.pdf",
  doi       = "10.1145/3332165.3347889"
}

@INPROCEEDINGS{Li2019-hw,
  title     = "{Robiot: A Design Tool for Actuating Everyday Objects with
               Automatically Generated {3D} Printable Mechanisms}",
  author    = "Li, Jiahao and Kim, Jeeeun and Chen, Xiang 'anthony'",
  booktitle = "{Proceedings of the 32nd Annual {ACM} Symposium on User Interface
               Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "673--685",
  abstract  = "Users can now easily communicate digital information with an
               Internet of Things; in contrast, there remains a lack of support
               to automate physical tasks that involve legacy static objects,
               e.g. adjusting a desk lamp's angle for optimal brightness,
               turning on/off a manual faucet when washing dishes, sliding a
               window to maintain a preferred indoor temperature. Automating
               these simple physical tasks has the potential to improve people's
               quality of life, which is particularly important for people with
               a disability or in situational impairment.We present Robiot -- a
               design tool for generating mechanisms that can be attached to,
               motorized, and actuating legacy static objects to perform simple
               physical tasks. Users only need to take a short video
               manipulating an object to demonstrate an intended physical
               behavior. Robiot then extracts requisite parameters and
               automatically generates 3D models of the enabling actuation
               mechanisms by performing a scene and motion analysis of the 2D
               video in alignment with the object's 3D model. In an hour-long
               design session, six participants used Robiot to actuate seven
               everyday objects, imbuing them with the robotic capability to
               automate various physical tasks.",
  series    = "UIST '19",
  month     =  oct,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3332165.3347894",
  file      = "All Papers/Other/Li et al. 2019 - Robiot - A Design Tool for Actuating Everyday Objects with Automatically Generated 3D Printable Mechanisms.pdf",
  keywords  = "actuation, generative design, design tool, everyday objects;world
               model",
  doi       = "10.1145/3332165.3347894",
  isbn      =  9781450368162
}

@INPROCEEDINGS{Li2019-nl,
  title     = "{Robiot: A design tool for actuating everyday objects with
               automatically generated 3D printable mechanisms}",
  author    = "Li, Jiahao and Kim, Jeeeun and Chen, Xiang 'anthony'",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "673–685",
  abstract  = "Users can now easily communicate digital information with an
               Internet of Things; in contrast, there remains a lack of support
               to automate physical tasks that involve legacy static objects,
               e.g. adjusting a desk lamp's angle for optimal brightness,
               turning on/off a manual faucet when washing dishes, sliding a
               window to maintain a preferred indoor temperature. Automating
               these simple physical tasks has the potential to improve people's
               quality of life, which is particularly important for people with
               a disability or in situational impairment.We present Robiot – a
               design tool for generating mechanisms that can be attached to,
               motorized, and actuating legacy static objects to perform simple
               physical tasks. Users only need to take a short video
               manipulating an object to demonstrate an intended physical
               behavior. Robiot then extracts requisite parameters and
               automatically generates 3D models of the enabling actuation
               mechanisms by performing a scene and motion analysis of the 2D
               video in alignment with the object's 3D model. In an hour-long
               design session, six participants used Robiot to actuate seven
               everyday objects, imbuing them with the robotic capability to
               automate various physical tasks.",
  series    = "UIST '19",
  month     =  oct,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3332165.3347894",
  file      = "All Papers/My Library/Li et al. 2019 - Robiot - A design tool for actuating everyday objects with automatically generated 3D printable mechanisms.pdf",
  doi       = "10.1145/3332165.3347894",
  isbn      =  9781450368162
}

@INPROCEEDINGS{Cao2019-rs,
  title     = "{GhostAR: A time-space editor for embodied authoring of
               human-robot collaborative task with augmented reality}",
  author    = "Cao, Yuanzhi and Wang, Tianyi and Qian, Xun and Rao, Pawan S and
               Wadhawan, Manav and Huo, Ke and Ramani, Karthik",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3332165.3347902",
  file      = "All Papers/My Library/Cao et al. 2019 - GhostAR - A time-space editor for embodied authoring of human-robot collaborative task with augmented reality.pdf",
  doi       = "10.1145/3332165.3347902"
}

@INPROCEEDINGS{Suzuki2019-lm,
  title     = "{ShapeBots: Shape-changing swarm robots}",
  author    = "Suzuki, Ryo and Zheng, Clement and Kakehi, Yasuaki and Yeh, Tom
               and Do, Ellen Yi-Luen and Gross, Mark D and Leithinger, Daniel",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3332165.3347911",
  file      = "All Papers/My Library/Suzuki et al. 2019 - ShapeBots - Shape-changing swarm robots.pdf",
  doi       = "10.1145/3332165.3347911"
}

@INPROCEEDINGS{Gebhardt2019-rv,
  title     = "{Learning cooperative personalized policies from gaze data}",
  author    = "Gebhardt, Christoph and Hecox, Brian and van Opheusden, Bas and
               Wigdor, Daniel and Hillis, James and Hilliges, Otmar and Benko,
               Hrvoje",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "197–208",
  abstract  = "An ideal Mixed Reality (MR) system would only present virtual
               information (e.g., a label) when it is useful to the person.
               However, deciding when a label is useful is challenging: it
               depends on a variety of factors, including the current task,
               previous knowledge, context, etc. In this paper, we propose a
               Reinforcement Learning (RL) method to learn when to show or hide
               an object's label given eye movement data. We demonstrate the
               capabilities of this approach by showing that an intelligent
               agent can learn cooperative policies that better support users in
               a visual search task than manually designed heuristics.
               Furthermore, we show the applicability of our approach to more
               realistic environments and use cases (e.g., grocery shopping). By
               posing MR object labeling as a model-free RL problem, we can
               learn policies implicitly by observing users' behavior without
               requiring a visual search model or data annotation.",
  series    = "UIST '19",
  month     =  "17~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145/3332165.3347933",
  file      = "All Papers/My Library/Gebhardt et al. 2019 - Learning cooperative personalized policies from gaze data.pdf",
  doi       = "10.1145/3332165.3347933",
  isbn      =  9781450368162
}

@INPROCEEDINGS{He2019-vx,
  title     = "{Ondulé: Designing and controlling 3D printable springs}",
  author    = "He, Liang and Peng, Huaishu and Lin, Michelle and Konjeti,
               Ravikanth and Guimbretière, François and Froehlich, Jon E",
  booktitle = "{Proceedings of the 32nd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "739–750",
  abstract  = "We present Ondulé-an interactive design tool that allows novices
               to create parameterizable deformation behaviors in 3D-printable
               models using helical springs and embedded joints. Informed by
               spring theory and our empirical mechanical experiments, we
               introduce spring and joint-based design techniques that support a
               range of parameterizable deformation behaviors, including
               compress, extend, twist, bend, and various combinations. To
               enable users to design and add these deformations to their
               models, we introduce a custom design tool for Rhino. Here, users
               can convert selected geometries into springs, customize spring
               stiffness, and parameterize their design with mechanical
               constraints for desired behaviors. To demonstrate the feasibility
               of our approach and the breadth of new designs that it enables,
               we showcase a set of example 3D-printed applications from
               launching rocket toys to tangible storytelling props. We conclude
               with a discussion of key challenges and open research questions.",
  series    = "UIST '19",
  month     =  "17~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145/3332165.3347951",
  doi       = "10.1145/3332165.3347951",
  isbn      =  9781450368162
}

@INPROCEEDINGS{Narazani2019-jz,
  title     = "{Extending AR interaction through 3D printed tangible interfaces
               in an urban planning context}",
  author    = "Narazani, Marla and Eghtebas, Chloe and Klinker, Gudrun and
               Jenney, Sarah L and Mühlhaus, Michael and Petzold, Frank",
  booktitle = "{The adjunct publication of the 32nd annual ACM symposium on user
               interface software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Embedding conductive material into 3D printed objects enables
               non-interactive objects to become tangible without the need to
               attach additional components. We present a novel use for such
               touch-sensitive objects in an augmented reality (AR) setting and
               explore the use of gestures for enabling different types of
               interaction with digital and physical content. In our
               demonstration, the setting is an urban planning scenario. The
               multi-material 3D printed buildings consist of thin layers of
               white plastic filament and a conductive wireframe to enable touch
               gestures. Attendees can either interact with the physical model
               or with the mobile AR interface for selecting, adding or deleting
               buildings.",
  month     =  "14~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3332167.3356891",
  doi       = "10.1145/3332167.3356891",
  isbn      =  9781450368179
}

@INPROCEEDINGS{Sasaki2019-bz,
  title     = "{Cross-ratio based gaze estimation for multiple displays using a
               polarization camera}",
  author    = "Sasaki, Masato and Nagamatsu, Takashi and Takemura, Kentaro",
  booktitle = "{The adjunct publication of the 32nd annual ACM symposium on user
               interface software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–3",
  abstract  = "While eye tracking has been typically used for achieving
               intuitive user interfaces, it is not sufficient when it comes to
               dealing with multiple-display environments. In such environments,
               which have become popular recently, the point-of-gaze should be
               estimated on multiple screens. Therefore, we propose a
               cross-ratio based gaze estimation using a polarization camera for
               multiple screens. The point-of-gaze can be estimated on each
               monitor by identifying the screen reflected on the corneal
               surface at a polarization angle. Near-infrared light emitting
               diodes (NIR-LEDs) attached to the display are not required. This
               means that standard displays can be used with high general
               versatility as an advantage.",
  series    = "UIST '19",
  month     =  "14~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145/3332167.3357095",
  file      = "All Papers/My Library/Sasaki et al. 2019 - Cross-ratio based gaze estimation for multiple displays using a polarization camera.pdf",
  doi       = "10.1145/3332167.3357095",
  isbn      =  9781450368179
}

@INPROCEEDINGS{Zhi2020-ix,
  title     = "{GameBot: A Visualization-augmented Chatbot for Sports Game}",
  author    = "Zhi, Qiyu and Metoyer, Ronald",
  booktitle = "{Extended Abstracts of the 2020 CHI Conference on Human Factors
               in Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "The major sports leagues, including The National Basketball
               Association (NBA) and the EPL (the English Premier League), are
               adopting conversational systems (chatbots) as an innovative
               outlet to deliver game information and engage fans. However,
               current sports chatbots only provide scores and game highlight
               videos, which are often inadequate for statistical data related
               requests. We present GameBot, an interactive chatbot for sports
               fans to explore game statistical data. GameBot features (1) the
               direct answers to user's stats-related questions, and (2) the use
               of data visualizations as supporting context for sports fans'
               stats-related questions.",
  series    = "CHI EA '20",
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3334480.3382794",
  doi       = "10.1145/3334480.3382794",
  isbn      =  9781450368193
}

@INPROCEEDINGS{Rivu2020-ze,
  title     = "{Gaze'N'Touch: Enhancing text selection on mobile devices using
               gaze}",
  author    = "Rivu, Radiah and Abdrabou, Yasmeen and Pfeuffer, Ken and Hassib,
               Mariam and Alt, Florian",
  booktitle = "{Extended abstracts of the 2020 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Text selection is a frequent task we do everyday to edit, modify
               or delete text. Selecting a word requires not only precision but
               also switching between selections and typing which influences
               both speed and error rates. In this paper, we evaluate a novel
               concept that extends text editing with an additional modality,
               that is gaze. We present a user study (N=16) where we explore
               how, the novel concepts called GazeButton can improve text
               selection by comparing it to touch based selection. In addition,
               we tested the effect of text size on the selection techniques by
               comparing two different text sizes.Results show that gaze based
               selection was faster with bigger text size, although not
               statistically significant. Qualitative feedback show a preference
               on gaze over touch which motivates a new direction of gaze usage
               in text editors.",
  series    = "CHI EA '20",
  month     =  "25~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145/3334480.3382802",
  file      = "All Papers/My Library/Rivu et al. 2020 - Gaze'N'Touch - Enhancing text selection on mobile devices using gaze.pdf",
  doi       = "10.1145/3334480.3382802",
  isbn      =  9781450368193
}

@INPROCEEDINGS{Fuste2020-am,
  title     = "{Kinetic AR: A framework for robotic motion systems in spatial
               computing}",
  author    = "Fuste, Anna and Reynolds, Ben and Hobin, James and Heun, Valentin",
  booktitle = "{Extended abstracts of the 2020 CHI conference on human factors
               in computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "25~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3334480.3382814",
  doi       = "10.1145/3334480.3382814",
  isbn      =  9781450368193
}

@INPROCEEDINGS{Babic2020-sx,
  title     = "{Simo: Interactions with distant displays by smartphones with
               simultaneous face and world tracking}",
  author    = "Babic, Teo and Perteneder, Florian and Reiterer, Harald and
               Haller, Michael",
  booktitle = "{Extended abstracts of the 2020 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "The interaction with distant displays often demands complex,
               multi-modal inputs which need to be achieved with a very simple
               hardware solution so that users can perform rich inputs wherever
               they encounter a distant display. We present Simo, a novel
               approach, that transforms a regular smartphone into a
               highly-expressive user motion tracking device and controller for
               distant displays. Both the front and back cameras of the
               smartphone are used simultaneously to track the user's hand as
               well as the head, and body movements in real-world space and
               scale. In this work, we first define the possibilities for
               simultaneous face- and world-tracking using current off-the-shelf
               smartphones. Next, we present the implementation of a smartphone
               app enabling hand, head, and body motion tracking. Finally, we
               present a technical analysis outlining the possible tracking
               range.",
  series    = "CHI EA '20",
  month     =  "25~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145/3334480.3382962",
  doi       = "10.1145/3334480.3382962",
  isbn      =  9781450368193
}

@INPROCEEDINGS{Thevin2019-kp,
  title     = "{Creating accessible interactive audio-tactile drawings using
               spatial augmented reality}",
  author    = "Thevin, Lauren and Jouffrais, Christophe and Rodier, Nicolas and
               Palard, Nicolas and Hachet, Martin and Brock, Anke M",
  booktitle = "{Proceedings of the 2019 ACM international conference on
               interactive surfaces and spaces}",
  publisher = "ACM",
  month     =  nov,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3343055.3359711",
  file      = "All Papers/My Library/Thevin et al. 2019 - Creating accessible interactive audio-tactile drawings using spatial augmented reality.pdf",
  doi       = "10.1145/3343055.3359711"
}

@ARTICLE{Tong2019-tl,
  title    = "{Tracking fatigue and health state in multiple sclerosis patients
              using connnected wellness devices}",
  author   = "Tong, Catherine and Craner, Matthew and Vegreville, Matthieu and
              Lane, Nicholas D",
  journal  = "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.",
  volume   =  3,
  number   =  3,
  pages    = "1–19",
  abstract = "Multiple Sclerosis requires long-term disease management, but
              tracking patients through the use of clinical surveys is hindered
              by high costs and patient burden. In this work, we investigate the
              feasibility of using data from ubiquitous sensing to predict MS
              patients' fatigue and health status, as measured by the Fatigue
              Severity Scale (FSS) and EQ-5D index. We collected data from 198
              MS patients who are given connected wellness devices for over 6
              months. We examine how accurately can the collected data predict
              reported FSS and EQ-5D scores per patient using an ensemble of
              regressors. In predicting for both FSS and EQ-5D, we are able to
              achieve errors aligning with the instrument' standard measurement
              error (SEM), as well as strong and significant correlations
              between predicted and ground truth values. We also show a simple
              adaptation method that greatly reduces prediction errors through
              the use of just 1 user-supplied ground truth datapoint. For FSS
              (SEM 0.7), the universal model predicts weekly scores with MAE
              1.00, while an adapted model predicts with MAE 0.58. For EQ-5D
              (SEM 0.093), the universal model predicts weekly scores with MAE
              0.097, while an adapted model predicts with MAE 0.065. Our study
              represents the first sets of results showing that fatigue and
              health state of MS patients can be measured using data from
              connected wellness devices and a small number of background
              features, with promising prediction performance with errors within
              the accepted range of error in the widely used
              clinically-validated questionnaires. Future extensions and
              potential applications of our results can positively impact MS
              patient disease management and support clinical research.",
  month    =  "9~" # sep,
  year     =  2019,
  url      = "https://doi.org/10.1145/3351264",
  doi      = "10.1145/3351264"
}

@ARTICLE{Le_Ngo2019-ui,
  title    = "{Seeing the Invisible: Survey of Video Motion Magnification and
              Small Motion Analysis}",
  author   = "Le Ngo, Anh Cat and Phan, Raphaël C-W",
  journal  = "ACM Computing Surveys",
  volume   =  52,
  number   =  6,
  pages    = "114:1–114:20",
  abstract = "The latest techniques in video motion magnification and relevant
              small motion analysis are surveyed. The main motion magnification
              techniques are discussed in chronological fashion, highlighting
              the inherent limitations of predecessor techniques in comparison
              with subsequent variants. The focus is then shifted to the
              specific stages within the motion magnification framework to
              discuss advancements that have been proposed in the literature,
              namely for spatial decomposition and for emphasizing,
              representing, and distinguishing different motion signals. The
              survey concludes with a treatment of different problems in varying
              application contexts that have benefited from motion magnification
              and small motion analysis.",
  year     =  2019,
  url      = "https://dl.acm.org/doi/10.1145/3355389",
  doi      = "10.1145/3355389",
  issn     = "0360-0300"
}

@INPROCEEDINGS{Villarreal-Narvaez2020-cf,
  title     = "{A systematic review of gesture elicitation studies}",
  author    = "Villarreal-Narvaez, Santiago and Vanderdonckt, Jean and Vatavu,
               Radu-Daniel and Wobbrock, Jacob O",
  booktitle = "{Proceedings of the 2020 ACM designing interactive systems
               conference}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "7~" # mar,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3357236.3395511",
  file      = "All Papers/My Library/Villarreal-Narvaez et al. 2020 - A systematic review of gesture elicitation studies.pdf",
  doi       = "10.1145/3357236.3395511",
  isbn      =  9781450369749
}

@INCOLLECTION{Petersen2020-oo,
  title     = "{Affordances of {Shape-Changing} Interfaces: An Information
               Perspective on Transformability and Movement}",
  author    = "Petersen, Marianne Graves and Rasmussen, Majken Kirkegaard and
               Trettvik, Johan",
  booktitle = "{Proceedings of the 2020 {ACM} Designing Interactive Systems
               Conference}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1959--1971",
  abstract  = "Affordances is an important term in the field of Shape-Changing
               Interfaces (SCI). But the field is challenged by a multitude of
               vaguely defined concepts around affordances such as dynamic,
               spatial, or tactile affordances. In line with this, Alexander et
               al [1] recently pointed to lack of theory as one of the grand
               challenges for SCI. This paper proposes a re-analysis of Gibson
               for resolving the conceptual cacophony in the field of SCI.
               Essential to the ecological approach to affordances, is that
               perception is dynamic and people pick up information through
               exploring objects and environments over time. From this
               perspective, affordances of Shape-Changing Interfaces become a
               matter of providing Information - through the design - regarding
               a) the interfaces' ability to transform its shape and b) how
               movement suggests level of animacy. Through analyzing scholarly
               SCI prototypes, from this perspective, we provide an initial set
               of design strategies for affordances of shape-changing
               interfaces.",
  month     =  jul,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3357236.3395521",
  file      = "All Papers/Other/Petersen et al. 2020 - Affordances of Shape-Changing Interfaces - An Information Perspective on Transformability and Movement.pdf",
  doi       = "10.1145/3357236.3395521",
  isbn      =  9781450369749
}

@INCOLLECTION{Petersen2020-id,
  title     = "{Affordances of Shape-Changing interfaces: An information
               perspective on transformability and movement}",
  author    = "Petersen, Marianne Graves and Rasmussen, Majken Kirkegaard and
               Trettvik, Johan",
  booktitle = "{Proceedings of the 2020 ACM designing interactive systems
               conference}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1959–1971",
  abstract  = "Affordances is an important term in the field of Shape-Changing
               Interfaces (SCI). But the field is challenged by a multitude of
               vaguely defined concepts around affordances such as dynamic,
               spatial, or tactile affordances. In line with this, Alexander et
               al [1] recently pointed to lack of theory as one of the grand
               challenges for SCI. This paper proposes a re-analysis of Gibson
               for resolving the conceptual cacophony in the field of SCI.
               Essential to the ecological approach to affordances, is that
               perception is dynamic and people pick up information through
               exploring objects and environments over time. From this
               perspective, affordances of Shape-Changing Interfaces become a
               matter of providing Information - through the design - regarding
               a) the interfaces' ability to transform its shape and b) how
               movement suggests level of animacy. Through analyzing scholarly
               SCI prototypes, from this perspective, we provide an initial set
               of design strategies for affordances of shape-changing
               interfaces.",
  month     =  jul,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3357236.3395521",
  file      = "All Papers/My Library/Petersen et al. 2020 - Affordances of Shape-Changing interfaces - An information perspective on transformability and movement.pdf",
  doi       = "10.1145/3357236.3395521",
  isbn      =  9781450369749
}

@INPROCEEDINGS{Anjos2019-kn,
  title     = "{Adventures in hologram space: Exploring the design space of
               eye-to-eye volumetric telepresence}",
  author    = "Anjos, Rafael Kuffner dos and Sousa, Maurício and Mendes, Daniel
               and Medeiros, Daniel and Billinghurst, Mark and Anslow, Craig and
               Jorge, Joaquim",
  booktitle = "{25th {ACM} Symposium on Virtual Reality Software and Technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "The design space for interacting with volumetric representations
               of people is discussed and an approach for dynamically
               manipulating scale, orientation and the position of holograms
               which guarantees eye-contact is presented. Modern volumetric
               projection-based telepresence approaches are capable of providing
               realistic full-size virtual representations of remote people.
               Interacting with full-size people may not be desirable due to the
               spatial constraints of the physical environment, application
               context, or display technology. However, the miniaturization of
               remote people is known to create an eye gaze matching problem.
               Eye-contact is essential to communication as it allows for people
               to use natural nonverbal cues and improves the sense of ``being
               there''. In this paper we discuss the design space for
               interacting with volumetric representations of people and present
               an approach for dynamically manipulating scale, orientation and
               the position of holograms which guarantees eye-contact. We
               created a working augmented reality-based prototype and validated
               it with 14 participants.",
  month     =  nov,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3359996.3364244",
  doi       = "10.1145/3359996.3364244",
  isbn      =  9781450370011,
  language  = "en"
}

@INPROCEEDINGS{Anjos2019-dm,
  title     = "{Adventures in hologram space: Exploring the design space of
               eye-to-eye volumetric telepresence}",
  author    = "Anjos, Rafael Kuffner dos and Sousa, Maurício and Mendes, Daniel
               and Medeiros, Daniel and Billinghurst, Mark and Anslow, Craig and
               Jorge, Joaquim",
  booktitle = "{25th ACM symposium on virtual reality software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "The design space for interacting with volumetric representations
               of people is discussed and an approach for dynamically
               manipulating scale, orientation and the position of holograms
               which guarantees eye-contact is presented. Modern volumetric
               projection-based telepresence approaches are capable of providing
               realistic full-size virtual representations of remote people.
               Interacting with full-size people may not be desirable due to the
               spatial constraints of the physical environment, application
               context, or display technology. However, the miniaturization of
               remote people is known to create an eye gaze matching problem.
               Eye-contact is essential to communication as it allows for people
               to use natural nonverbal cues and improves the sense of “being
               there”. In this paper we discuss the design space for interacting
               with volumetric representations of people and present an approach
               for dynamically manipulating scale, orientation and the position
               of holograms which guarantees eye-contact. We created a working
               augmented reality-based prototype and validated it with 14
               participants.",
  month     =  nov,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3359996.3364244",
  doi       = "10.1145/3359996.3364244",
  isbn      =  9781450370011,
  language  = "en"
}

@INPROCEEDINGS{Wang2019-ke,
  title     = "{LiDAR aided integrated navigation system for indoor
               environments}",
  author    = "Wang, Xiaorui and Han, Shuai and Wei, Baoshan",
  booktitle = "{Proceedings of the 2019 international conference on video,
               signal and image processing}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "29~" # oct,
  year      =  2019,
  url       = "https://doi.org/10.1145%2F3369318.3369319",
  doi       = "10.1145/3369318.3369319",
  isbn      =  9781450371483
}

@INPROCEEDINGS{Barral2020-tp,
  title     = "{Understanding the effectiveness of adaptive guidance for
               narrative visualization: a gaze-based analysis}",
  author    = "Barral, Oswald and Lallé, Sébastien and Conati, Cristina",
  booktitle = "{Proceedings of the 25th International Conference on Intelligent
               User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "17~" # mar,
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3377325.3377517",
  file      = "All Papers/Other/Barral et al. 2020 - Understanding the effectiveness of adaptive guidance for narrative visualization - a gaze-based analysis.pdf",
  doi       = "10.1145/3377325.3377517",
  isbn      =  9781450371186
}

@INPROCEEDINGS{Constantinides2020-zl,
  title     = "{An eye gaze-driven metric for estimating the strength of
               graphical passwords based on image hotspots}",
  author    = "Constantinides, Argyris and Belk, Marios and Fidas, Christos and
               Pitsillides, Andreas",
  booktitle = "{Proceedings of the 25th International Conference on Intelligent
               User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "17~" # mar,
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3377325.3377537",
  file      = "All Papers/Other/Constantinides et al. 2020 - An eye gaze-driven metric for estimating the strength of graphical passwords based on image hotspots.pdf",
  doi       = "10.1145/3377325.3377537",
  isbn      =  9781450371186
}

@INPROCEEDINGS{Barz2020-ff,
  title     = "{Visual search target inference in natural interaction settings
               with machine learning}",
  author    = "Barz, Michael and Stauden, Sven and Sonntag, Daniel",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Visual search is a perceptual task in which humans aim at
               identifying a search target object such as a traffic sign among
               other objects. Search target inference subsumes computational
               methods for predicting this target by tracking and analyzing
               overt behavioral cues of that person, e.g., the human gaze and
               fixated visual stimuli. We present a generic approach to
               inferring search targets in natural scenes by predicting the
               class of the surrounding image segment. Our method encodes visual
               search sequences as histograms of fixated segment classes
               determined by SegNet, a deep learning image segmentation model
               for natural scenes. We compare our sequence encoding and model
               training (SVM) to a recent baseline from the literature for
               predicting the target segment. Also, we use a new search target
               inference dataset. The results show that, first, our new
               segmentation-based sequence encoding outperforms the method from
               the literature, and second, that it enables target inference in
               natural settings.",
  series    = "ETRA '20 full papers",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379155.3391314",
  doi       = "10.1145/3379155.3391314",
  isbn      =  9781450371339
}

@INPROCEEDINGS{Bace2020-zz,
  title     = "{Combining gaze estimation and optical flow for pursuits
               interaction}",
  author    = "Bace, Mihai and Becker, Vincent and Wang, Chenyang and Bulling,
               Andreas",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–10",
  abstract  = "Pursuit eye movements have become widely popular because they
               enable spontaneous eye-based interaction. However, existing
               methods to detect smooth pursuits require special-purpose eye
               trackers. We propose the first method to detect pursuits using a
               single off-the-shelf RGB camera in unconstrained remote settings.
               The key novelty of our method is that it combines
               appearance-based gaze estimation with optical flow in the eye
               region to jointly analyse eye movement dynamics in a single
               pipeline. We evaluate the performance and robustness of our
               method for different numbers of targets and trajectories in a
               13-participant user study. We show that our method not only
               outperforms the current state of the art but also achieves
               competitive performance to a consumer eye tracker for a small
               number of targets. As such, our work points towards a new family
               of methods for pursuit interaction directly applicable to an
               ever-increasing number of devices readily equipped with cameras.",
  series    = "ETRA '20 full papers",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379155.3391315",
  doi       = "10.1145/3379155.3391315",
  isbn      =  9781450371339
}

@INPROCEEDINGS{Venuprasad2020-wz,
  title     = "{Analyzing Gaze Behavior Using Object Detection and Unsupervised
               Clustering}",
  author    = "Venuprasad, Pranav and Xu, Li and Huang, Enoch and Gilman, Andrew
               and Ph.D., Leanne Chukoskie and Cosman, Pamela",
  booktitle = "{ACM Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "Gaze behavior is important in early development, and atypical
               gaze behavior is among the first symptoms of autism. Here we
               describe a system that quantitatively assesses gaze behavior
               using eye-tracking glasses. Objects in the subject’s field of
               view are detected using a deep learning model on the video
               captured by the glasses’ world-view camera, and a stationary
               frame of reference is estimated using the positions of the
               detected objects. The gaze positions relative to the new frame of
               reference are subjected to unsupervised clustering to obtain the
               time sequence of looks. The clustering method increases the
               accuracy of look detection on test videos compared against a
               previous algorithm, and is considerably more robust on videos
               with poor calibration.",
  series    = "ETRA '20 Full Papers",
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3379155.3391316",
  file      = "All Papers/My Library/Venuprasad et al. 2020 - Analyzing Gaze Behavior Using Object Detection and Unsupervised Clustering.pdf",
  doi       = "10.1145/3379155.3391316",
  isbn      =  9781450371339
}

@INPROCEEDINGS{Krishna_Sharma2020-db,
  title     = "{Eye Gaze Controlled Robotic Arm for Persons with Severe Speech
               and Motor Impairment}",
  author    = "Krishna Sharma, Vinay and Saluja, Kamalpreet and Mollyn, Vimal
               and Biswas, Pradipta",
  booktitle = "{{ACM} Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--9",
  abstract  = "Recent advancements in the field of robotics offers new promises
               for people with different range of abilities although making a
               human robot interface for people with severe disabilities is
               challenging. This paper describes the design and development of
               an eye gaze controlled interface for users with severe speech and
               motor impairment to manipulate a robotic arm. Two user studies
               were reported on pick and drop and reachability studies involving
               users with severe speech and motor impairment. Using the eye gaze
               controlled interface users could undertake representative pick
               and drop task at an average duration less than 15 secs and reach
               a randomly designated target within 60 secs.",
  series    = "ETRA '20 Full Papers",
  month     =  jun,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379155.3391324",
  keywords  = "assistive technology, Human-Robot Interaction, Eye gaze
               tracking;prj-gaze-shorthand",
  doi       = "10.1145/3379155.3391324",
  isbn      =  9781450371339
}

@INPROCEEDINGS{Krishna_Sharma2020-kh,
  title     = "{Eye gaze controlled robotic arm for persons with severe speech
               and motor impairment}",
  author    = "Krishna Sharma, Vinay and Saluja, Kamalpreet and Mollyn, Vimal
               and Biswas, Pradipta",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "Recent advancements in the field of robotics offers new promises
               for people with different range of abilities although making a
               human robot interface for people with severe disabilities is
               challenging. This paper describes the design and development of
               an eye gaze controlled interface for users with severe speech and
               motor impairment to manipulate a robotic arm. Two user studies
               were reported on pick and drop and reachability studies involving
               users with severe speech and motor impairment. Using the eye gaze
               controlled interface users could undertake representative pick
               and drop task at an average duration less than 15 secs and reach
               a randomly designated target within 60 secs.",
  series    = "ETRA '20 full papers",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379155.3391324",
  doi       = "10.1145/3379155.3391324",
  isbn      =  9781450371339
}

@INPROCEEDINGS{Geisler2020-we,
  title     = "{A MinHash approach for fast scanpath classification}",
  author    = "Geisler, David and Castner, Nora and Kasneci, Gjergji and
               Kasneci, Enkelejda",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "The visual scanpath describes the shift of visual attention over
               time. Characteristic patterns in the attention shifts allow
               inferences about cognitive processes, performed tasks, intention,
               or expertise. To analyse such patterns, the scanpath is often
               represented as a sequence of symbols that can be used to
               calculate a similarity score to other scanpaths. However, as the
               length of the scanpath or the number of possible symbols
               increases, established methods for scanpath similarity become
               inefficient, both in terms of runtime and memory consumption. We
               present a MinHash approach for efficient scanpath similarity
               calculation. Our approach shows competitive results in clustering
               and classification of scanpaths compared to established methods
               such as Needleman-Wunsch, but at a fraction of the required
               runtime. Furthermore, with time complexity of and constant memory
               consumption, our approach is ideally suited for real-time
               operation or analyzing large amounts of data.",
  series    = "ETRA '20 full papers",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379155.3391325",
  doi       = "10.1145/3379155.3391325",
  isbn      =  9781450371339
}

@INPROCEEDINGS{Bafna2020-jd,
  title     = "{Cognitive load during eye-typing}",
  author    = "Bafna, Tanya and Hansen, John Paulin Paulin and Baekgaard, Per",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "In this paper, we have measured cognitive load during an
               interactive eye-tracking task. Eye-typing was chosen as the task,
               because of its familiarity, ubiquitousness and ease. Experiments
               with 18 participants, where they memorized and eye-typed easy and
               difficult sentences over four days, were used to compare the
               difficulty levels of the tasks using subjective scores and
               eye-metrics like blink duration, frequency and interval and pupil
               dilation were explored, in addition to performance measures like
               typing speed, error rate and attended but not selected rate.
               Typing performance lowered with increased task difficulty, while
               blink frequency, duration and interval were higher for the
               difficult tasks. Pupil dilation indicated the memorization
               process, but did not demonstrate a difference between easy and
               difficult tasks.",
  series    = "ETRA '20 full papers",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379155.3391333",
  doi       = "10.1145/3379155.3391333",
  isbn      =  9781450371339
}

@INPROCEEDINGS{Freytag2020-pn,
  title     = "{Sweet Pursuit: User Acceptance and Performance of a Smooth
               Pursuit controlled Candy Dispensing Machine in a Public Setting}",
  author    = "Freytag, Sarah-Christin",
  booktitle = "{{ACM} Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--5",
  abstract  = "A prototypical smooth pursuit controlled candy dispensing machine
               was set up in a public area and evaluated regarding performance
               data, self reported joy of use, learnability, perceived stress
               and perceived usefulness. 359 sets of user data were collected
               from visitors ranging from eight to 75 years. The results show an
               overall high rate of successful interactions (89.8\%), indicating
               no correlation between height, age, gender or the use of
               corrective glasses or lenses and the ease and success of
               interaction. Incorrectly entered digits occurred for 36.2\% of
               all participants, with half attributing the error to their own
               incorrect entry and half reporting the system to have detected a
               false number. Users reported a generally high joy of use and
               found the system easy to learn. Users indicated interest to use
               similar interaction technologies in public settings if privacy
               requirements such as protection from observers, are met.",
  series    = "ETRA '20 Short Papers",
  month     =  jun,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379156.3391356",
  keywords  = "evaluation, user acceptance, UX, smooth pursuit, HCI, novel
               interaction technology, gaze-based interaction, gaze entry",
  doi       = "10.1145/3379156.3391356",
  isbn      =  9781450371346
}

@INPROCEEDINGS{Freytag2020-bo,
  title     = "{Sweet pursuit: User acceptance and performance of a smooth
               pursuit controlled candy dispensing machine in a public setting}",
  author    = "Freytag, Sarah-Christin",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–5",
  abstract  = "A prototypical smooth pursuit controlled candy dispensing machine
               was set up in a public area and evaluated regarding performance
               data, self reported joy of use, learnability, perceived stress
               and perceived usefulness. 359 sets of user data were collected
               from visitors ranging from eight to 75 years. The results show an
               overall high rate of successful interactions (89.8\%), indicating
               no correlation between height, age, gender or the use of
               corrective glasses or lenses and the ease and success of
               interaction. Incorrectly entered digits occurred for 36.2\% of
               all participants, with half attributing the error to their own
               incorrect entry and half reporting the system to have detected a
               false number. Users reported a generally high joy of use and
               found the system easy to learn. Users indicated interest to use
               similar interaction technologies in public settings if privacy
               requirements such as protection from observers, are met.",
  series    = "ETRA '20 short papers",
  month     =  jun,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379156.3391356",
  doi       = "10.1145/3379156.3391356",
  isbn      =  9781450371346
}

@INPROCEEDINGS{Hou2020-eq,
  title     = "{GIMIS: Gaze input with motor imagery selection}",
  author    = "Hou, Baosheng James and Bekgaard, Per and MacKenzie, Scott and
               Hansen, John Paulin Paulin and Puthusserypady, Sadasivan",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–10",
  abstract  = "A hybrid gaze and brain-computer interface (BCI) was developed to
               accomplish target selection in a Fitts' law experiment. The
               method, GIMIS, uses gaze input to steer the computer cursor for
               target pointing and motor imagery (MI) via the BCI to execute a
               click for target selection. An experiment (n = 15) compared three
               motor imagery selection methods: using the left-hand only, using
               the legs, and using either the left-hand or legs. The latter
               selection method (”either”) had the highest throughput (0.59
               bps), the fastest selection time (2650 ms), and an error rate of
               14.6\%. Pupil size significantly increased with increased target
               width. We recommend the use of large targets, which significantly
               reduced error rate, and the ”either” option for BCI selection,
               which significantly increased throughput. BCI selection is slower
               compared to dwell time selection, but if gaze control is
               deteriorating, for example in a late stage of the ALS disease,
               GIMIS may be a way to gradually introduce BCI.",
  series    = "ETRA '20 adjunct",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379157.3388932",
  file      = "All Papers/My Library/Hou et al. 2020 - GIMIS - Gaze input with motor imagery selection.pdf",
  doi       = "10.1145/3379157.3388932",
  isbn      =  9781450371353
}

@INPROCEEDINGS{Araujo2020-og,
  title     = "{Exploring eye-gaze wheelchair control}",
  author    = "Araujo, Jacopo M and Zhang, Guangtao and Hansen, John Paulin
               Paulin and Puthusserypady, Sadasivan",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Eye-gaze may potentially be used for steering wheelchairs or
               robots and thereby support independence in choosing where to
               move. This paper investigates the feasibility of gaze-controlled
               interfaces. We present an experiment with wheelchair control in a
               simulated, virtual reality (VR) driving experiment and a field
               study with five people using wheelchairs. In the VR experiment,
               three control interfaces were tested by 18 able-bodied subjects:
               (i) dwell buttons for direction commands on an overlay display,
               (ii) steering by continuous gaze point assessment on the ground
               plane in front of the driver, and (iii) waypoint navigation to
               targets placed on the ground plane. Results indicate that the
               waypoint method had superior performance, and it was also most
               preferred by the users, closely followed by the
               continuous-control interface. However, the field study revealed
               that our wheelchair users felt uncomfortable and excluded when
               they had to look down at the floor to steer a vehicle. Hence, our
               VR testing had a simplified representation of the steering task
               and ignored an important part of the use-context. In the
               discussion, we suggest potential improvements of simulation-based
               design of wheelchair gaze control interfaces.",
  series    = "ETRA '20 adjunct",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379157.3388933",
  file      = "All Papers/My Library/Araujo et al. 2020 - Exploring eye-gaze wheelchair control.pdf",
  doi       = "10.1145/3379157.3388933",
  isbn      =  9781450371353
}

@INPROCEEDINGS{Aldaqre2020-cm,
  title     = "{Looking outside the box: Gaze-pointing for augmentative and
               alternative communication}",
  author    = "Aldaqre, Iyad",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "Augmentative and Alternative Communication (AAC) provides
               different methods for people with disabilities to communicate. By
               employing eye tracking, these methods allowed the use of aided
               AAC autonomously, without the need for a caregiver to assist the
               user in choosing what they want to convey. However, these methods
               focus on verbal communication, which typically covers only a
               small portion of our daily communication. We present a system
               that can be integrated in modern AAC devices to allow users with
               impaired communication and mobility to take a picture of what is
               in front of them, zoom-in at a specific portion of the picture
               and share it with others. Such a simple solution could provide an
               alternative to pointing gestures, allowing users to express
               preferences to real objects in their environment. Other use cases
               of this system are discussed.",
  series    = "ETRA '20 adjunct",
  month     =  "6~" # feb,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379157.3391987",
  doi       = "10.1145/3379157.3391987",
  isbn      =  9781450371353
}

@INPROCEEDINGS{Costi2020-of,
  title     = "{CogniKit: An extensible tool for human cognitive modeling based
               on eye gaze analysis}",
  author    = "Costi, Andreas and Belk, Marios and Fidas, Christos and
               Constantinides, Argyris and Pitsillides, Andreas",
  booktitle = "{Proceedings of the 25th International Conference on Intelligent
               User Interfaces Companion}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "17~" # mar,
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3379336.3381460",
  file      = "All Papers/Other/Costi et al. 2020 - CogniKit - An extensible tool for human cognitive modeling based on eye gaze analysis.pdf",
  doi       = "10.1145/3379336.3381460",
  isbn      =  9781450375139
}

@INPROCEEDINGS{Seidenschwarz2020-kl,
  title     = "{The SportSense user interface for holistic tactical performance
               analysis in football}",
  author    = "Seidenschwarz, Philipp and Jonsson, Adalsteinn and Plüss, Michael
               and Rumo, Martin and Probst, Lukas and Schuldt, Heiko",
  booktitle = "{Proceedings of the 25th International Conference on Intelligent
               User Interfaces Companion}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "17~" # mar,
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3379336.3381473",
  doi       = "10.1145/3379336.3381473",
  isbn      =  9781450375139
}

@INPROCEEDINGS{Richardson2020-uj,
  title     = "{Decoding surface touch typing from hand-tracking}",
  author    = "Richardson, Mark and Durasoff, Matt and Wang, Robert",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "686–696",
  abstract  = "We propose a novel text decoding method that enables touch typing
               on an uninstrumented flat surface. Rather than relying on
               physical keyboards or capacitive touch, our method takes as input
               hand motion of the typist, obtained through hand-tracking, and
               decodes this motion directly into text. We use a temporal
               convolutional network to represent a motion model that maps the
               hand motion, represented as a sequence of hand pose features,
               into text characters. To enable touch typing without the haptic
               feedback of a physical keyboard, we had to address more erratic
               typing motion due to drift of the fingers. Thus, we incorporate a
               language model as a text prior and use beam search to efficiently
               combine our motion and language models to decode text from
               erratic or ambiguous hand motion. We collected a dataset of 20
               touch typists and evaluated our model on several baselines,
               including contact-based text decoding and typing on a physical
               keyboard. Our proposed method is able to leverage continuous hand
               pose information to decode text more accurately than
               contact-based methods and an offline study shows parity (73 WPM,
               2.38\% UER) with typing on a physical keyboard. Our results show
               that hand-tracking has the potential to enable rapid text entry
               in mobile environments.",
  series    = "UIST '20",
  month     =  "20~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379337.3415816",
  file      = "All Papers/My Library/Richardson et al. 2020 - Decoding surface touch typing from hand-tracking.pdf",
  doi       = "10.1145/3379337.3415816",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Park2020-hf,
  title     = "{DeepFisheye: Near-surface multi-finger tracking technology using
               fisheye camera}",
  author    = "Park, Keunwoo and Kim, Sunbum and Yoon, Youngwoo and Kim,
               Tae-Kyun and Lee, Geehyuk",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1132–1146",
  abstract  = "Near-surface multi-finger tracking (NMFT) technology expands the
               input space of touchscreens by enabling novel interactions such
               as mid-air and finger-aware interactions. We present DeepFisheye,
               a practical NMFT solution for mobile devices, that utilizes a
               fisheye camera attached at the bottom of a touchscreen.
               DeepFisheye acquires the image of an interacting hand positioned
               above the touchscreen using the camera and employs deep learning
               to estimate the 3D position of each fingertip. We created two new
               hand pose datasets comprising fisheye images, on which our
               network was trained. We evaluated DeepFisheye's performance for
               three device sizes. DeepFisheye showed average errors with
               approximate value of 20 mm for fingertip tracking across the
               different device sizes. Additionally, we created simple
               rule-based classifiers that estimate the contact finger and hand
               posture from DeepFisheye's output. The contact finger and hand
               posture classifiers showed accuracy of approximately 83 and 90\%,
               respectively, across the device sizes.",
  series    = "UIST '20",
  month     =  "20~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379337.3415818",
  doi       = "10.1145/3379337.3415818",
  isbn      =  9781450375146
}

@INCOLLECTION{Li2020-nc,
  title     = "{Romeo: A Design Tool for Embedding Transformable Parts in {3D}
               Models to Robotically Augment Default Functionalities}",
  author    = "Li, Jiahao and Cui, Meilin and Kim, Jeeeun and Chen, Xiang
               'anthony'",
  booktitle = "{Proceedings of the 33rd Annual {ACM} Symposium on User Interface
               Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "897--911",
  abstract  = "Reconfiguring shapes of objects enables transforming existing
               passive objects with robotic functionalities, e.g., a
               transformable coffee cup holder can be attached to a chair's
               armrest, a piggy bank can reach out an arm to 'steal' coins.
               Despite the advance in end-user 3D design and fabrication, it
               remains challenging for non-experts to create such
               'transformables' using existing tools due to the requirement of
               specific engineering knowledge such as mechanisms and robotic
               design. We present Romeo -- a design tool for creating
               transformables to robotically augment objects' default
               functionalities. Romeo allows users to transform an object into a
               robotic arm by expressing at a high level what type of task is
               expected. Users can select which part of the object to be
               transformed, specify motion points in space for the transformed
               part to follow and the corresponding action to be taken. Romeo
               then automatically generates a robotic arm embedded in the
               transformable part ready for fabrication. A design session
               validated this tool where participants used Romeo to accomplish
               controlled design tasks and to open-endedly create coin-stealing
               piggy banks by transforming 3D objects of their own choice.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415826",
  file      = "All Papers/Other/Li et al. 2020 - Romeo - A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities.pdf",
  doi       = "10.1145/3379337.3415826",
  isbn      =  9781450375146
}

@INCOLLECTION{Li2020-xi,
  title     = "{Romeo: A design tool for embedding transformable parts in 3D
               models to robotically augment default functionalities}",
  author    = "Li, Jiahao and Cui, Meilin and Kim, Jeeeun and Chen, Xiang
               'anthony'",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "897–911",
  abstract  = "Reconfiguring shapes of objects enables transforming existing
               passive objects with robotic functionalities, e.g., a
               transformable coffee cup holder can be attached to a chair's
               armrest, a piggy bank can reach out an arm to 'steal' coins.
               Despite the advance in end-user 3D design and fabrication, it
               remains challenging for non-experts to create such
               'transformables' using existing tools due to the requirement of
               specific engineering knowledge such as mechanisms and robotic
               design. We present Romeo – a design tool for creating
               transformables to robotically augment objects' default
               functionalities. Romeo allows users to transform an object into a
               robotic arm by expressing at a high level what type of task is
               expected. Users can select which part of the object to be
               transformed, specify motion points in space for the transformed
               part to follow and the corresponding action to be taken. Romeo
               then automatically generates a robotic arm embedded in the
               transformable part ready for fabrication. A design session
               validated this tool where participants used Romeo to accomplish
               controlled design tasks and to open-endedly create coin-stealing
               piggy banks by transforming 3D objects of their own choice.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415826",
  file      = "All Papers/My Library/Li et al. 2020 - Romeo - A design tool for embedding transformable parts in 3D models to robotically augment default functionalities.pdf",
  doi       = "10.1145/3379337.3415826",
  isbn      =  9781450375146
}

@INCOLLECTION{Nakagaki2020-zz,
  title     = "{{HERMITS}: Dynamically Reconfiguring the Interactivity of
               Self-propelled {TUIs} with Mechanical Shell Add-ons}",
  author    = "Nakagaki, Ken and Leong, Joanne and Tappa, Jordan L and Wilbert,
               João and Ishii, Hiroshi",
  booktitle = "{Proceedings of the 33rd Annual {ACM} Symposium on User Interface
               Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "882--896",
  abstract  = "We introduce HERMITS, a modular interaction architecture for
               self-propelled Tangible User Interfaces (TUIs) that incorporates
               physical add-ons, referred to as mechanical shells. The
               mechanical shell add-ons are intended to be dynamically
               reconfigured by utilizing the locomotion capability of
               self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed
               a proof-of-concept system that demonstrates this novel
               architecture using two-wheeled robots and a variety of mechanical
               shell examples. These mechanical shell add-ons are passive
               physical attatchments that extend the primitive interactivities
               (e.g. shape, motion and light) of the self-propelled robots. The
               paper proposes the architectural design, interactive
               functionality of HERMITS as well as design primitives for
               mechanical shells. The paper also introduces the prototype
               implementation that is based on an off-the-shelf robotic toy with
               a modified docking mechanism. A range of applications is
               demonstrated with the prototype to motivate the collective and
               dynamically reconfigurable capability of the modular
               architecture, such as an interactive mobility simulation, an
               adaptive home/desk environment, and a story-telling narrative.
               Lastly, we discuss the future research opportunity of HERMITS to
               enrich the interactivity and adaptability of actuated and shape
               changing TUIs.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415831",
  file      = "All Papers/Other/Nakagaki et al. 2020 - HERMITS - Dynamically Reconfiguring the Interactivity of Self-propelled TUIs with Mechanical Shell Add-ons.pdf",
  doi       = "10.1145/3379337.3415831",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Nakagaki2020-cj,
  title     = "{HERMITS}",
  author    = "Nakagaki, Ken and Leong, Joanne and Tappa, Jordan L and Wilbert,
               João and Ishii, Hiroshi",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3379337.3415831",
  file      = "All Papers/My Library/Nakagaki et al. 2020 - HERMITS.pdf",
  doi       = "10.1145/3379337.3415831"
}

@INCOLLECTION{Nakagaki2020-cw,
  title     = "{HERMITS: Dynamically reconfiguring the interactivity of
               self-propelled TUIs with mechanical shell add-ons}",
  author    = "Nakagaki, Ken and Leong, Joanne and Tappa, Jordan L and Wilbert,
               João and Ishii, Hiroshi",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "882–896",
  abstract  = "We introduce HERMITS, a modular interaction architecture for
               self-propelled Tangible User Interfaces (TUIs) that incorporates
               physical add-ons, referred to as mechanical shells. The
               mechanical shell add-ons are intended to be dynamically
               reconfigured by utilizing the locomotion capability of
               self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed
               a proof-of-concept system that demonstrates this novel
               architecture using two-wheeled robots and a variety of mechanical
               shell examples. These mechanical shell add-ons are passive
               physical attatchments that extend the primitive interactivities
               (e.g. shape, motion and light) of the self-propelled robots. The
               paper proposes the architectural design, interactive
               functionality of HERMITS as well as design primitives for
               mechanical shells. The paper also introduces the prototype
               implementation that is based on an off-the-shelf robotic toy with
               a modified docking mechanism. A range of applications is
               demonstrated with the prototype to motivate the collective and
               dynamically reconfigurable capability of the modular
               architecture, such as an interactive mobility simulation, an
               adaptive home/desk environment, and a story-telling narrative.
               Lastly, we discuss the future research opportunity of HERMITS to
               enrich the interactivity and adaptability of actuated and shape
               changing TUIs.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415831",
  file      = "All Papers/My Library/Nakagaki et al. 2020 - HERMITS - Dynamically reconfiguring the interactivity of self-propelled TUIs with mechanical shell add-ons.pdf",
  doi       = "10.1145/3379337.3415831",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Hartmann2020-lb,
  title     = "{{AAR}: Augmenting a Wearable Augmented Reality Display with an
               Actuated {Head-Mounted} Projector}",
  author    = "Hartmann, Jeremy and Yeh, Yen-Ting and Vogel, Daniel",
  booktitle = "{Proceedings of the 33rd Annual {ACM} Symposium on User Interface
               Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "445--458",
  abstract  = "Current wearable AR devices create an isolated experience with a
               limited field of view, vergence-accommodation conflicts, and
               difficulty communicating the virtual environment to observers. To
               address these issues and enable new ways to visualize,
               manipulate, and share virtual content, we introduce Augmented
               Augmented Reality (AAR) by combining a wearable AR display with a
               wearable spatial augmented reality projector. To explore this
               idea, a system is constructed to combine a head-mounted actuated
               pico projector with a Hololens AR headset. Projector calibration
               uses a modified structure from motion pipeline to reconstruct the
               geometric structure of the pan-tilt actuator axes and offsets. A
               toolkit encapsulates a set of high-level functionality to manage
               content placement relative to each augmented display and the
               physical environment. Demonstrations showcase ways to utilize the
               projected and head-mounted displays together, such as expanding
               field of view, distributing content across depth surfaces, and
               enabling bystander collaboration.",
  series    = "UIST '20",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415849",
  keywords  = "calibration, asymmetric interaction, spatial augmented reality,
               augmented reality, projection mapping;ar/mr",
  doi       = "10.1145/3379337.3415849",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Hartmann2020-mc,
  title     = "{AAR: Augmenting a wearable augmented reality display with an
               actuated Head-Mounted projector}",
  author    = "Hartmann, Jeremy and Yeh, Yen-Ting and Vogel, Daniel",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "445–458",
  abstract  = "Current wearable AR devices create an isolated experience with a
               limited field of view, vergence-accommodation conflicts, and
               difficulty communicating the virtual environment to observers. To
               address these issues and enable new ways to visualize,
               manipulate, and share virtual content, we introduce Augmented
               Augmented Reality (AAR) by combining a wearable AR display with a
               wearable spatial augmented reality projector. To explore this
               idea, a system is constructed to combine a head-mounted actuated
               pico projector with a Hololens AR headset. Projector calibration
               uses a modified structure from motion pipeline to reconstruct the
               geometric structure of the pan-tilt actuator axes and offsets. A
               toolkit encapsulates a set of high-level functionality to manage
               content placement relative to each augmented display and the
               physical environment. Demonstrations showcase ways to utilize the
               projected and head-mounted displays together, such as expanding
               field of view, distributing content across depth surfaces, and
               enabling bystander collaboration.",
  series    = "UIST '20",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415849",
  doi       = "10.1145/3379337.3415849",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Brudy2020-gq,
  title     = "{{SurfaceFleet}: Exploring distributed interactions unbounded
               from device, application, user, and time}",
  author    = "Brudy, Frederik and Ledo, David and Pahud, Michel and Henry
               Riche, Nathalie and Holz, Christian and Waghmare, Anand and
               Surale, Hemant Bhaskar and Peinado, Marcus and Zhang, Xiaokuan
               and Joyner, Shannon and Chandramouli, Badrish and Minhas, Umar
               Farooq and Goldstein, Jonathan and Buxton, William and Hinckley,
               Ken",
  booktitle = "{Proceedings of the 33rd Annual {ACM} Symposium on User Interface
               Software and Technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415874",
  doi       = "10.1145/3379337.3415874",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Brudy2020-pv,
  title     = "{SurfaceFleet: Exploring distributed interactions unbounded from
               device, application, user, and time}",
  author    = "Brudy, Frederik and Ledo, David and Pahud, Michel and Henry
               Riche, Nathalie and Holz, Christian and Waghmare, Anand and
               Surale, Hemant Bhaskar and Peinado, Marcus and Zhang, Xiaokuan
               and Joyner, Shannon and Chandramouli, Badrish and Minhas, Umar
               Farooq and Goldstein, Jonathan and Buxton, William and Hinckley,
               Ken",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379337.3415874",
  doi       = "10.1145/3379337.3415874",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Kim2020-oi,
  title     = "{OddEyeCam: A sensing technique for body-centric peephole
               interaction using WFoV RGB and NFoV depth cameras}",
  author    = "Kim, Daehwa and Park, Keunwoo and Lee, Geehyuk",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "85–97",
  abstract  = "The space around the body not only expands the interaction space
               of a mobile device beyond its small screen, but also enables
               users to utilize their kinesthetic sense. Therefore, body-centric
               peephole interaction has gained considerable attention. To
               support its practical implementation, we propose OddEyeCam, which
               is a vision-based method that tracks the 3D location of a mobile
               device in an absolute, wide, and continuous manner with respect
               to the body of a user in both static and mobile environments.
               OddEyeCam tracks the body of a user using a wide-view RGB camera
               and obtains precise depth information using a narrow-view depth
               camera from a smartphone close to the body. We quantitatively
               evaluated OddEyeCam through an accuracy test and two user
               studies. The accuracy test showed the average tracking accuracy
               of OddEyeCam was 4.17 and 4.47cm in 3D space when a participant
               is standing and walking, respectively. In the frst user study, we
               implemented various interaction scenarios and observed that
               OddEyeCam was well received by the participants. In the second
               user study, we observed that the peephole target acquisition task
               performed using our system followed Fitts? law. We also analyzed
               the performance of OddEyeCam using the obtained measurements and
               observed that the participants completed the tasks with suffcient
               speed and accuracy.",
  series    = "UIST '20",
  month     =  "20~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379337.3415889",
  doi       = "10.1145/3379337.3415889",
  isbn      =  9781450375146
}

@INPROCEEDINGS{Suzuki2020-ho,
  title     = "{RealitySketch}",
  author    = "Suzuki, Ryo and Kazi, Rubaiat Habib and Wei, Li-Yi and DiVerdi,
               Stephen and Li, Wilmot and Leithinger, Daniel",
  booktitle = "{Proceedings of the 33rd annual ACM symposium on user interface
               software and technology}",
  publisher = "ACM",
  month     =  oct,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3379337.3415892",
  file      = "All Papers/My Library/Suzuki et al. 2020 - RealitySketch.pdf",
  doi       = "10.1145/3379337.3415892"
}

@INPROCEEDINGS{Kurauchi2020-cx,
  title     = "{{Swipe\&Switch}: Text Entry Using Gaze Paths and Context
               Switching}",
  author    = "Kurauchi, Andrew and Feng, Wenxin and Joshi, Ajjen and Morimoto,
               Carlos H and Betke, Margrit",
  booktitle = "{Adjunct Publication of the 33rd Annual {ACM} Symposium on User
               Interface Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "84--86",
  abstract  = "Swipe-based methods for text entry by gaze allow users to swipe
               through the letters of a word by gaze, analogous to how they can
               swipe with a finger on a touchscreen keyboard. Two challenges for
               these methods are: (1) gaze paths do not possess clear start and
               end positions, and (2) it is difficult to design text editing
               features. We introduce Swipe\&Switch, a text-entry interface that
               uses swiping and switching to improve gaze-based interaction. The
               interface contains three context regions, and detects the
               start/end of a gesture and emits text editing commands (e.g.,
               word insertion, deletion) when a user switches focus between
               these regions. A user study showed that Swipe\&Switch provides a
               better user experience and higher text entry rate over a
               baseline, EyeSwipe.",
  series    = "UIST '20 Adjunct",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379350.3416193",
  keywords  = "gaze swiping, gesture-based typing, text entry, eye tracking, eye
               typing;prj-gaze-shorthand",
  doi       = "10.1145/3379350.3416193",
  isbn      =  9781450375153
}

@INPROCEEDINGS{Kurauchi2020-ym,
  title     = "{Swipe\&Switch: Text entry using gaze paths and context
               switching}",
  author    = "Kurauchi, Andrew and Feng, Wenxin and Joshi, Ajjen and Morimoto,
               Carlos H and Betke, Margrit",
  booktitle = "{Adjunct publication of the 33rd annual ACM symposium on user
               interface software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "84–86",
  abstract  = "Swipe-based methods for text entry by gaze allow users to swipe
               through the letters of a word by gaze, analogous to how they can
               swipe with a finger on a touchscreen keyboard. Two challenges for
               these methods are: (1) gaze paths do not possess clear start and
               end positions, and (2) it is difficult to design text editing
               features. We introduce Swipe\&Switch, a text-entry interface that
               uses swiping and switching to improve gaze-based interaction. The
               interface contains three context regions, and detects the
               start/end of a gesture and emits text editing commands (e.g.,
               word insertion, deletion) when a user switches focus between
               these regions. A user study showed that Swipe\&Switch provides a
               better user experience and higher text entry rate over a
               baseline, EyeSwipe.",
  series    = "UIST '20 adjunct",
  month     =  "20~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145/3379350.3416193",
  doi       = "10.1145/3379350.3416193",
  isbn      =  9781450375153
}

@INPROCEEDINGS{Rietz2020-hm,
  title     = "{Cody}",
  author    = "Rietz, Tim and Toreini, Peyman and Maedche, Alexander",
  booktitle = "{Adjunct publication of the 33rd annual ACM symposium on user
               interface software and technology}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "20~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3379350.3416195",
  doi       = "10.1145/3379350.3416195"
}

@INPROCEEDINGS{Tausif2020-hd,
  title     = "{Towards Enabling Eye Contact and Perspective Control in Video
               Conference}",
  author    = "Tausif, Md Tahsin and Weaver, R J and Lee, Sang Won",
  booktitle = "{Adjunct Publication of the 33rd Annual {ACM} Symposium on User
               Interface Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "96--98",
  abstract  = "Eye contact is an important part of in-person communication.
               However, in modern remote communication - video conference calls,
               non-verbal cues through eye contact are lost. This is because it
               is not possible to make eye contact in a video call; to make it
               look like a person is making an eye contact, the user has to look
               at the camera directly, but then this means that the user is not
               looking at the visual of the interlocutor on the screen. In our
               research, we aim to build a hardware and software system that
               helps the users to make eye contact in video conference and
               change their perspective, placing a moving camera behind a
               semi-transparent screen. The system keeps the position of camera
               right behind the remote user's eye position on screen tracked by
               a computer vision algorithm, so users will be able to make eye
               contact during the video call. We believe the system will lead to
               better user experience in remote conference calls. In this work,
               we share our motivation, design choices, the current progress,
               and a plan to evaluate the system.",
  series    = "UIST '20 Adjunct",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379350.3416197",
  keywords  = "video call, mutual gaze awareness, remote
               communication;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/3379350.3416197",
  isbn      =  9781450375153
}

@INPROCEEDINGS{Tausif2020-tu,
  title     = "{Towards enabling eye contact and perspective control in video
               conference}",
  author    = "Tausif, Md Tahsin and Weaver, R J and Lee, Sang Won",
  booktitle = "{Adjunct publication of the 33rd annual ACM symposium on user
               interface software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "96–98",
  abstract  = "Eye contact is an important part of in-person communication.
               However, in modern remote communication - video conference calls,
               non-verbal cues through eye contact are lost. This is because it
               is not possible to make eye contact in a video call; to make it
               look like a person is making an eye contact, the user has to look
               at the camera directly, but then this means that the user is not
               looking at the visual of the interlocutor on the screen. In our
               research, we aim to build a hardware and software system that
               helps the users to make eye contact in video conference and
               change their perspective, placing a moving camera behind a
               semi-transparent screen. The system keeps the position of camera
               right behind the remote user's eye position on screen tracked by
               a computer vision algorithm, so users will be able to make eye
               contact during the video call. We believe the system will lead to
               better user experience in remote conference calls. In this work,
               we share our motivation, design choices, the current progress,
               and a plan to evaluate the system.",
  series    = "UIST '20 adjunct",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3379350.3416197",
  doi       = "10.1145/3379350.3416197",
  isbn      =  9781450375153
}

@INPROCEEDINGS{Arakawa2020-lm,
  title     = "{Hand with sensing sphere: Body-centered spatial interactions
               with a hand-worn spherical camera}",
  author    = "Arakawa, Riku and Maekawa, Azumi and Kashino, Zendai and Inami,
               Masahiko",
  booktitle = "{Symposium on spatial user interaction}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "31~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3385959.3418450",
  doi       = "10.1145/3385959.3418450",
  isbn      =  9781450379434
}

@INPROCEEDINGS{Hartmann2020-bs,
  title     = "{Extend, push, pull: Smartphone mediated interaction in spatial
               augmented reality via intuitive mode switching}",
  author    = "Hartmann, Jeremy and Gupta, Aakar and Vogel, Daniel",
  booktitle = "{Symposium on spatial user interaction}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "31~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3385959.3418456",
  doi       = "10.1145/3385959.3418456",
  isbn      =  9781450379434
}

@INPROCEEDINGS{Akers2020-kk,
  title     = "{Mixed reality spatial computing in a remote learning classroom}",
  author    = "Akers, John and Zimmermann, Joelle and Trutoiu, Laura and
               Schowengerdt, Brian and Kemelmacher-Shlizerman, Ira",
  booktitle = "{Symposium on spatial user interaction}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "31~" # oct,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3385959.3422705",
  doi       = "10.1145/3385959.3422705",
  isbn      =  9781450379434
}

@INPROCEEDINGS{Antoniadi2020-ll,
  title     = "{Using patient information for the prediction of caregiver burden
               in amyotrophic lateral sclerosis}",
  author    = "Antoniadi, Anna and Galvin, Miriam and Heverin, Mark and
               Hardiman, Orla and Mooney, Catherine",
  booktitle = "{Proceedings of the 11th ACM international conference on
               bioinformatics, computational biology and health informatics}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     =  1,
  abstract  = "The aim of this study is to create a Clinical Decision Support
               System (CDSS) to assist in the early identification and support
               of caregivers at risk of experiencing burden while caring for a
               person with Amyotrophic Lateral Sclerosis. We work towards a
               system that uses a minimum amount of data that could be routinely
               collected. We investigated if the impairment of patients alone
               provides sufficient information for the prediction of caregiver
               burden. Results reveal a better performance of our system in
               identifying those at risk of high burden, but more information is
               needed for an accurate CDSS.",
  series    = "BCB '20",
  month     =  "21~" # sep,
  year      =  2020,
  url       = "https://doi.org/10.1145/3388440.3414908",
  file      = "All Papers/My Library/Antoniadi et al. 2020 - Using patient information for the prediction of caregiver burden in amyotrophic lateral sclerosis.pdf",
  doi       = "10.1145/3388440.3414908",
  isbn      =  9781450379649
}

@INCOLLECTION{Xia2020-vx,
  title     = "{Controllable Continuous Gaze Redirection}",
  author    = "Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao and Feng, Wensen",
  booktitle = "{Proceedings of the 28th {ACM} International Conference on
               Multimedia}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1782--1790",
  abstract  = "In this work, we present interpGaze, a novel framework for
               controllable gaze redirection that achieves both precise
               redirection and continuous interpolation. Given two gaze images
               with different attributes, our goal is to redirect the eye gaze
               of one person into any gaze direction depicted in the reference
               image or to generate continuous intermediate results. To
               accomplish this, we design a model including three cooperative
               components: an encoder, a controller and a decoder. The encoder
               maps images into a well-disentangled and hierarchically-organized
               latent space. The controller adjusts the magnitudes of latent
               vectors to the desired strength of corresponding attributes by
               altering a control vector. The decoder converts the desired
               representations from the attribute space to the image space. To
               facilitate covering the full space of gaze directions, we
               introduce a high-quality gaze image dataset with a large range of
               directions, which also benefits researchers in related areas.
               Extensive experimental validation and comparisons to several
               baseline methods show that the proposed interpGaze outperforms
               state-of-the-art methods in terms of image quality and
               redirection precision.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3394171.3413868",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/3394171.3413868",
  isbn      =  9781450379885
}

@INCOLLECTION{Xia2020-mn,
  title     = "{Controllable continuous gaze redirection}",
  author    = "Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao and Feng, Wensen",
  booktitle = "{Proceedings of the 28th ACM international conference on
               multimedia}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1782–1790",
  abstract  = "In this work, we present interpGaze, a novel framework for
               controllable gaze redirection that achieves both precise
               redirection and continuous interpolation. Given two gaze images
               with different attributes, our goal is to redirect the eye gaze
               of one person into any gaze direction depicted in the reference
               image or to generate continuous intermediate results. To
               accomplish this, we design a model including three cooperative
               components: an encoder, a controller and a decoder. The encoder
               maps images into a well-disentangled and hierarchically-organized
               latent space. The controller adjusts the magnitudes of latent
               vectors to the desired strength of corresponding attributes by
               altering a control vector. The decoder converts the desired
               representations from the attribute space to the image space. To
               facilitate covering the full space of gaze directions, we
               introduce a high-quality gaze image dataset with a large range of
               directions, which also benefits researchers in related areas.
               Extensive experimental validation and comparisons to several
               baseline methods show that the proposed interpGaze outperforms
               state-of-the-art methods in terms of image quality and
               redirection precision.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3394171.3413868",
  file      = "All Papers/My Library/Xia et al. 2020 - Controllable continuous gaze redirection.pdf",
  doi       = "10.1145/3394171.3413868",
  isbn      =  9781450379885
}

@INPROCEEDINGS{Zhang2020-en,
  title     = "{Dual in-painting model for unsupervised gaze correction and
               animation in the wild}",
  author    = "Zhang, Jichao and Chen, Jingjing and Tang, Hao and Wang, Wei and
               Yan, Yan and Sangineto, Enver and Sebe, Nicu",
  booktitle = "{Proceedings of the 28th {ACM} International Conference on
               Multimedia}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This work addresses the problem of unsupervised gaze correction
               in the wild, presenting a solution that works without the need of
               precise annotations of the gaze angle and the head pose and
               proposes the PAM with an Autoencoder, which is based on
               Self-Supervised mirror learning where the bottleneck features are
               angle-invariant and which works as an extra input to the dual
               in-painting models. We address the problem of unsupervised gaze
               correction in the wild, presenting a solution that works without
               the need of precise annotations of the gaze angle and the head
               pose. We created a new dataset called CelebAGaze consisting of
               two domains X, Y, where the eyes are either staring at the camera
               or somewhere else. Our method consists of three novel modules:
               the Gaze Correction module(GCM), the Gaze Animation module(GAM),
               and the Pretrained Autoencoder module (PAM). Specifically, GCM
               and GAM separately train a dual in-painting network using data
               from the domain X for gaze correction and data from the domain Y
               for gaze animation. Additionally, a Synthesis-As-Training method
               is proposed when training GAM to encourage the features encoded
               from the eye region to be correlated with the angle information,
               resulting in gaze animation achieved by interpolation in the
               latent space. To further preserve the identity information e.g.,
               eye shape, iris color, we propose the PAM with an Autoencoder,
               which is based on Self-Supervised mirror learning where the
               bottleneck features are angle-invariant and which works as an
               extra input to the dual in-painting models. Extensive experiments
               validate the effectiveness of the proposed method for gaze
               correction and gaze animation in the wild and demonstrate the
               superiority of our approach in producing more compelling results
               than state-of-the-art baselines. Our code, the pretrained models
               and supplementary results are available
               at:https://github.com/zhangqianhui/GazeAnimation.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3394171.3413981",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1145/3394171.3413981",
  isbn      =  9781450379885,
  language  = "en"
}

@INPROCEEDINGS{Zhang2020-dw,
  title     = "{Dual in-painting model for unsupervised gaze correction and
               animation in the wild}",
  author    = "Zhang, Jichao and Chen, Jingjing and Tang, Hao and Wang, Wei and
               Yan, Yan and Sangineto, Enver and Sebe, Nicu",
  booktitle = "{Proceedings of the 28th ACM international conference on
               multimedia}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This work addresses the problem of unsupervised gaze correction
               in the wild, presenting a solution that works without the need of
               precise annotations of the gaze angle and the head pose and
               proposes the PAM with an Autoencoder, which is based on
               Self-Supervised mirror learning where the bottleneck features are
               angle-invariant and which works as an extra input to the dual
               in-painting models. We address the problem of unsupervised gaze
               correction in the wild, presenting a solution that works without
               the need of precise annotations of the gaze angle and the head
               pose. We created a new dataset called CelebAGaze consisting of
               two domains X, Y, where the eyes are either staring at the camera
               or somewhere else. Our method consists of three novel modules:
               the Gaze Correction module(GCM), the Gaze Animation module(GAM),
               and the Pretrained Autoencoder module (PAM). Specifically, GCM
               and GAM separately train a dual in-painting network using data
               from the domain X for gaze correction and data from the domain Y
               for gaze animation. Additionally, a Synthesis-As-Training method
               is proposed when training GAM to encourage the features encoded
               from the eye region to be correlated with the angle information,
               resulting in gaze animation achieved by interpolation in the
               latent space. To further preserve the identity information e.g.,
               eye shape, iris color, we propose the PAM with an Autoencoder,
               which is based on Self-Supervised mirror learning where the
               bottleneck features are angle-invariant and which works as an
               extra input to the dual in-painting models. Extensive experiments
               validate the effectiveness of the proposed method for gaze
               correction and gaze animation in the wild and demonstrate the
               superiority of our approach in producing more compelling results
               than state-of-the-art baselines. Our code, the pretrained models
               and supplementary results are available
               at:https://github.com/zhangqianhui/GazeAnimation.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3394171.3413981",
  file      = "All Papers/My Library/Zhang et al. 2020 - Dual in-painting model for unsupervised gaze correction and animation in the wild.pdf",
  doi       = "10.1145/3394171.3413981",
  isbn      =  9781450379885,
  language  = "en"
}

@INPROCEEDINGS{Papakostas2021-xg,
  title     = "{Understanding driving distractions: A multimodal analysis on
               distraction characterization}",
  author    = "Papakostas, Michalis and Riani, Kais and Gasiorowski, Andrew
               Brian and Sun, Yan and Abouelenien, Mohamed and Mihalcea, Rada
               and Burzo, Mihai",
  booktitle = "{26th International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "14~" # apr,
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3397481.3450635",
  doi       = "10.1145/3397481.3450635",
  isbn      =  9781450380171
}

@INPROCEEDINGS{Heck2021-ce,
  title     = "{The subconscious director: Dynamically personalizing videos
               using gaze data}",
  author    = "Heck, Melanie and Edinger, Janick and Bünemann, Jonathan and
               Becker, Christian",
  booktitle = "{26th International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "14~" # apr,
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3397481.3450679",
  file      = "All Papers/Other/Heck et al. 2021 - The subconscious director - Dynamically personalizing videos using gaze data.pdf",
  doi       = "10.1145/3397481.3450679",
  isbn      =  9781450380171
}

@INCOLLECTION{Ryskeldiev2021-jp,
  title     = "{Immersive Inclusivity at {CHI}: Design and Creation of Inclusive
               User Interactions Through Immersive Media}",
  author    = "Ryskeldiev, Bektur and Ochiai, Yoichi and Kusano, Koki and Li,
               Jie and Saraiji, Yamen and Kunze, Kai and Billinghurst, Mark and
               Nanayakkara, Suranga and Sugano, Yusuke and Honda, Tatsuya",
  booktitle = "{Extended Abstracts of the 2021 {CHI} Conference on Human Factors
               in Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--4",
  abstract  = "Immersive media is becoming increasingly common in day-to-day
               scenarios: from extended reality systems to multimodal
               interfaces. Such ubiquity opens an opportunity for building more
               inclusive environments for users with disabilities (permanent,
               temporary, or situational) by either introducing immersive and
               multimodal elements into existing applications, or designing and
               creating immersive applications with inclusivity in mind. Thus
               the aim of this workshop is to create a discussion platform on
               intersections between the fields of immersive media,
               accessibility, and human-computer interaction, outline the key
               current and future problems of immersive inclusive design, and
               define a set of methodologies for design and evaluation of
               immersive systems from inclusivity perspective.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411763.3441322",
  keywords  = "spatial computing",
  doi       = "10.1145/3411763.3441322",
  isbn      =  9781450380959
}

@INCOLLECTION{Ryskeldiev2021-nh,
  title     = "{Immersive inclusivity at CHI: Design and creation of inclusive
               user interactions through immersive media}",
  author    = "Ryskeldiev, Bektur and Ochiai, Yoichi and Kusano, Koki and Li,
               Jie and Saraiji, Yamen and Kunze, Kai and Billinghurst, Mark and
               Nanayakkara, Suranga and Sugano, Yusuke and Honda, Tatsuya",
  booktitle = "{Extended abstracts of the 2021 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–4",
  abstract  = "Immersive media is becoming increasingly common in day-to-day
               scenarios: from extended reality systems to multimodal
               interfaces. Such ubiquity opens an opportunity for building more
               inclusive environments for users with disabilities (permanent,
               temporary, or situational) by either introducing immersive and
               multimodal elements into existing applications, or designing and
               creating immersive applications with inclusivity in mind. Thus
               the aim of this workshop is to create a discussion platform on
               intersections between the fields of immersive media,
               accessibility, and human-computer interaction, outline the key
               current and future problems of immersive inclusive design, and
               define a set of methodologies for design and evaluation of
               immersive systems from inclusivity perspective.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411763.3441322",
  file      = "All Papers/My Library/Ryskeldiev et al. 2021 - Immersive inclusivity at CHI - Design and creation of inclusive user interactions through immersive media.pdf",
  doi       = "10.1145/3411763.3441322",
  isbn      =  9781450380959
}

@INCOLLECTION{Ryskeldiev2021-rq,
  title     = "{Creative Immersive {AI:Emerging} Challenges and Opportunities
               forCreative Applications of {AI} in Immersive Media}",
  author    = "Ryskeldiev, Bektur and Ilić, Suzana and Ochiai, Yoichi and
               Elliott, Luba and Nikonole, Helena and Billinghurst, Mark",
  booktitle = "{Extended Abstracts of the 2021 {CHI} Conference on Human Factors
               in Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--3",
  abstract  = "Over the past several years artificial intelligence (AI)
               techniques have gained a considerable presence in immersive
               media. From assistance with real-time digital production to
               emerging novel forms of creative expression, applications of AI
               are becoming ubiquitous in the fields of human-computer
               interaction (HCI), computer graphics, and media art. As we are
               interested in how novel computational techniques are shaping the
               state of creativity in immersive and interactive technologies, we
               organize this special interest group (SIG) to stimulate a
               discussion among AI, HCI, immersive media, and art communities.
               The goal of this SIG includes outlining existing and emerging
               areas of cross-disciplinary collaborations, proposing a roadmap
               of future goals and challenges for creative immersive AI
               research, and establishing a diverse group of researchers and
               practitioners involved in creative applications of AI in
               immersive and interactive media.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411763.3450399",
  keywords  = "spatial computing",
  doi       = "10.1145/3411763.3450399",
  isbn      =  9781450380959
}

@INCOLLECTION{Ryskeldiev2021-tl,
  title     = "{Creative immersive AI:Emerging challenges and opportunities
               forCreative applications of AI in immersive media}",
  author    = "Ryskeldiev, Bektur and Ilić, Suzana and Ochiai, Yoichi and
               Elliott, Luba and Nikonole, Helena and Billinghurst, Mark",
  booktitle = "{Extended abstracts of the 2021 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–3",
  abstract  = "Over the past several years artificial intelligence (AI)
               techniques have gained a considerable presence in immersive
               media. From assistance with real-time digital production to
               emerging novel forms of creative expression, applications of AI
               are becoming ubiquitous in the fields of human-computer
               interaction (HCI), computer graphics, and media art. As we are
               interested in how novel computational techniques are shaping the
               state of creativity in immersive and interactive technologies, we
               organize this special interest group (SIG) to stimulate a
               discussion among AI, HCI, immersive media, and art communities.
               The goal of this SIG includes outlining existing and emerging
               areas of cross-disciplinary collaborations, proposing a roadmap
               of future goals and challenges for creative immersive AI
               research, and establishing a diverse group of researchers and
               practitioners involved in creative applications of AI in
               immersive and interactive media.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411763.3450399",
  doi       = "10.1145/3411763.3450399",
  isbn      =  9781450380959
}

@INPROCEEDINGS{Kimura2021-fl,
  title     = "{Mobile, hands-free, silent speech texting using {SilentSpeller}}",
  author    = "Kimura, Naoki and Gemicioglu, Tan and Womack, Jonathan and Li,
               Richard and Zhao, Yuhui and Bedri, Abdelkareem and Olwal, Alex
               and Rekimoto, Jun and Starner, Thad",
  booktitle = "{Extended abstracts of the 2021 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–5",
  abstract  = "Voice control provides hands-free access to computing, but there
               are many situations where audible speech is not appropriate. Most
               unvoiced speech text entry systems can not be used while
               on-the-go due to movement artifacts. SilentSpeller enables mobile
               silent texting using a dental retainer with capacitive touch
               sensors to track tongue movement. Users type by spelling words
               without voicing. In offline isolated word testing on a 1164-word
               dictionary, SilentSpeller achieves an average 97\% character
               accuracy. 97\% offline accuracy is also achieved on phrases
               recorded while walking or seated. Live text entry achieves up to
               53 words per minute and 90\% accuracy, which is competitive with
               expert text entry on mini-QWERTY keyboards without encumbering
               the hands.",
  series    = "CHI EA '21",
  month     =  "5~" # aug,
  year      =  2021,
  url       = "https://doi.org/10.1145/3411763.3451552",
  doi       = "10.1145/3411763.3451552",
  isbn      =  9781450380959
}

@INPROCEEDINGS{McHugh2021-kz,
  title     = "{Towards inclusive streaming: Building multimodal music
               experiences for the deaf and hard of hearing}",
  author    = "McHugh, Thomas Barlow and Saha, Abir and Bar-El, David and
               Worsley, Marcelo and Piper, Anne Marie",
  booktitle = "{Extended abstracts of the 2021 CHI conference on human factors
               in computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # aug,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411763.3451690",
  doi       = "10.1145/3411763.3451690",
  isbn      =  9781450380959
}

@INPROCEEDINGS{Elmadjian2021-ny,
  title     = "{GazeBar: Exploiting the midas touch in gaze interaction}",
  author    = "Elmadjian, Carlos and Morimoto, Carlos H",
  booktitle = "{Extended abstracts of the 2021 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Imagine an application that requires constant configuration
               changes, such as modifying the brush type in a drawing
               application. Typically, options are hierarchically organized in
               menu bars that the user must navigate, sometimes through several
               levels, to select the desired mode. An alternative to reduce hand
               motion is the use of multimodal techniques such as gaze-touch,
               that combines gaze pointing with mechanical selection. In this
               paper, we introduce GazeBar, a novel multimodal gaze interaction
               technique that uses gaze paths as a combined pointing and
               selection mechanism. The idea behind GazeBar is to maximize the
               interaction flow by reducing ”safety” mechanisms (such as
               clicking) under certain circumstances. We present GazeBar's
               design and demonstrate it using a digital drawing application
               prototype. Advantages and disadvantages of GazeBar are discussed
               based on a user performance model.",
  series    = "CHI EA '21",
  month     =  "5~" # aug,
  year      =  2021,
  url       = "https://doi.org/10.1145/3411763.3451703",
  file      = "All Papers/My Library/Elmadjian and Morimoto 2021 - GazeBar - Exploiting the midas touch in gaze interaction.pdf",
  doi       = "10.1145/3411763.3451703",
  isbn      =  9781450380959
}

@INPROCEEDINGS{Kim2021-am,
  title     = "{``I can't talk now'': Speaking with voice output communication
               aid using text-to-speech synthesis during multiparty video
               conference}",
  author    = "Kim, Wooseok and Lee, Sangsu",
  booktitle = "{Extended Abstracts of the 2021 {CHI} Conference on Human Factors
               in Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411763.3451745",
  doi       = "10.1145/3411763.3451745",
  isbn      =  9781450380959
}

@INPROCEEDINGS{Kim2021-yv,
  title     = "{“I can't talk now”: Speaking with voice output communication aid
               using text-to-speech synthesis during multiparty video
               conference}",
  author    = "Kim, Wooseok and Lee, Sangsu",
  booktitle = "{Extended abstracts of the 2021 CHI conference on human factors
               in computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411763.3451745",
  doi       = "10.1145/3411763.3451745",
  isbn      =  9781450380959
}

@INPROCEEDINGS{Danyluk2021-wh,
  title     = "{A Design Space Exploration of Worlds in Miniature}",
  author    = "Danyluk, Kurtis and Ens, Barrett and Jenny, Bernhard and Willett,
               Wesley",
  booktitle = "{Proceedings of the 2021 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--15",
  abstract  = "Worlds-in-Miniature (WiMs) are interactive worlds within a world
               and combine the advantages of an input space, a cartographic map,
               and an overview+detail interface. They have been used across the
               extended virtuality spectrum for a variety of applications.
               Building on an analysis of examples of WiMs from the research
               literature we contribute a design space for WiMs based on seven
               design dimensions. Further, we expand upon existing definitions
               of WiMs to provide a definition that applies across the extended
               reality spectrum. We identify the design dimensions of
               size-scope-scale, abstraction, geometry, reference frame, links,
               multiples, and virtuality. Using our framework we describe
               existing Worlds-in-Miniature from the research literature and
               reveal unexplored research areas. Finally, we generate new
               examples of WiMs using our framework to fill some of these gaps.
               With our findings, we identify opportunities that can guide
               future research into WiMs.",
  series    = "CHI '21",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445098",
  keywords  = "Virtual/Augmented Reality, Meta-Analysis/Literature Survey",
  doi       = "10.1145/3411764.3445098",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Danyluk2021-ny,
  title     = "{A design space exploration of worlds in miniature}",
  author    = "Danyluk, Kurtis and Ens, Barrett and Jenny, Bernhard and Willett,
               Wesley",
  booktitle = "{Proceedings of the 2021 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1–15",
  abstract  = "Worlds-in-Miniature (WiMs) are interactive worlds within a world
               and combine the advantages of an input space, a cartographic map,
               and an overview+detail interface. They have been used across the
               extended virtuality spectrum for a variety of applications.
               Building on an analysis of examples of WiMs from the research
               literature we contribute a design space for WiMs based on seven
               design dimensions. Further, we expand upon existing definitions
               of WiMs to provide a definition that applies across the extended
               reality spectrum. We identify the design dimensions of
               size-scope-scale, abstraction, geometry, reference frame, links,
               multiples, and virtuality. Using our framework we describe
               existing Worlds-in-Miniature from the research literature and
               reveal unexplored research areas. Finally, we generate new
               examples of WiMs using our framework to fill some of these gaps.
               With our findings, we identify opportunities that can guide
               future research into WiMs.",
  series    = "CHI '21",
  month     =  "6~" # may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445098",
  file      = "All Papers/My Library/Danyluk et al. 2021 - A design space exploration of worlds in miniature.pdf",
  doi       = "10.1145/3411764.3445098",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Ahuja2021-me,
  title     = "{Vid2Doppler: Synthesizing Doppler radar data from videos for
               training privacy-preserving activity recognition}",
  author    = "Ahuja, Karan and Jiang, Yue and Goel, Mayank and Harrison, Chris",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445138",
  file      = "All Papers/My Library/Ahuja et al. 2021 - Vid2Doppler - Synthesizing Doppler radar data from videos for training privacy-preserving activity recognition.pdf",
  doi       = "10.1145/3411764.3445138",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Iravantchi2021-qf,
  title     = "{PrivacyMic: Utilizing inaudible frequencies for privacy
               preserving daily activity recognition}",
  author    = "Iravantchi, Yasha and Ahuja, Karan and Goel, Mayank and Harrison,
               Chris and Sample, Alanson",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "Sound presents an invaluable signal source that enables computing
               systems to perform daily activity recognition. However,
               microphones are optimized for human speech and hearing ranges:
               capturing private content, such as speech, while omitting useful,
               inaudible information that can aid in acoustic recognition tasks.
               We simulated acoustic recognition tasks using sounds from 127
               everyday household/workplace objects, finding that inaudible
               frequencies can act as a substitute for privacy-sensitive
               frequencies. To take advantage of these inaudible frequencies, we
               designed a Raspberry Pi-based device that captures inaudible
               acoustic frequencies with settings that can remove speech or all
               audible frequencies entirely. We conducted a perception study,
               where participants “eavesdropped” on PrivacyMic's filtered audio
               and found that none of our participants could transcribe speech.
               Finally, PrivacyMic's real-world activity recognition performance
               is comparable to our simulated results, with over 95\%
               classification accuracy across all environments, suggesting
               immediate viability in performing privacy-preserving daily
               activity recognition.",
  series    = "CHI '21",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145/3411764.3445169",
  doi       = "10.1145/3411764.3445169",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Chen2021-ch,
  title     = "{An adaptive model of gaze-based selection}",
  author    = "Chen, Xiuli and Acharya, Aditya and Oulasvirta, Antti and Howes,
               Andrew",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–11",
  abstract  = "Gaze-based selection has received significant academic attention
               over a number of years. While advances have been made, it is
               possible that further progress could be made if there were a
               deeper understanding of the adaptive nature of the mechanisms
               that guide eye movement and vision. Control of eye movement
               typically results in a sequence of movements (saccades) and
               fixations followed by a `dwell' at a target and a selection. To
               shed light on how these sequences are planned, this paper
               presents a computational model of the control of eye movements in
               gaze-based selection. We formulate the model as an optimal
               sequential planning problem bounded by the limits of the human
               visual and motor systems and use reinforcement learning to
               approximate optimal solutions. The model accurately replicates
               earlier results on the effects of target size and distance and
               captures a number of other aspects of performance. The model can
               be used to predict number of fixations and duration required to
               make a gaze-based selection. The future development of the model
               is discussed.",
  series    = "CHI '21",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145/3411764.3445177",
  file      = "All Papers/My Library/Chen et al. 2021 - An adaptive model of gaze-based selection.pdf",
  doi       = "10.1145/3411764.3445177",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Ahn2021-ww,
  title     = "{StickyPie: A gaze-based, scale-invariant marking menu optimized
               for AR/{VR}}",
  author    = "Ahn, Sunggeun and Santosa, Stephanie and Parent, Mark and Wigdor,
               Daniel and Grossman, Tovi and Giordano, Marcello",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–16",
  abstract  = "This work explores the design of marking menus for gaze-based
               AR/VR menu selection by expert and novice users. It first
               identifies and explains the challenges inherent in ocular motor
               control and current eye tracking hardware, including
               overshooting, incorrect selections, and false activations.
               Through three empirical studies, we optimized and validated
               design parameters to mitigate these errors while reducing
               completion time, task load, and eye fatigue. Based on the
               findings from these studies, we derived a set of design
               guidelines to support gaze-based marking menus in AR/VR. To
               overcome the overshoot errors found with eye-based expert marking
               menu behaviour, we developed StickyPie, a marking menu technique
               that enables scale-independent marking input by estimating
               saccade landing positions. An evaluation of StickyPie revealed
               that StickyPie was easier to learn than the traditional technique
               (i.e., RegularPie) and was 10\% more efficient after 3 sessions.",
  series    = "CHI '21",
  month     =  "5~" # jul,
  year      =  2021,
  url       = "https://doi.org/10.1145/3411764.3445297",
  file      = "All Papers/My Library/Ahn et al. 2021 - StickyPie - A gaze-based, scale-invariant marking menu optimized for AR - VR.pdf",
  doi       = "10.1145/3411764.3445297",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Arakawa2021-cx,
  title     = "{Mindless Attractor: A {False-Positive} Resistant Intervention
               for Drawing Attention Using Auditory Perturbation}",
  author    = "Arakawa, Riku and Yakura, Hiromu",
  booktitle = "{Proceedings of the 2021 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--15",
  abstract  = "Explicitly alerting users is not always an optimal intervention,
               especially when they are not motivated to obey. For example, in
               video-based learning, learners who are distracted from the video
               would not follow an alert asking them to pay attention. Inspired
               by the concept of Mindless Computing, we propose a novel
               intervention approach, Mindless Attractor, that leverages the
               nature of human speech communication to help learners refocus
               their attention without relying on their motivation.
               Specifically, it perturbs the voice in the video to direct their
               attention without consuming their conscious awareness. Our
               experiments not only confirmed the validity of the proposed
               approach but also emphasized its advantages in combination with a
               machine learning-based sensing module. Namely, it would not
               frustrate users even though the intervention is activated by
               false-positive detection of their attentive state. Our
               intervention approach can be a reliable way to induce behavioral
               change in human--AI symbiosis.",
  series    = "CHI '21",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445339",
  file      = "All Papers/Other/Arakawa and Yakura 2021 - Mindless Attractor - A False-Positive Resistant Intervention for Drawing Attention Using Auditory Perturbation.pdf",
  keywords  = "Machine learning-based sensing, Computational intervention,
               Video-based learning, Mindless computing, Human attention",
  doi       = "10.1145/3411764.3445339",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Arakawa2021-oh,
  title     = "{Mindless attractor: A False-Positive resistant intervention for
               drawing attention using auditory perturbation}",
  author    = "Arakawa, Riku and Yakura, Hiromu",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–15",
  abstract  = "Explicitly alerting users is not always an optimal intervention,
               especially when they are not motivated to obey. For example, in
               video-based learning, learners who are distracted from the video
               would not follow an alert asking them to pay attention. Inspired
               by the concept of Mindless Computing, we propose a novel
               intervention approach, Mindless Attractor, that leverages the
               nature of human speech communication to help learners refocus
               their attention without relying on their motivation.
               Specifically, it perturbs the voice in the video to direct their
               attention without consuming their conscious awareness. Our
               experiments not only confirmed the validity of the proposed
               approach but also emphasized its advantages in combination with a
               machine learning-based sensing module. Namely, it would not
               frustrate users even though the intervention is activated by
               false-positive detection of their attentive state. Our
               intervention approach can be a reliable way to induce behavioral
               change in human–AI symbiosis.",
  series    = "CHI '21",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445339",
  file      = "All Papers/My Library/Arakawa and Yakura 2021 - Mindless attractor - A False-Positive resistant intervention for drawing attention using auditory perturbation.pdf",
  doi       = "10.1145/3411764.3445339",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Belo2021-jl,
  title     = "{XRgonomics: Facilitating the creation of ergonomic 3D
               interfaces}",
  author    = "Belo, João Marcelo Evangelista and Feit, Anna Maria and
               Feuchtner, Tiare and Grønbæk, Kaj",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  may,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445349",
  file      = "All Papers/My Library/Belo et al. 2021 - XRgonomics - Facilitating the creation of ergonomic 3D interfaces.pdf",
  doi       = "10.1145/3411764.3445349"
}

@INPROCEEDINGS{Hammad2021-dh,
  title     = "{Homecoming: Exploring Returns to {Long-Term} Single Player
               Games}",
  author    = "Hammad, Noor and Brierley, Owen and McKendrick, Zachary and
               Somanath, Sowmya and Finn, Patrick and Hammer, Jessica and
               Sharlin, Ehud",
  booktitle = "{Proceedings of the 2021 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--13",
  abstract  = "We present an autobiographical design journey exploring the
               experience of returning to long-term single player games.
               Continuing progress from a previously saved game, particularly
               when substantial time has passed, is an understudied area in
               games research. To begin our exploration in this domain, we
               investigated what the return experience is like first-hand. By
               returning to four long-term single player games played
               extensively in the past, we revealed a phenomenon we call The
               Pivot Point, a `eureka' moment in return gameplay. The pivot
               point anchors our design explorations, where we created
               prototypes to leverage the pivot point in reconnecting with the
               experience. These return experiences and subsequent prototyping
               iterations inform our understanding of how to design better
               returns to gameplay, which can benefit both producers and
               consumers of long-term single player games.",
  series    = "CHI '21",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445357",
  file      = "All Papers/Other/Hammad et al. 2021 - Homecoming - Exploring Returns to Long-Term Single Player Games.pdf",
  keywords  = "long-term single player game, Pivot Point, autobiographical
               design",
  doi       = "10.1145/3411764.3445357",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Hammad2021-lg,
  title     = "{Homecoming: Exploring returns to Long-Term single player games}",
  author    = "Hammad, Noor and Brierley, Owen and McKendrick, Zachary and
               Somanath, Sowmya and Finn, Patrick and Hammer, Jessica and
               Sharlin, Ehud",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "We present an autobiographical design journey exploring the
               experience of returning to long-term single player games.
               Continuing progress from a previously saved game, particularly
               when substantial time has passed, is an understudied area in
               games research. To begin our exploration in this domain, we
               investigated what the return experience is like first-hand. By
               returning to four long-term single player games played
               extensively in the past, we revealed a phenomenon we call The
               Pivot Point, a `eureka' moment in return gameplay. The pivot
               point anchors our design explorations, where we created
               prototypes to leverage the pivot point in reconnecting with the
               experience. These return experiences and subsequent prototyping
               iterations inform our understanding of how to design better
               returns to gameplay, which can benefit both producers and
               consumers of long-term single player games.",
  series    = "CHI '21",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445357",
  file      = "All Papers/My Library/Hammad et al. 2021 - Homecoming - Exploring returns to Long-Term single player games.pdf",
  doi       = "10.1145/3411764.3445357",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Hayashi2021-iz,
  title     = "{RadarNet: Efficient gesture recognition technique utilizing a
               miniature radar sensor}",
  author    = "Hayashi, Eiji and Lien, Jaime and Gillian, Nicholas and Giusti,
               Leonardo and Weber, Dave and Yamanaka, Jin and Bedal, Lauren and
               Poupyrev, Ivan",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–14",
  abstract  = "Gestures are a promising candidate as an input modality for
               ambient computing where conventional input modalities such as
               touchscreens are not available. Existing works have focused on
               gesture recognition using image sensors. However, their cost,
               high battery consumption, and privacy concerns made cameras
               challenging as an always-on solution. This paper introduces an
               efficient gesture recognition technique using a miniaturized 60
               GHz radar sensor. The technique recognizes four directional
               swipes and an omni-swipe using a radar chip (6.5 × 5.0 mm)
               integrated into a mobile phone. We developed a convolutional
               neural network model efficient enough for battery powered and
               computationally constrained processors. Its model size and
               inference time is less than 1/5000 compared to an existing
               gesture recognition technique using radar. Our evaluations with
               large scale datasets consisting of 558,000 gesture samples and
               3,920,000 negative samples demonstrated our algorithm's
               efficiency, robustness, and readiness to be deployed outside of
               research laboratories.",
  series    = "CHI '21",
  month     =  "5~" # jul,
  year      =  2021,
  url       = "https://doi.org/10.1145/3411764.3445367",
  file      = "All Papers/My Library/Hayashi et al. 2021 - RadarNet - Efficient gesture recognition technique utilizing a miniature radar sensor.pdf",
  doi       = "10.1145/3411764.3445367",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Kang2021-xu,
  title     = "{ToonNote: Improving communication in computational notebooks
               using interactive data comics}",
  author    = "Kang, Daye and Ho, Tony and Marquardt, Nicolai and Mutlu, Bilge
               and Bianchi, Andrea",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  may,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445434",
  file      = "All Papers/My Library/Kang et al. 2021 - ToonNote - Improving communication in computational notebooks using interactive data comics.pdf",
  doi       = "10.1145/3411764.3445434"
}

@INPROCEEDINGS{Lee2021-mo,
  title     = "{ADIO: An interactive artifact physically representing the
               intangible digital audiobook listening experience in everyday
               living spaces}",
  author    = "Lee, Kyung-Ryong and Kim, Beom and Kim, Junyoung and Hong,
               Hwajung and Park, Young-Woo",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445440",
  doi       = "10.1145/3411764.3445440",
  isbn      =  9781450380966
}

@INCOLLECTION{Teyssier2021-mn,
  title     = "{Eyecam: Revealing Relations between Humans and Sensing Devices
               through an Anthropomorphic Webcam}",
  author    = "Teyssier, Marc and Koelle, Marion and Strohmeier, Paul and
               Fruchard, Bruno and Steimle, Jürgen",
  booktitle = "{Proceedings of the 2021 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--13",
  abstract  = "We are surrounded by sensing devices. We are accustomed to them,
               appreciate their benefits, and even create affective bonds and
               might neglect the implications they might have for our daily
               life. By presenting Eyecam, an anthropomorphic webcam mimicking a
               human eye, we challenge conventional relationships with
               ubiquitous sensing devices and call to re-think how sensing
               devices might appear and behave. Inspired by critical design,
               Eyecam is an exaggeration of a familiar sensing device which
               allows for critical reflections on its perceived functionalities
               and its impact on human-human and human-device relations. We
               identify 5 different roles Eyecam can take: Mediator, Observer,
               Mirror, Presence, and Agent. Contributing design fictions and
               thinking prompts, we allow for articulation on privacy awareness
               and intrusion, affect in mediated communication, agency and
               self-perception along with speculation on potential futures. We
               envision this work to contribute to a bold and responsible design
               of ubiquitous sensing devices.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445491",
  doi       = "10.1145/3411764.3445491",
  isbn      =  9781450380966
}

@INCOLLECTION{Teyssier2021-hh,
  title     = "{Eyecam: Revealing relations between humans and sensing devices
               through an anthropomorphic webcam}",
  author    = "Teyssier, Marc and Koelle, Marion and Strohmeier, Paul and
               Fruchard, Bruno and Steimle, Jürgen",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "We are surrounded by sensing devices. We are accustomed to them,
               appreciate their benefits, and even create affective bonds and
               might neglect the implications they might have for our daily
               life. By presenting Eyecam, an anthropomorphic webcam mimicking a
               human eye, we challenge conventional relationships with
               ubiquitous sensing devices and call to re-think how sensing
               devices might appear and behave. Inspired by critical design,
               Eyecam is an exaggeration of a familiar sensing device which
               allows for critical reflections on its perceived functionalities
               and its impact on human-human and human-device relations. We
               identify 5 different roles Eyecam can take: Mediator, Observer,
               Mirror, Presence, and Agent. Contributing design fictions and
               thinking prompts, we allow for articulation on privacy awareness
               and intrusion, affect in mediated communication, agency and
               self-perception along with speculation on potential futures. We
               envision this work to contribute to a bold and responsible design
               of ubiquitous sensing devices.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445491",
  file      = "All Papers/My Library/Teyssier et al. 2021 - Eyecam - Revealing relations between humans and sensing devices through an anthropomorphic webcam.pdf",
  doi       = "10.1145/3411764.3445491",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Cho2021-lx,
  title     = "{Rethinking eye-blink: Assessing task difficulty through
               physiological representation of spontaneous blinking}",
  author    = "Cho, Youngjun",
  booktitle = "{Proceedings of the 2021 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "6~" # may,
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3411764.3445577",
  file      = "All Papers/Other/Cho 2021 - Rethinking eye-blink - Assessing task difficulty through physiological representation of spontaneous blinking.pdf",
  doi       = "10.1145/3411764.3445577",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Syiem2021-eh,
  title     = "{Impact of task on attentional tunneling in handheld augmented
               reality}",
  author    = "Syiem, Brandon Victor and Kelly, Ryan M and Goncalves, Jorge and
               Velloso, Eduardo and Dingler, Tilman",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445580",
  file      = "All Papers/My Library/Syiem et al. 2021 - Impact of task on attentional tunneling in handheld augmented reality.pdf",
  doi       = "10.1145/3411764.3445580",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Palani2021-nw,
  title     = "{CoNotate: Suggesting queries based on notes promotes knowledge
               discovery}",
  author    = "Palani, Srishti and Ding, Zijian and Nguyen, Austin and Chuang,
               Andrew and MacNeil, Stephen and Dow, Steven P",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  month     =  may,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445618",
  file      = "All Papers/My Library/Palani et al. 2021 - CoNotate - Suggesting queries based on notes promotes knowledge discovery.pdf",
  doi       = "10.1145/3411764.3445618"
}

@INPROCEEDINGS{Ogata2021-ot,
  title     = "{A computational approach to magnetic force feedback design}",
  author    = "Ogata, Masa and Koyama, Yuki",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445631",
  doi       = "10.1145/3411764.3445631",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Lu2021-td,
  title     = "{More Kawaii than a Real-Person Live Streamer: Understanding How
               the Otaku Community Engages with and Perceives Virtual
               {YouTubers}}",
  author    = "Lu, Zhicong and Shen, Chenxinran and Li, Jiannan and Shen, Hong
               and Wigdor, Daniel",
  booktitle = "{Proceedings of the 2021 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–14",
  abstract  = "Live streaming has become increasingly popular, with most
               streamers presenting their real-life appearance. However, Virtual
               YouTubers (VTubers), virtual 2D or 3D avatars that are voiced by
               humans, are emerging as live streamers and attracting a growing
               viewership in East Asia. Although prior research has found that
               many viewers seek real-life interpersonal interactions with
               real-person streamers, it is currently unknown what makes VTuber
               live streams engaging or how they are perceived differently than
               real-person streamers. We conducted an interview study to
               understand how viewers engage with VTubers and perceive the
               identities of the voice actors behind the avatars (i.e.,
               Nakanohito). The data revealed that Virtual avatars bring unique
               performative opportunities which result in different viewer
               expectations and interpretations of VTuber behavior. Viewers
               intentionally upheld the disembodiment of VTuber avatars from
               their voice actors. We uncover the nuances in viewer perceptions
               and attitudes and further discuss the implications of VTuber
               practices to the understanding of live streaming in general.",
  series    = "CHI '21",
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3411764.3445660",
  file      = "All Papers/My Library/Lu et al. 2021 - More Kawaii than a Real-Person Live Streamer - Understanding How the Otaku Community Engages with and Perceives Virtual YouTubers.pdf",
  doi       = "10.1145/3411764.3445660",
  isbn      =  9781450380966
}

@INCOLLECTION{Mayer2021-la,
  title     = "{{Super-Resolution} Capacitive Touchscreens}",
  author    = "Mayer, Sven and Xu, Xiangyu and Harrison, Chris",
  booktitle = "{Proceedings of the 2021 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--10",
  abstract  = "Capacitive touchscreens are near-ubiquitous in today's
               touch-driven devices, such as smartphones and tablets. By using
               rows and columns of electrodes, specialized touch controllers are
               able to capture a 2D image of capacitance at the surface of a
               screen. For over a decade, capacitive ``pixels'' have been around
               4 millimeters in size -- a surprisingly low resolution that
               precludes a wide range of interesting applications. In this
               paper, we show how super-resolution techniques, long used in
               fields such as biology and astronomy, can be applied to
               capacitive touchscreen data. By integrating data from many
               frames, our software-only process is able to resolve geometric
               details finer than the original sensor resolution. This opens the
               door to passive tangibles with higher-density fiducials and also
               recognition of every-day metal objects, such as keys and coins.
               We built several applications to illustrate the potential of our
               approach and report the findings of a multipart evaluation.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445703",
  file      = "All Papers/Other/Mayer et al. 2021 - Super-Resolution Capacitive Touchscreens.pdf",
  keywords  = "eye contact;telepresence",
  doi       = "10.1145/3411764.3445703",
  isbn      =  9781450380966
}

@INCOLLECTION{Mayer2021-mz,
  title     = "{Super-Resolution capacitive touchscreens}",
  author    = "Mayer, Sven and Xu, Xiangyu and Harrison, Chris",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–10",
  abstract  = "Capacitive touchscreens are near-ubiquitous in today's
               touch-driven devices, such as smartphones and tablets. By using
               rows and columns of electrodes, specialized touch controllers are
               able to capture a 2D image of capacitance at the surface of a
               screen. For over a decade, capacitive “pixels” have been around 4
               millimeters in size – a surprisingly low resolution that
               precludes a wide range of interesting applications. In this
               paper, we show how super-resolution techniques, long used in
               fields such as biology and astronomy, can be applied to
               capacitive touchscreen data. By integrating data from many
               frames, our software-only process is able to resolve geometric
               details finer than the original sensor resolution. This opens the
               door to passive tangibles with higher-density fiducials and also
               recognition of every-day metal objects, such as keys and coins.
               We built several applications to illustrate the potential of our
               approach and report the findings of a multipart evaluation.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3445703",
  file      = "All Papers/My Library/Mayer et al. 2021 - Super-Resolution capacitive touchscreens.pdf",
  doi       = "10.1145/3411764.3445703",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Ahuja2021-nc,
  title     = "{Classroom digital twins with instrumentation-free gaze tracking}",
  author    = "Ahuja, Karan and Shah, Deval and Pareddy, Sujeath and Xhakaj,
               Franceska and Ogan, Amy and Agarwal, Yuvraj and Harrison, Chris",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–9",
  abstract  = "Classroom sensing is an important and active area of research
               with great potential to improve instruction. Complementing
               professional observers – the current best practice – automated
               pedagogical professional development systems can attend every
               class and capture fine-grained details of all occupants. One
               particularly valuable facet to capture is class gaze behavior.
               For students, certain gaze patterns have been shown to correlate
               with interest in the material, while for instructors,
               student-centered gaze patterns have been shown to increase
               approachability and immediacy. Unfortunately, prior classroom
               gaze-sensing systems have limited accuracy and often require
               specialized external or worn sensors. In this work, we developed
               a new computer-vision-driven system that powers a 3D “digital
               twin” of the classroom and enables whole-class, 6DOF head gaze
               vector estimation without instrumenting any of the occupants. We
               describe our open source implementation, and results from both
               controlled studies and real-world classroom deployments.",
  series    = "CHI '21",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145/3411764.3445711",
  file      = "All Papers/My Library/Ahuja et al. 2021 - Classroom digital twins with instrumentation-free gaze tracking.pdf",
  doi       = "10.1145/3411764.3445711",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Jasche2021-yl,
  title     = "{Comparison of different types of augmented reality
               visualizations for instructions}",
  author    = "Jasche, Florian and Hoffmann, Sven and Ludwig, Thomas and Wulf,
               Volker",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "5~" # jun,
  year      =  2021,
  url       = "https://doi.org/10.1145%2F3411764.3445724",
  doi       = "10.1145/3411764.3445724",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Ens2021-ec,
  title     = "{Grand Challenges in Immersive Analytics}",
  author    = "Ens, Barrett and Bach, Benjamin and Cordeil, Maxime and Engelke,
               Ulrich and Serrano, Marcos and Willett, Wesley and Prouzeau,
               Arnaud and Anthes, Christoph and Büschel, Wolfgang and Dunne,
               Cody and Dwyer, Tim and Grubert, Jens and Haga, Jason H and
               Kirshenbaum, Nurit and Kobayashi, Dylan and Lin, Tica and
               Olaosebikan, Monsurat and Pointecker, Fabian and Saffo, David and
               Saquib, Nazmus and Schmalstieg, Dieter and Szafir, Danielle
               Albers and Whitlock, Matt and Yang, Yalong",
  booktitle = "{Proceedings of the 2021 {CHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--17",
  abstract  = "Immersive Analytics is a quickly evolving field that unites
               several areas such as visualisation, immersive environments, and
               human-computer interaction to support human data analysis with
               emerging technologies. This research has thrived over the past
               years with multiple workshops, seminars, and a growing body of
               publications, spanning several conferences. Given the rapid
               advancement of interaction technologies and novel application
               domains, this paper aims toward a broader research agenda to
               enable widespread adoption. We present 17 key research challenges
               developed over multiple sessions by a diverse group of 24
               international experts, initiated from a virtual scientific
               workshop at ACM CHI 2020. These challenges aim to coordinate
               future work by providing a systematic roadmap of current
               directions and impending hurdles to facilitate productive and
               effective applications for Immersive Analytics.",
  series    = "CHI '21",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3446866",
  file      = "All Papers/Other/Ens et al. 2021 - Grand Challenges in Immersive Analytics.pdf",
  keywords  = "data visualisation, Immersive analytics, grand research
               challenges, augmented reality, virtual reality",
  doi       = "10.1145/3411764.3446866",
  isbn      =  9781450380966
}

@INPROCEEDINGS{Ens2021-uz,
  title     = "{Grand challenges in immersive analytics}",
  author    = "Ens, Barrett and Bach, Benjamin and Cordeil, Maxime and Engelke,
               Ulrich and Serrano, Marcos and Willett, Wesley and Prouzeau,
               Arnaud and Anthes, Christoph and Büschel, Wolfgang and Dunne,
               Cody and Dwyer, Tim and Grubert, Jens and Haga, Jason H and
               Kirshenbaum, Nurit and Kobayashi, Dylan and Lin, Tica and
               Olaosebikan, Monsurat and Pointecker, Fabian and Saffo, David and
               Saquib, Nazmus and Schmalstieg, Dieter and Szafir, Danielle
               Albers and Whitlock, Matt and Yang, Yalong",
  booktitle = "{Proceedings of the 2021 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–17",
  abstract  = "Immersive Analytics is a quickly evolving field that unites
               several areas such as visualisation, immersive environments, and
               human-computer interaction to support human data analysis with
               emerging technologies. This research has thrived over the past
               years with multiple workshops, seminars, and a growing body of
               publications, spanning several conferences. Given the rapid
               advancement of interaction technologies and novel application
               domains, this paper aims toward a broader research agenda to
               enable widespread adoption. We present 17 key research challenges
               developed over multiple sessions by a diverse group of 24
               international experts, initiated from a virtual scientific
               workshop at ACM CHI 2020. These challenges aim to coordinate
               future work by providing a systematic roadmap of current
               directions and impending hurdles to facilitate productive and
               effective applications for Immersive Analytics.",
  series    = "CHI '21",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3411764.3446866",
  file      = "All Papers/My Library/Ens et al. 2021 - Grand challenges in immersive analytics.pdf",
  doi       = "10.1145/3411764.3446866",
  isbn      =  9781450380966
}

@ARTICLE{Kutt2020-qj,
  title     = "{Effects of shared gaze on audio- versus text-based remote
               collaborations}",
  author    = "Kütt, Grete Helena and Tanprasert, Teerapaun and Rodolitz, Jay
               and Moyza, Bernardo and So, Samuel and Kenderova, Georgia and
               Papoutsaki, Alexandra",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  4,
  number    = "CSCW2",
  pages     = "1--25",
  abstract  = "The findings demonstrate the differences in how shared gaze
               impacts audio- versus text-based communication and highlight the
               need to further understand the nuances of the medium of
               communication when designing novel tools to support remote
               collaborators. Remote collaborations are becoming ubiquitous,
               but, despite their many advantages, face unique challenges
               compared to collocated collaborations. Visualizing the
               collaborator's point of gaze on a shared screen has been explored
               as a promising way to alleviate some of these limitations by
               increasing shared awareness. However, prior studies on shared
               gaze have not considered the medium of communication and have
               only studied its effect on audio. This paper presents a study
               that compares the effects of shared gaze on collaboration
               performance during audio- and text-based communication. We find
               that for text, shared gaze improved task correctness and led
               collaborators to look at and talk more about shared content.
               Similar trends are found for gaze-augmented voice communication,
               but contrary to the slower performance in text, it also saw
               improvements in completion time as well as in cognitive workload.
               Our findings demonstrate the differences in how shared gaze
               impacts audio- versus text-based communication and highlight the
               need to further understand the nuances of the medium of
               communication when designing novel tools to support remote
               collaborators.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3415207",
  file      = "All Papers/Other/Kütt et al. 2020 - Effects of shared gaze on audio- versus text-based remote collaborations.pdf",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1145/3415207",
  issn      = "2573-0142",
  language  = "en"
}

@ARTICLE{Kutt2020-kw,
  title    = "{Effects of shared gaze on audio- versus text-based remote
              collaborations}",
  author   = "Kütt, Grete Helena and Tanprasert, Teerapaun and Rodolitz, Jay and
              Moyza, Bernardo and So, Samuel and Kenderova, Georgia and
              Papoutsaki, Alexandra",
  journal  = "Proc. ACM Hum. Comput. Interact.",
  volume   =  4,
  number   = "CSCW2",
  pages    = "1–25",
  abstract = "The findings demonstrate the differences in how shared gaze
              impacts audio- versus text-based communication and highlight the
              need to further understand the nuances of the medium of
              communication when designing novel tools to support remote
              collaborators. Remote collaborations are becoming ubiquitous, but,
              despite their many advantages, face unique challenges compared to
              collocated collaborations. Visualizing the collaborator's point of
              gaze on a shared screen has been explored as a promising way to
              alleviate some of these limitations by increasing shared
              awareness. However, prior studies on shared gaze have not
              considered the medium of communication and have only studied its
              effect on audio. This paper presents a study that compares the
              effects of shared gaze on collaboration performance during audio-
              and text-based communication. We find that for text, shared gaze
              improved task correctness and led collaborators to look at and
              talk more about shared content. Similar trends are found for
              gaze-augmented voice communication, but contrary to the slower
              performance in text, it also saw improvements in completion time
              as well as in cognitive workload. Our findings demonstrate the
              differences in how shared gaze impacts audio- versus text-based
              communication and highlight the need to further understand the
              nuances of the medium of communication when designing novel tools
              to support remote collaborators.",
  month    =  oct,
  year     =  2020,
  url      = "http://dx.doi.org/10.1145/3415207",
  file     = "All Papers/My Library/Kütt et al. 2020 - Effects of shared gaze on audio- versus text-based remote collaborations.pdf",
  doi      = "10.1145/3415207",
  issn     = "2573-0142",
  language = "en"
}

@INPROCEEDINGS{Wang2020-xe,
  title     = "{SHARIdeas: A visual representation of intention sharing between
               designer and executor supporting AR assembly}",
  author    = "Wang, Zhuo and Bai, Xiaoliang and Zhang, Shusheng and He, Weiping
               and Wang, Peng and Zhang, Xiangyu and Yan, Yuxiang",
  booktitle = "{SIGGRAPH asia 2020 posters}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "12~" # apr,
  year      =  2020,
  url       = "https://doi.org/10.1145%2F3415264.3431858",
  doi       = "10.1145/3415264.3431858",
  isbn      =  9781450381130
}

@INPROCEEDINGS{Munz2020-aj,
  title     = "{Comparative visual gaze analysis for virtual board games}",
  author    = "Munz, Tanja and Schäfer, Noel and Blascheck, Tanja and Kurzhals,
               Kuno and Zhang, Eugene and Weiskopf, Daniel",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "We introduce an approach for the visual analysis of eye movement
               data from two people playing competitive virtual board games. Our
               approach provides methods to temporally synchronize and spatially
               register gaze and mouse recordings from two eye tracking devices.
               Analysts can examine such fused data visually with a combination
               of techniques: attention maps and gaze plots as well as a
               temporal summary of the distance between gaze positions and mouse
               events of the two players. We show different game scenarios from
               the competitive game Go, which is especially complex for
               analyzing strategies of individual players, to demonstrate our
               methods. In general, our visual analysis approach can provide
               analysts with insights into strategies, learning processes, and
               means of communication between people.",
  series    = "VINCI '20",
  month     =  "8~" # dec,
  year      =  2020,
  url       = "https://dl.acm.org/doi/10.1145/3430036.3430038",
  doi       = "10.1145/3430036.3430038",
  isbn      =  9781450387507
}

@INPROCEEDINGS{Kamikubo2021-yy,
  title     = "{Sharing practices for datasets related to accessibility and
               aging}",
  author    = "Kamikubo, Rie and Dwivedi, Utkarsh and Kacorri, Hernisa",
  booktitle = "{The 23rd international ACM SIGACCESS conference on computers and
               accessibility}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–16",
  abstract  = "Datasets sourced from people with disabilities and older adults
               play an important role in innovation, benchmarking, and
               mitigating bias for both assistive and inclusive AI-infused
               applications. However, they are scarce. We conduct a systematic
               review of 137 accessibility datasets manually located across
               different disciplines over the last 35 years. Our analysis
               highlights how researchers navigate tensions between benefits and
               risks in data collection and sharing. We uncover patterns in data
               collection purpose, terminology, sample size, data types, and
               data sharing practices across communities of focus. We conclude
               by critically reflecting on challenges and opportunities related
               to locating and sharing accessibility datasets calling for
               technical, legal, and institutional privacy frameworks that are
               more attuned to concerns from these communities.",
  series    = "ASSETS '21",
  month     =  "17~" # oct,
  year      =  2021,
  url       = "https://doi.org/10.1145/3441852.3471208",
  file      = "All Papers/My Library/Kamikubo et al. 2021 - Sharing practices for datasets related to accessibility and aging.pdf",
  doi       = "10.1145/3441852.3471208",
  isbn      =  9781450383066
}

@INCOLLECTION{Feng2021-cc,
  title     = "{{HGaze} Typing: {Head-Gesture} Assisted Gaze Typing}",
  author    = "Feng, Wenxin and Zou, Jiangnan and Kurauchi, Andrew and Morimoto,
               Carlos H and Betke, Margrit",
  booktitle = "{{ACM} Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--11",
  abstract  = "This paper introduces a bi-modal typing interface, HGaze Typing,
               which combines the simplicity of head gestures with the speed of
               gaze inputs to provide efficient and comfortable dwell-free text
               entry. HGaze Typing uses gaze path information to compute
               candidate words and allows explicit activation of common text
               entry commands, such as selection, deletion, and revision, by
               using head gestures (nodding, shaking, and tilting). By adding a
               head-based input channel, HGaze Typing reduces the size of the
               screen regions for cancel/deletion buttons and the word candidate
               list, which are required by most eye-typing interfaces. A user
               study finds HGaze Typing outperforms a dwell-time-based keyboard
               in efficacy and user satisfaction. The results demonstrate that
               the proposed method of integrating gaze and head-movement inputs
               can serve as an effective interface for text entry and is robust
               to unintended selections.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3448017.3457379",
  file      = "All Papers/Other/Feng et al. 2021 - HGaze Typing - Head-Gesture Assisted Gaze Typing.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1145/3448017.3457379",
  isbn      =  9781450383448
}

@INCOLLECTION{Feng2021-po,
  title     = "{HGaze typing: Head-Gesture assisted gaze typing}",
  author    = "Feng, Wenxin and Zou, Jiangnan and Kurauchi, Andrew and Morimoto,
               Carlos H and Betke, Margrit",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–11",
  abstract  = "This paper introduces a bi-modal typing interface, HGaze Typing,
               which combines the simplicity of head gestures with the speed of
               gaze inputs to provide efficient and comfortable dwell-free text
               entry. HGaze Typing uses gaze path information to compute
               candidate words and allows explicit activation of common text
               entry commands, such as selection, deletion, and revision, by
               using head gestures (nodding, shaking, and tilting). By adding a
               head-based input channel, HGaze Typing reduces the size of the
               screen regions for cancel/deletion buttons and the word candidate
               list, which are required by most eye-typing interfaces. A user
               study finds HGaze Typing outperforms a dwell-time-based keyboard
               in efficacy and user satisfaction. The results demonstrate that
               the proposed method of integrating gaze and head-movement inputs
               can serve as an effective interface for text entry and is robust
               to unintended selections.",
  month     =  may,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3448017.3457379",
  file      = "All Papers/My Library/Feng et al. 2021 - HGaze typing - Head-Gesture assisted gaze typing.pdf",
  doi       = "10.1145/3448017.3457379",
  isbn      =  9781450383448
}

@INPROCEEDINGS{Ramirez_Gomez2021-fv,
  title     = "{Gaze+Hold: Eyes-only Direct Manipulation with Continuous Gaze
               Modulated by Closure of One Eye}",
  author    = "Ramirez Gomez, Argenis Ramirez and Clarke, Christopher and
               Sidenmark, Ludwig and Gellersen, Hans",
  booktitle = "{ACM Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "The eyes are coupled in their gaze function and therefore usually
               treated as a single input channel, limiting the range of
               interactions. However, people are able to open and close one eye
               while still gazing with the other. We introduce Gaze+Hold as an
               eyes-only technique that builds on this ability to leverage the
               eyes as separate input channels, with one eye modulating the
               state of interaction while the other provides continuous input.
               Gaze+Hold enables direct manipulation beyond pointing which we
               explore through the design of Gaze+Hold techniques for a range of
               user interface tasks. In a user study, we evaluated performance,
               usability and user’s spontaneous choice of eye for modulation of
               input. The results show that users are effective with Gaze+Hold.
               The choice of dominant versus non-dominant eye had no effect on
               performance, perceived usability and workload. This is
               significant for the utility of Gaze+Hold as it affords
               flexibility for mapping of either eye in different
               configurations.",
  series    = "ETRA '21 Full Papers",
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3448017.3457381",
  file      = "All Papers/My Library/Ramirez Gomez et al. 2021 - Gaze+Hold - Eyes-only Direct Manipulation with Continuous Gaze Modulated by Closure of One Eye.pdf",
  doi       = "10.1145/3448017.3457381",
  isbn      =  9781450383448
}

@INPROCEEDINGS{Mutasim2021-vk,
  title     = "{Pinch, Click, or Dwell: Comparing Different Selection Techniques
               for Eye-Gaze-Based Pointing in Virtual Reality}",
  author    = "Mutasim, Aunnoy K and Batmaz, Anil Ufuk and Stuerzlinger,
               Wolfgang",
  booktitle = "{ACM Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "While a pinch action is gaining popularity for selection of
               virtual objects in eye-gaze-based systems, it is still unknown
               how well this method performs compared to other popular
               alternatives, e.g., a button click or a dwell action. To
               determine pinch’s performance in terms of execution time, error
               rate, and throughput, we implemented a Fitts’ law task in Virtual
               Reality (VR) where the subjects pointed with their (eye-)gaze and
               selected / activated the targets by pinch, clicking a button, or
               dwell. Results revealed that although pinch was slower, made more
               errors, and had less throughput compared to button clicks, none
               of these differences were significant. Dwell exhibited the least
               errors but was significantly slower and achieved less throughput
               compared to the other conditions. Based on these findings, we
               conclude that the pinch gesture is a reasonable alternative to
               button clicks for eye-gaze-based VR systems.",
  series    = "ETRA '21 Short Papers",
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3448018.3457998",
  doi       = "10.1145/3448018.3457998",
  isbn      =  9781450383455
}

@INPROCEEDINGS{Nishizono2021-ot,
  title     = "{Synchronization of Spontaneous Eyeblink during Formula Car
               Driving}",
  author    = "Nishizono, Ryota and Saijo, Naoki and Kashino, Makio",
  booktitle = "{ACM Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–6",
  abstract  = "Formula car racing is a highly competitive sport. Previous
               studies have investigated the physiological characteristics and
               motor behaviors of drivers; however, little is known about how
               they modulate their cognitive states to improve their skills.
               Spontaneous eyeblink is a noteworthy factor because it reflects
               attentional states and is important for drivers to minimize the
               chance of losing critical visual information. In this study, we
               investigated whether the blink rate, blink synchronization among
               laps in each driver, and synchronization across drivers were
               related to their performance. Toward this end, we recorded the
               blinks and car behavior data of two professional drivers in
               quasi-racing environments. The results showed higher
               synchronization in higher-performance laps of each driver and
               across drivers but no significant change in blink rate. These
               results suggest that blink synchronization could reflect the
               changes in performance mode during formula car driving.",
  series    = "ETRA '21 Short Papers",
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3448018.3458002",
  file      = "All Papers/My Library/Nishizono et al. 2021 - Synchronization of Spontaneous Eyeblink during Formula Car Driving.pdf",
  doi       = "10.1145/3448018.3458002",
  isbn      =  9781450383455
}

@INPROCEEDINGS{Bafna2021-nq,
  title     = "{EyeTell: Tablet-based Calibration-free Eye-typing using
               Smooth-pursuit movements}",
  author    = "Bafna, Tanya and Bækgaard, Per and Paulin Hansen, John Paulin",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–6",
  abstract  = "Gaze tracking technology, with the increasingly robust and
               lightweight equipment, can have tremendous applications. To use
               the technology during short interactions, such as in public
               displays or hospitals to communicate non-verbally after a
               surgery, the application needs to be intuitive without requiring
               a calibration. Gaze gestures such as smooth-pursuit eye movements
               can be detected without calibration. We report the working
               performance of a calibration-free eye-typing application using
               only the front-facing camera of a tablet. In a user study with 29
               participants, we obtained an average typing speed of 1.27 WPM
               after four trials and a maximum typing speed of 1.95 WPM.",
  series    = "ETRA '21 short papers",
  month     =  "25~" # may,
  year      =  2021,
  url       = "https://doi.org/10.1145/3448018.3458015",
  file      = "All Papers/My Library/Bafna et al. 2021 - EyeTell - Tablet-based Calibration-free Eye-typing using Smooth-pursuit movements.pdf",
  doi       = "10.1145/3448018.3458015",
  isbn      =  9781450383455
}

@INPROCEEDINGS{Pfeuffer2021-cy,
  title     = "{Multi-user gaze-based interaction techniques on collaborative
               touchscreens}",
  author    = "Pfeuffer, Ken and Alexander, Jason and Gellersen, Hans",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Eye-gaze is a technology for implicit, fast, and hands-free input
               for a variety of use cases, with the majority of techniques
               focusing on single-user contexts. In this work, we present an
               exploration into gaze techniques of users interacting together on
               the same surface. We explore interaction concepts that exploit
               two states in an interactive system: 1) users visually attending
               to the same object in the UI, or 2) users focusing on separate
               targets. Interfaces can exploit these states with increasing
               availability of eye-tracking. For example, to dynamically
               personalise content on the UI to each user, and to provide a
               merged or compromised view on an object when both users' gaze are
               falling upon it. These concepts are explored with a prototype
               horizontal interface that tracks gaze of two users facing each
               other. We build three applications that illustrate different
               mappings of gaze to multi-user support: an indoor map with
               gaze-highlighted information, an interactive tree-of-life
               visualisation that dynamically expands on users' gaze, and a
               worldmap application with gaze-aware fisheye zooming. We conclude
               with insights from a public deployment of this system, pointing
               toward the engaging and seamless ways how eye based input
               integrates into collaborative interaction.",
  series    = "ETRA '21 short papers",
  month     =  "25~" # may,
  year      =  2021,
  url       = "https://doi.org/10.1145/3448018.3458016",
  file      = "All Papers/My Library/Pfeuffer et al. 2021 - Multi-user gaze-based interaction techniques on collaborative touchscreens.pdf",
  doi       = "10.1145/3448018.3458016",
  isbn      =  9781450383455
}

@ARTICLE{Arakawa2021-hy,
  title   = "{Reaction or speculation: Building computational support for users
             in catching-up series based on an emerging media consumption
             phenomenon}",
  author  = "Arakawa, Riku and Yakura, Hiromu",
  journal = "Proc. ACM Hum. -Comput. Interact.",
  volume  =  5,
  number  = "CSCW1",
  pages   = "1–28",
  month   =  apr,
  year    =  2021,
  url     = "https://doi.org/10.1145%2F3449225",
  file    = "All Papers/My Library/Arakawa and Yakura 2021 - Reaction or speculation - Building computation ... catching-up series based on an emerging media consumption phenomenon.pdf",
  doi     = "10.1145/3449225"
}

@INPROCEEDINGS{Lewien2021-ap,
  title     = "{GazeHelp: Exploring practical gaze-assisted interactions for
               graphic design tools}",
  author    = "Lewien, Ryan",
  booktitle = "{ACM symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–4",
  abstract  = "This system development project introduces the Adobe Photoshop
               plugin GazeHelp, exploring the practical application of
               multimodal gaze-assisted interaction in assisting current graphic
               design activities. It implements three core features, including
               QuickTool: a gaze-triggered popup that allows the user to select
               their next tool with gaze; X-Ray: creating a small
               non-destructive window at the gaze point, cutting through an
               artboard's layers to expose an element on a selected underlying
               layer; and Privacy Shield: dimming and blocking the current art
               board from view when looking away from the display. Each harness
               the speed, gaze-contingent observational nature and
               presence-implying strengths of gaze respectively, and are
               customisable to the user's preferences. The accompanying
               GazeHelpServer, complete with intuitive GUI, can also be flexibly
               used by other programs and plugins for further development.",
  series    = "ETRA '21 adjunct",
  month     =  "25~" # may,
  year      =  2021,
  url       = "https://doi.org/10.1145/3450341.3458764",
  file      = "All Papers/My Library/Lewien 2021 - GazeHelp - Exploring practical gaze-assisted interactions for graphic design tools.pdf",
  doi       = "10.1145/3450341.3458764",
  isbn      =  9781450383578
}

@ARTICLE{Khan2021-mo,
  title    = "{GAVIN: Gaze-assisted voice-based implicit note-taking}",
  author   = "Khan, Anam Ahmad and Newn, Joshua and Kelly, Ryan M and
              Srivastava, Namrata and Bailey, James and Velloso, Eduardo",
  journal  = "ACM Trans. Comput.-Hum. Interact.",
  volume   =  28,
  number   =  4,
  pages    = "1–32",
  abstract = "Annotation is an effective reading strategy people often undertake
              while interacting with digital text. It involves highlighting
              pieces of text and making notes about them. Annotating while
              reading in a desktop environment is considered trivial but, in a
              mobile setting where people read while hand-holding devices, the
              task of highlighting and typing notes on a mobile display is
              challenging. In this article, we introduce GAVIN, a gaze-assisted
              voice note-taking application, which enables readers to seamlessly
              take voice notes on digital documents by implicitly anchoring them
              to text passages. We first conducted a contextual enquiry focusing
              on participants' note-taking practices on digital documents. Using
              these findings, we propose a method which leverages eye-tracking
              and machine learning techniques to annotate voice notes with
              reference text passages. To evaluate our approach, we recruited 32
              participants performing voice note-taking. Following, we trained a
              classifier on the data collected to predict text passage where
              participants made voice notes. Lastly, we employed the classifier
              to built GAVIN and conducted a user study to demonstrate the
              feasibility of the system. This research demonstrates the
              feasibility of using gaze as a resource for implicit anchoring of
              voice notes, enabling the design of systems that allow users to
              record voice notes with minimal effort and high accuracy.",
  month    =  "8~" # nov,
  year     =  2021,
  url      = "https://doi.org/10.1145/3453988",
  file     = "All Papers/My Library/Khan et al. 2021 - GAVIN - Gaze-assisted voice-based implicit note-taking.pdf",
  doi      = "10.1145/3453988",
  issn     = "1073-0516"
}

@INCOLLECTION{Kari2021-ip,
  title     = "{{SoundsRide}: {Affordance-Synchronized} Music Mixing for
               {In-Car} Audio Augmented Reality}",
  author    = "Kari, Mohamed and Grosse-Puppendahl, Tobias and Jagaciak,
               Alexander and Bethge, David and Schütte, Reinhard and Holz,
               Christian",
  booktitle = "{The 34th Annual {ACM} Symposium on User Interface Software and
               Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "118--133",
  abstract  = "Music is a central instrument in video gaming to attune a
               player's attention to the current atmosphere and increase their
               immersion in the game. We transfer the idea of scene-adaptive
               music to car drives and propose SoundsRide, an in-car audio
               augmented reality system that mixes music in real-time
               synchronized with sound affordances along the ride. After
               exploring the design space of affordance-synchronized music, we
               design SoundsRide to temporally and spatially align high-contrast
               events on the route, e. g., highway entrances or tunnel exits,
               with high-contrast events in music, e. g., song transitions or
               beat drops, for any recorded and annotated GPS trajectory by a
               three-step procedure. In real-time, SoundsRide 1) estimates
               temporal distances to events on the route, 2) fuses these novel
               estimates with previous estimates in a cost-aware music-mixing
               plan, and 3) if necessary, re-computes an updated mix to be
               propagated to the audio output. To minimize user-noticeable
               updates to the mix, SoundsRide fuses new distance information
               with a filtering procedure that chooses the best updating
               strategy given the last music-mixing plan, the novel distance
               estimations, and the system parameterization. We technically
               evaluate SoundsRide and conduct a user evaluation with 8
               participants to gain insights into how users perceive SoundsRide
               in terms of mixing, affordances, and synchronicity. We find that
               SoundsRide can create captivating music experiences and
               positively as well as negatively influence subjectively perceived
               driving safety, depending on the mix and user.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474739",
  doi       = "10.1145/3472749.3474739",
  isbn      =  9781450386357
}

@INCOLLECTION{Kari2021-io,
  title     = "{SoundsRide: Affordance-Synchronized music mixing for In-Car
               audio augmented reality}",
  author    = "Kari, Mohamed and Grosse-Puppendahl, Tobias and Jagaciak,
               Alexander and Bethge, David and Schütte, Reinhard and Holz,
               Christian",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "118–133",
  abstract  = "Music is a central instrument in video gaming to attune a
               player's attention to the current atmosphere and increase their
               immersion in the game. We transfer the idea of scene-adaptive
               music to car drives and propose SoundsRide, an in-car audio
               augmented reality system that mixes music in real-time
               synchronized with sound affordances along the ride. After
               exploring the design space of affordance-synchronized music, we
               design SoundsRide to temporally and spatially align high-contrast
               events on the route, e. g., highway entrances or tunnel exits,
               with high-contrast events in music, e. g., song transitions or
               beat drops, for any recorded and annotated GPS trajectory by a
               three-step procedure. In real-time, SoundsRide 1) estimates
               temporal distances to events on the route, 2) fuses these novel
               estimates with previous estimates in a cost-aware music-mixing
               plan, and 3) if necessary, re-computes an updated mix to be
               propagated to the audio output. To minimize user-noticeable
               updates to the mix, SoundsRide fuses new distance information
               with a filtering procedure that chooses the best updating
               strategy given the last music-mixing plan, the novel distance
               estimations, and the system parameterization. We technically
               evaluate SoundsRide and conduct a user evaluation with 8
               participants to gain insights into how users perceive SoundsRide
               in terms of mixing, affordances, and synchronicity. We find that
               SoundsRide can create captivating music experiences and
               positively as well as negatively influence subjectively perceived
               driving safety, depending on the mix and user.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474739",
  doi       = "10.1145/3472749.3474739",
  isbn      =  9781450386357
}

@INPROCEEDINGS{Nair2021-xt,
  title     = "{NavStick: Making video games blind-accessible via the ability to
               look around}",
  author    = "Nair, Vishnu and Karp, Jay L and Silverman, Samuel and Kalra,
               Mohar and Lehv, Hollis and Jamil, Faizan and Smith, Brian A",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "538–551",
  abstract  = "Video games remain largely inaccessible to visually impaired
               people (VIPs). Today's blind-accessible games are highly
               simplified renditions of what sighted players enjoy, and they do
               not give VIPs the same freedom to look around and explore game
               worlds on their own terms. In this work, we introduce NavStick,
               an audio-based tool for looking around within virtual
               environments, with the aim of making 3D adventure video games
               more blind-accessible. NavStick repurposes a game controller's
               thumbstick to allow VIPs to survey what is around them via
               line-of-sight. In a user study, we compare NavStick with
               traditional menu-based surveying for different navigation tasks
               and find that VIPs were able to form more accurate mental maps of
               their environment with NavStick than with menu-based surveying.
               In an additional exploratory study, we investigate NavStick in
               the context of a representative 3D adventure game. Our findings
               reveal several implications for blind-accessible games, and we
               close by discussing these.",
  series    = "UIST '21",
  month     =  "10~" # oct,
  year      =  2021,
  url       = "https://doi.org/10.1145/3472749.3474768",
  file      = "All Papers/My Library/Nair et al. 2021 - NavStick - Making video games blind-accessible via the ability to look around.pdf",
  doi       = "10.1145/3472749.3474768",
  isbn      =  9781450386357
}

@INCOLLECTION{Wang2021-hv,
  title     = "{{GesturAR}: An Authoring System for Creating Freehand
               Interactive Augmented Reality Applications}",
  author    = "Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and
               Cao, Yuanzhi and Ramani, Karthik",
  booktitle = "{The 34th Annual {ACM} Symposium on User Interface Software and
               Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "552--567",
  abstract  = "Freehand gesture is an essential input modality for modern
               Augmented Reality (AR) user experiences. However, developing AR
               applications with customized hand interactions remains a
               challenge for end-users. Therefore, we propose GesturAR, an
               end-to-end authoring tool that supports users to create in-situ
               freehand AR applications through embodied demonstration and
               visual programming. During authoring, users can intuitively
               demonstrate the customized gesture inputs while referring to the
               spatial and temporal context. Based on the taxonomy of gestures
               in AR, we proposed a hand interaction model which maps the
               gesture inputs to the reactions of the AR contents. Thus, users
               can author comprehensive freehand applications using
               trigger-action visual programming and instantly experience the
               results in AR. Further, we demonstrate multiple application
               scenarios enabled by GesturAR, such as interactive virtual
               objects, robots, and avatars, room-level interactive AR spaces,
               embodied AR presentations, etc. Finally, we evaluate the
               performance and usability of GesturAR through a user study.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474769",
  file      = "All Papers/Other/Wang et al. 2021 - GesturAR - An Authoring System for Creating Freehand Interactive Augmented Reality Applications.pdf",
  doi       = "10.1145/3472749.3474769",
  isbn      =  9781450386357
}

@INCOLLECTION{Wang2021-sq,
  title     = "{GesturAR: An authoring system for creating freehand interactive
               augmented reality applications}",
  author    = "Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and
               Cao, Yuanzhi and Ramani, Karthik",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "552–567",
  abstract  = "Freehand gesture is an essential input modality for modern
               Augmented Reality (AR) user experiences. However, developing AR
               applications with customized hand interactions remains a
               challenge for end-users. Therefore, we propose GesturAR, an
               end-to-end authoring tool that supports users to create in-situ
               freehand AR applications through embodied demonstration and
               visual programming. During authoring, users can intuitively
               demonstrate the customized gesture inputs while referring to the
               spatial and temporal context. Based on the taxonomy of gestures
               in AR, we proposed a hand interaction model which maps the
               gesture inputs to the reactions of the AR contents. Thus, users
               can author comprehensive freehand applications using
               trigger-action visual programming and instantly experience the
               results in AR. Further, we demonstrate multiple application
               scenarios enabled by GesturAR, such as interactive virtual
               objects, robots, and avatars, room-level interactive AR spaces,
               embodied AR presentations, etc. Finally, we evaluate the
               performance and usability of GesturAR through a user study.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474769",
  file      = "All Papers/My Library/Wang et al. 2021 - GesturAR - An authoring system for creating freehand interactive augmented reality applications.pdf",
  doi       = "10.1145/3472749.3474769",
  isbn      =  9781450386357
}

@INPROCEEDINGS{Senft2021-ob,
  title     = "{Situated live programming for human-robot collaboration}",
  author    = "Senft, Emmanuel and Hagenow, Michael and Radwin, Robert and Zinn,
               Michael and Gleicher, Michael and Mutlu, Bilge",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "613–625",
  abstract  = "We present situated live programming for human-robot
               collaboration, an approach that enables users with limited
               programming experience to program collaborative applications for
               human-robot interaction. Allowing end users, such as shop floor
               workers, to program collaborative robots themselves would make it
               easy to “retask” robots from one process to another, facilitating
               their adoption by small and medium enterprises. Our approach
               builds on the paradigm of trigger-action programming (TAP) by
               allowing end users to create rich interactions through simple
               trigger-action pairings. It enables end users to iteratively
               create, edit, and refine a reactive robot program while executing
               partial programs. This live programming approach enables the user
               to utilize the task space and objects by incrementally specifying
               situated trigger-action pairs, substantially lowering the barrier
               to entry for programming or reprogramming robots for
               collaboration. We instantiate situated live programming in an
               authoring system where users can create trigger-action programs
               by annotating an augmented video feed from the robot's
               perspective and assign robot actions to trigger conditions. We
               evaluated this system in a study where participants (n = 10)
               developed robot programs for solving collaborative
               light-manufacturing tasks. Results showed that users with little
               programming experience were able to program HRC tasks in an
               interactive fashion and our situated live programming approach
               further supported individualized strategies and workflows. We
               conclude by discussing opportunities and limitations of the
               proposed approach, our system implementation, and our study and
               discuss a roadmap for expanding this approach to a broader range
               of tasks and applications.",
  series    = "UIST '21",
  month     =  "10~" # oct,
  year      =  2021,
  url       = "https://doi.org/10.1145/3472749.3474773",
  file      = "All Papers/My Library/Senft et al. 2021 - Situated live programming for human-robot collaboration.pdf",
  doi       = "10.1145/3472749.3474773",
  isbn      =  9781450386357
}

@INPROCEEDINGS{Tran_OLeary2021-nk,
  title     = "{Taxon: a language for formal reasoning with digital fabrication
               machines}",
  author    = "Tran O'Leary, Jasper and Nandi, Chandrakana and Lee, Khang and
               Peek, Nadya",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "691–709",
  abstract  = "Digital fabrication machines for makers have expanded access to
               manufacturing processes such as 3D printing, laser cutting, and
               milling. While digital models encode the data necessary for a
               machine to manufacture an object, understanding the trade-offs
               and limitations of the machines themselves is crucial for
               successful production. Yet, this knowledge is not codified and
               must be gained through experience, which limits both adoption of
               and creative exploration with digital fabrication tools. To
               formally represent machines, we present Taxon, a language that
               encodes a machine's high-level characteristics, physical
               composition, and performable actions. With this programmatic
               foundation, makers can develop rules of thumb that filter for
               appropriate machines for a given job and verify that actions are
               feasible and safe. We integrate the language with a browser-based
               system for simulating and experimenting with machine workflows.
               The system lets makers engage with rules of thumb and enrich
               their understanding of machines. We evaluate Taxon by
               representing several machines from both common practice and
               digital fabrication research. We find that while Taxon does not
               exhaustively describe all machines, it provides a starting point
               for makers and HCI researchers to develop tools for reasoning
               about and making decisions with machines.",
  series    = "UIST '21",
  month     =  "10~" # oct,
  year      =  2021,
  url       = "https://doi.org/10.1145/3472749.3474779",
  file      = "All Papers/My Library/Tran O'Leary et al. 2021 - Taxon - a language for formal reasoning with digital fabrication machines.pdf",
  doi       = "10.1145/3472749.3474779",
  isbn      =  9781450386357
}

@INPROCEEDINGS{Vatavu2021-lz,
  title     = "{GestuRING: A web-based tool for designing gesture input with
               rings, ring-like, and ring-ready devices}",
  author    = "Vatavu, Radu-Daniel and Bilius, Laura-Bianca",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "710–723",
  abstract  = "Despite an exciting area with many promises for innovations in
               wearable interactive systems, research on interaction techniques
               for smart rings lacks structured knowledge and readily-available
               resources for designers to systematically attain such
               innovations. In this work, we conduct a systematic literature
               review of ring-based gesture input, from which we extract key
               results and a large set of gesture commands for ring, ring-like,
               and ring-ready devices. We use these findings to deliver
               GestuRING, our web-based tool to support design of ring-based
               gesture input. GestuRING features a searchable
               gesture-to-function dictionary of 579 records with downloadable
               numerical data files and an associated YouTube video library.
               These resources are meant to assist the community in attaining
               further innovations in ring-based gesture input for interactive
               systems.",
  series    = "UIST '21",
  month     =  "10~" # oct,
  year      =  2021,
  url       = "https://doi.org/10.1145/3472749.3474780",
  doi       = "10.1145/3472749.3474780",
  isbn      =  9781450386357
}

@INCOLLECTION{He2021-ps,
  title     = "{{GazeChat}: Enhancing Virtual Conferences with Gaze-aware {3D}
               Photos}",
  author    = "He, Zhenyi and Wang, Keru and Feng, Brandon Yushan and Du, Ruofei
               and Perlin, Ken",
  booktitle = "{The 34th Annual {ACM} Symposium on User Interface Software and
               Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "769--782",
  abstract  = "Communication software such as Clubhouse and Zoom has evolved to
               be an integral part of many people's daily lives. However, due to
               network bandwidth constraints and concerns about privacy, cameras
               in video conferencing are often turned off by participants. This
               leads to a situation in which people can only see each others'
               profile images, which is essentially an audio-only experience.
               Even when switched on, video feeds do not provide accurate cues
               as to who is talking to whom. This paper introduces GazeChat, a
               remote communication system that visually represents users as
               gaze-aware 3D profile photos. This satisfies users' privacy needs
               while keeping online conversations engaging and efficient.
               GazeChat uses a single webcam to track whom any participant is
               looking at, then uses neural rendering to animate all
               participants' profile images so that participants appear to be
               looking at each other. We have conducted a remote user study
               (N=16) to evaluate GazeChat in three conditions: audio
               conferencing with profile photos, GazeChat, and video
               conferencing. Based on the results of our user study, we conclude
               that GazeChat maintains the feeling of presence while preserving
               more privacy and requiring lower bandwidth than video
               conferencing, provides a greater level of engagement than to
               audio conferencing, and helps people to better understand the
               structure of their conversation.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474785",
  file      = "All Papers/Other/He et al. 2021 - GazeChat - Enhancing Virtual Conferences with Gaze-aware 3D Photos.pdf",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1145/3472749.3474785",
  isbn      =  9781450386357
}

@INCOLLECTION{He2021-cl,
  title     = "{GazeChat: Enhancing virtual conferences with gaze-aware 3D
               photos}",
  author    = "He, Zhenyi and Wang, Keru and Feng, Brandon Yushan and Du, Ruofei
               and Perlin, Ken",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "769–782",
  abstract  = "Communication software such as Clubhouse and Zoom has evolved to
               be an integral part of many people's daily lives. However, due to
               network bandwidth constraints and concerns about privacy, cameras
               in video conferencing are often turned off by participants. This
               leads to a situation in which people can only see each others'
               profile images, which is essentially an audio-only experience.
               Even when switched on, video feeds do not provide accurate cues
               as to who is talking to whom. This paper introduces GazeChat, a
               remote communication system that visually represents users as
               gaze-aware 3D profile photos. This satisfies users' privacy needs
               while keeping online conversations engaging and efficient.
               GazeChat uses a single webcam to track whom any participant is
               looking at, then uses neural rendering to animate all
               participants' profile images so that participants appear to be
               looking at each other. We have conducted a remote user study
               (N=16) to evaluate GazeChat in three conditions: audio
               conferencing with profile photos, GazeChat, and video
               conferencing. Based on the results of our user study, we conclude
               that GazeChat maintains the feeling of presence while preserving
               more privacy and requiring lower bandwidth than video
               conferencing, provides a greater level of engagement than to
               audio conferencing, and helps people to better understand the
               structure of their conversation.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474785",
  doi       = "10.1145/3472749.3474785",
  isbn      =  9781450386357
}

@INCOLLECTION{Achberger2021-vt,
  title     = "{{STRIVE}: {String-Based} Force Feedback for Automotive
               Engineering}",
  author    = "Achberger, Alexander and Aust, Fabian and Pohlandt, Daniel and
               Vidackovic, Kresimir and Sedlmair, Michael",
  booktitle = "{The 34th Annual {ACM} Symposium on User Interface Software and
               Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "841--853",
  abstract  = "The large potential of force feedback devices for interacting in
               Virtual Reality (VR) has been illustrated in a plethora of
               research prototypes. Yet, these devices are still rarely used in
               practice and it remains an open challenge how to move this
               research into practice. To that end, we contribute a
               participatory design study on the use of haptic feedback devices
               in the automotive industry. Based on a 10-month observing process
               with 13 engineers, we developed STRIVE, a string-based haptic
               feedback device. In addition to the design of STRIVE, this
               process led to a set of requirements for introducing haptic
               devices into industrial settings, which center around a need for
               flexibility regarding forces, comfort, and mobility. We evaluated
               STRIVE with 16 engineers in five different day-to-day automotive
               VR use cases. The main results show an increased level of trust
               and perceived safety as well as further challenges towards moving
               haptics research into practice.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474790",
  doi       = "10.1145/3472749.3474790",
  isbn      =  9781450386357
}

@INCOLLECTION{Achberger2021-qb,
  title     = "{STRIVE: String-Based force feedback for automotive engineering}",
  author    = "Achberger, Alexander and Aust, Fabian and Pohlandt, Daniel and
               Vidackovic, Kresimir and Sedlmair, Michael",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "841–853",
  abstract  = "The large potential of force feedback devices for interacting in
               Virtual Reality (VR) has been illustrated in a plethora of
               research prototypes. Yet, these devices are still rarely used in
               practice and it remains an open challenge how to move this
               research into practice. To that end, we contribute a
               participatory design study on the use of haptic feedback devices
               in the automotive industry. Based on a 10-month observing process
               with 13 engineers, we developed STRIVE, a string-based haptic
               feedback device. In addition to the design of STRIVE, this
               process led to a set of requirements for introducing haptic
               devices into industrial settings, which center around a need for
               flexibility regarding forces, comfort, and mobility. We evaluated
               STRIVE with 16 engineers in five different day-to-day automotive
               VR use cases. The main results show an increased level of trust
               and perceived safety as well as further challenges towards moving
               haptics research into practice.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474790",
  doi       = "10.1145/3472749.3474790",
  isbn      =  9781450386357
}

@INCOLLECTION{Tao2021-ya,
  title     = "{Altering Perceived Softness of Real Rigid Objects by Restricting
               Fingerpad Deformation}",
  author    = "Tao, Yujie and Teng, Shan-Yuan and Lopes, Pedro",
  booktitle = "{The 34th Annual {ACM} Symposium on User Interface Software and
               Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "985--996",
  abstract  = "We propose a haptic device that alters the perceived softness of
               real rigid objects without requiring to instrument the objects.
               Instead, our haptic device works by restricting the user's
               fingerpad lateral deformation via a hollow frame that squeezes
               the sides of the fingerpad. This causes the fingerpad to become
               bulgier than it originally was---when users touch an object's
               surface with their now-restricted fingerpad, they feel the object
               to be softer than it is. To illustrate the extent of softness
               illusion induced by our device, touching the tip of a wooden
               chopstick will feel as soft as a rubber eraser. Our haptic device
               operates by pulling the hollow frame using a motor. Unlike most
               wearable haptic devices, which cover up the user's fingerpad to
               create force sensations, our device creates softness while
               leaving the center of the fingerpad free, which allows the users
               to feel most of the object they are interacting with. This makes
               our device a unique contribution to altering the softness of
               everyday objects, creating ``buttons'' by softening protrusions
               of existing appliances or tangibles, or even, altering the
               softness of handheld props for VR. Finally, we validated our
               device through two studies: (1) a psychophysics study showed that
               the device brings down the perceived softness of any object
               between 50A-90A to around 40A (on Shore A hardness scale); and
               (2) a user study demonstrated that participants preferred our
               device for interactive applications that leverage haptic props,
               such as making a VR prop feel softer or making a rigid 3D printed
               remote control feel softer on its button.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474800",
  file      = "All Papers/Other/Tao et al. 2021 - Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation.pdf",
  doi       = "10.1145/3472749.3474800",
  isbn      =  9781450386357
}

@INCOLLECTION{Tao2021-sz,
  title     = "{Altering perceived softness of real rigid objects by restricting
               fingerpad deformation}",
  author    = "Tao, Yujie and Teng, Shan-Yuan and Lopes, Pedro",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "985–996",
  abstract  = "We propose a haptic device that alters the perceived softness of
               real rigid objects without requiring to instrument the objects.
               Instead, our haptic device works by restricting the user's
               fingerpad lateral deformation via a hollow frame that squeezes
               the sides of the fingerpad. This causes the fingerpad to become
               bulgier than it originally was—when users touch an object's
               surface with their now-restricted fingerpad, they feel the object
               to be softer than it is. To illustrate the extent of softness
               illusion induced by our device, touching the tip of a wooden
               chopstick will feel as soft as a rubber eraser. Our haptic device
               operates by pulling the hollow frame using a motor. Unlike most
               wearable haptic devices, which cover up the user's fingerpad to
               create force sensations, our device creates softness while
               leaving the center of the fingerpad free, which allows the users
               to feel most of the object they are interacting with. This makes
               our device a unique contribution to altering the softness of
               everyday objects, creating “buttons” by softening protrusions of
               existing appliances or tangibles, or even, altering the softness
               of handheld props for VR. Finally, we validated our device
               through two studies: (1) a psychophysics study showed that the
               device brings down the perceived softness of any object between
               50A-90A to around 40A (on Shore A hardness scale); and (2) a user
               study demonstrated that participants preferred our device for
               interactive applications that leverage haptic props, such as
               making a VR prop feel softer or making a rigid 3D printed remote
               control feel softer on its button.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474800",
  doi       = "10.1145/3472749.3474800",
  isbn      =  9781450386357
}

@INCOLLECTION{Zhang2021-bm,
  title     = "{Evaluating the Effects of Saccade Types and Directions on Eye
               Pointing Tasks}",
  author    = "Zhang, Xinyong",
  booktitle = "{The 34th Annual {ACM} Symposium on User Interface Software and
               Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1221--1234",
  abstract  = "With the portable and affordable gaze input devices being
               marketed for end users, gaze-based interactions were getting
               increasingly popular. Unfortunately, the understanding about the
               dominant task of gaze input, i.e. eye pointing task, was still
               not sufficient although a performance model had been specifically
               proposed in previous study because of that 1) the original model
               was based on a specific circular target condition, without the
               ability to predict the performance of acquiring conventional
               rectangular targets and that 2) there was a lack of explanation
               from the perspective of the anatomical structure of the eyes. In
               this paper, we proposed a 2D extension to take account of more
               general target conditions. Carrying out two experiments, we
               evaluated the effectiveness of the new model and furthermore we
               found that the index of difficulty that we redefined for 2D eye
               pointing (IDeye) was able to properly reflect the asymmetrical
               impacts of target width and height, and consequently the IDeye
               model could more accurately and properly predict the performance
               when acquiring 2D targets than Fitts' law, no matter what kind of
               saccades or eye orientations (i.e. saccadic eye movement
               directions) was employed to acquire the desired targets.
               According to the results, we provided more useful implications
               and recommendations for gaze-based applications.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474818",
  keywords  = "uist2022-gaze-design",
  doi       = "10.1145/3472749.3474818",
  isbn      =  9781450386357
}

@INCOLLECTION{Zhang2021-lh,
  title     = "{Evaluating the effects of saccade types and directions on eye
               pointing tasks}",
  author    = "Zhang, Xinyong",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1221–1234",
  abstract  = "With the portable and affordable gaze input devices being
               marketed for end users, gaze-based interactions were getting
               increasingly popular. Unfortunately, the understanding about the
               dominant task of gaze input, i.e. eye pointing task, was still
               not sufficient although a performance model had been specifically
               proposed in previous study because of that 1) the original model
               was based on a specific circular target condition, without the
               ability to predict the performance of acquiring conventional
               rectangular targets and that 2) there was a lack of explanation
               from the perspective of the anatomical structure of the eyes. In
               this paper, we proposed a 2D extension to take account of more
               general target conditions. Carrying out two experiments, we
               evaluated the effectiveness of the new model and furthermore we
               found that the index of difficulty that we redefined for 2D eye
               pointing (IDeye) was able to properly reflect the asymmetrical
               impacts of target width and height, and consequently the IDeye
               model could more accurately and properly predict the performance
               when acquiring 2D targets than Fitts' law, no matter what kind of
               saccades or eye orientations (i.e. saccadic eye movement
               directions) was employed to acquire the desired targets.
               According to the results, we provided more useful implications
               and recommendations for gaze-based applications.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3472749.3474818",
  doi       = "10.1145/3472749.3474818",
  isbn      =  9781450386357
}

@INPROCEEDINGS{Marquardt2021-hz,
  title     = "{AirConstellations: In-air device formations for cross-device
               interaction via multiple spatially-aware armatures}",
  author    = "Marquardt, Nicolai and Henry Riche, Nathalie and Holz, Christian
               and Romat, Hugo and Pahud, Michel and Brudy, Frederik and Ledo,
               David and Park, Chunjong and Nicholas, Molly Jane and Seyed,
               Teddy and Ofek, Eyal and Lee, Bongshin and Buxton, William A S
               and Hinckley, Ken",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1252–1268",
  abstract  = "AirConstellations supports a unique semi-fixed style of
               cross-device interactions via multiple self-spatially-aware
               armatures to which users can easily attach (or detach) tablets
               and other devices. In particular, AirConstellations affords
               highly flexible and dynamic device formations where the users can
               bring multiple devices together in-air — with 2–5 armatures
               poseable in 7DoF within the same workspace — to suit the demands
               of their current task, social situation, app scenario, or
               mobility needs. This affords an interaction metaphor where
               relative orientation, proximity, attaching (or detaching)
               devices, and continuous movement into and out of ad-hoc ensembles
               can drive context-sensitive interactions. Yet all devices remain
               self-stable in useful configurations even when released in
               mid-air. We explore flexible physical arrangement, feedforward of
               transition options, and layering of devices in-air across a
               variety of multi-device app scenarios. These include video
               conferencing with flexible arrangement of the person-space of
               multiple remote participants around a shared task-space, layered
               and tiled device formations with overview+detail and
               shared-to-personal transitions, and flexible composition of UI
               panels and tool palettes across devices for productivity
               applications. A preliminary interview study highlights user
               reactions to AirConstellations, such as for minimally disruptive
               device formations, easier physical transitions, and balancing
               ”seeing and being seen” in remote work.",
  series    = "UIST '21",
  month     =  "10~" # oct,
  year      =  2021,
  url       = "https://doi.org/10.1145/3472749.3474820",
  file      = "All Papers/My Library/Marquardt et al. 2021 - AirConstellations - In-air device formations for cross-device interaction via multiple spatially-aware armatures.pdf",
  doi       = "10.1145/3472749.3474820",
  isbn      =  9781450386357
}

@INPROCEEDINGS{Suzuki2021-hf,
  title     = "{HapticBots: Distributed encountered-type haptics for VR with
               multiple shape-changing mobile robots}",
  author    = "Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger,
               Daniel and Gonzalez-Franco, Mar",
  booktitle = "{The 34th annual ACM symposium on user interface software and
               technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1269–1281",
  abstract  = "HapticBots introduces a novel encountered-type haptic approach
               for Virtual Reality (VR) based on multiple tabletop-size
               shape-changing robots. These robots move on a tabletop and change
               their height and orientation to haptically render various
               surfaces and objects on-demand. Compared to previous
               encountered-type haptic approaches like shape displays or robotic
               arms, our proposed approach has an advantage in deployability,
               scalability, and generalizability—these robots can be easily
               deployed due to their compact form factor. They can support
               multiple concurrent touch points in a large area thanks to the
               distributed nature of the robots. We propose and evaluate a novel
               set of interactions enabled by these robots which include: 1)
               rendering haptics for VR objects by providing just-in-time
               touch-points on the user's hand, 2) simulating continuous
               surfaces with the concurrent height and position change, and 3)
               enabling the user to pick up and move VR objects through
               graspable proxy objects. Finally, we demonstrate HapticBots with
               various applications, including remote collaboration, education
               and training, design and 3D modeling, and gaming and
               entertainment.",
  series    = "UIST '21",
  month     =  "10~" # dec,
  year      =  2021,
  url       = "https://doi.org/10.1145/3472749.3474821",
  file      = "All Papers/My Library/Suzuki et al. 2021 - HapticBots - Distributed encountered-type haptics for VR with multiple shape-changing mobile robots.pdf",
  doi       = "10.1145/3472749.3474821",
  isbn      =  9781450386357
}

@ARTICLE{Ramirez_Gomez2021-cy,
  title    = "{Eyesthetics: Making Sense of the Aesthetics of Playing with Gaze}",
  author   = "Ramirez Gomez, Argenis and Lankes, Michael",
  journal  = "Proceedings of the ACM on Human-Computer Interaction",
  volume   =  5,
  number   = "CHI PLAY",
  pages    = "259:1–259:24",
  abstract = "Gaze interaction has been growing fast as a compelling tool for
              control and immersion for gameplay. Here, we present a conceptual
              framework focusing on the aesthetic player experience and the
              potential interpretation (meaning) players could give to playing
              with gaze interaction capabilities. The framework is illustrated
              by a survey of state of the art research-based and commercial
              games. We complement existing frameworks by reflecting on gaze
              interaction in games as the attention relationship between the
              player (the subject) and the game (the object) with four
              dimensions: Identity; Mapping; Attention; and Direction. The
              framework serves as a design and inquiry toolbox to analyse and
              communicate gaze mechanics in games, reflect on the complexity of
              gaze interaction, and formulate new research questions. We
              visualise the resulting design space, highlighting future
              opportunities for gaze interaction design and HCI gaze research
              through the framework's lens. We deem, this novel approach
              advocates for the design of gaze-based interactions revealing the
              richness of gaze input in future meaningful game experiences.",
  year     =  2021,
  url      = "https://dl.acm.org/doi/10.1145/3474686",
  doi      = "10.1145/3474686"
}

@INPROCEEDINGS{Momose2021-qr,
  title     = "{Multimodal Feedback Pen Shaped Interface and MR Application with
               Spatial Reality Display}",
  author    = "Momose, Jun and Koda, Yuta and Mori, Hideki and Kakiuchi, Morio
               and Imamura, Kotaro and Wakabayashi, Makoto",
  booktitle = "{SIGGRAPH Asia 2021 Emerging Technologies}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "Multimodal interface is essential to enrich the reality of
               drawing in the virtual 3D environment. We propose the pen shaped
               interface capable of providing the following multimodal
               feedbacks: (1) Linear motion force feedback to express contact
               pressure of virtual object, (2) Rotational force feedback to
               simulate friction of rubbing virtual surface and tangential
               contact force with virtual object, (3) Vibrotactile feedback, and
               (4) auditory feedback to express contact information and texture
               of virtual object. We developed Mixed Reality (MR) interaction
               system with pen shaped interface and Spatial Reality Display.
               This system will display virtual pen tip extended from the actual
               pen shaped interface, and user can use it to draw into virtual
               workspace as well as interact with virtual objects. This
               demonstrates the advantage of our proposal by improving the
               reality of virtual space interaction.",
  series    = "SA '21",
  year      =  2021,
  url       = "https://doi.org/10.1145/3476122.3484834",
  doi       = "10.1145/3476122.3484834",
  isbn      =  9781450386852
}

@ARTICLE{Cheng2021-el,
  title    = "{Duco: Autonomous large-scale direct-circuit-writing (DCW) on
              vertical everyday surfaces using A scalable hanging plotter}",
  author   = "Cheng, Tingyu and Li, Bu and Zhang, Yang and Li, Yunzhi and Ramey,
              Charles and Jung, Eui Min and Cui, Yepu and Swaminathan, Sai
              Ganesh and Do, Youngwook and Tentzeris, Manos and Abowd, Gregory D
              and Oh, Hyunjoo",
  journal  = "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.",
  volume   =  5,
  number   =  3,
  pages    = "1–25",
  abstract = "Human environments are filled with large open spaces that are
              separated by structures like walls, facades, glass windows, etc.
              Most often, these structures are largely passive offering little
              to no interactivity. In this paper, we present Duco, a large-scale
              electronics fabrication robot that enables room-scale \&
              building-scale circuitry to add interactivity to vertical everyday
              surfaces. Duco negates the need for any human intervention by
              leveraging a hanging robotic system that automatically sketches
              multi-layered circuity to enable novel large-scale interfaces. The
              key idea behind Duco is that it achieves single-layer or
              multi-layer circuit fabrication on 2D surfaces as well as 2D
              cutouts that can be assembled into 3D objects by loading various
              functional inks (e.g., conductive, dielectric, or cleaning) to the
              wall-hanging drawing robot, as well as employing an optional laser
              cutting head as a cutting tool. Our technical evaluation shows
              that Duco's mechanical system works reliably on various surface
              materials with a wide range of roughness and surface morphologies.
              The system achieves superior mechanical tolerances (0.1mm XY axis
              resolution and 1mm smallest feature size). We demonstrate our
              system with five application examples, including an interactive
              piano, an IoT coffee maker controller, an FM energy-harvester
              printed on a large glass window, a human-scale touch sensor and a
              3D interactive lamp.",
  month    =  "14~" # sep,
  year     =  2021,
  url      = "https://doi.org/10.1145/3478118",
  doi      = "10.1145/3478118"
}

@ARTICLE{Lawrence2021-mm,
  title    = "{Project starline: a high-fidelity telepresence system}",
  author   = "Lawrence, Jason and Goldman, Danb and Achar, Supreeth and
              Blascovich, Gregory Major and Desloge, Joseph G and Fortes, Tommy
              and Gomez, Eric M and Häberling, Sascha and Hoppe, Hugues and
              Huibers, Andy and Knaus, Claude and Kuschak, Brian and
              Martin-Brualla, Ricardo and Nover, Harris and Russell, Andrew Ian
              and Seitz, Steven M and Tong, Kevin",
  journal  = "ACM transactions on graphics",
  volume   =  40,
  number   =  6,
  pages    = "242:1–242:16",
  abstract = "We present a real-time bidirectional communication system that
              lets two people, separated by distance, experience a face-to-face
              conversation as if they were copresent. It is the first
              telepresence system that is demonstrably better than 2D
              videoconferencing, as measured using participant ratings (e.g.,
              presence, attentiveness, reaction-gauging, engagement), meeting
              recall, and observed nonverbal behaviors (e.g., head nods, eyebrow
              movements). This milestone is reached by maximizing audiovisual
              fidelity and the sense of copresence in all design elements,
              including physical layout, lighting, face tracking, multi-view
              capture, microphone array, multi-stream compression, loudspeaker
              output, and lenticular display. Our system achieves key 3D
              audiovisual cues (stereopsis, motion parallax, and spatialized
              audio) and enables the full range of communication cues (eye
              contact, hand gestures, and body language), yet does not require
              special glasses or body-worn microphones/headphones. The system
              consists of a head-tracked autostereoscopic display,
              high-resolution 3D capture and rendering subsystems, and network
              transmission using compressed color and depth video streams. Other
              contributions include a novel image-based geometry fusion
              algorithm, free-space dereverberation, and talker localization.",
  year     =  2021,
  url      = "https://dl.acm.org/doi/10.1145/3478513.3480490",
  file     = "All Papers/My Library/Lawrence et al. 2021 - Project starline - a high-fidelity telepresence system.pdf",
  doi      = "10.1145/3478513.3480490",
  issn     = "0730-0301"
}

@ARTICLE{Lawrence2021-go,
  title    = "{Project starline: a high-fidelity telepresence system}",
  author   = "Lawrence, Jason and Goldman, Danb and Achar, Supreeth and
              Blascovich, Gregory Major and Desloge, Joseph G and Fortes, Tommy
              and Gomez, Eric M and Häberling, Sascha and Hoppe, Hugues and
              Huibers, Andy and Knaus, Claude and Kuschak, Brian and
              Martin-Brualla, Ricardo and Nover, Harris and Russell, Andrew Ian
              and Seitz, Steven M and Tong, Kevin",
  journal  = "ACM transactions on graphics",
  volume   =  40,
  number   =  6,
  pages    = "1–16",
  abstract = "We present a real-time bidirectional communication system that
              lets two people, separated by distance, experience a face-to-face
              conversation as if they were copresent. It is the first
              telepresence system that is demonstrably better than 2D
              videoconferencing, as measured using participant ratings (e.g.,
              presence, attentiveness, reaction-gauging, engagement), meeting
              recall, and observed nonverbal behaviors (e.g., head nods, eyebrow
              movements). This milestone is reached by maximizing audiovisual
              fidelity and the sense of copresence in all design elements,
              including physical layout, lighting, face tracking, multi-view
              capture, microphone array, multi-stream compression, loudspeaker
              output, and lenticular display. Our system achieves key 3D
              audiovisual cues (stereopsis, motion parallax, and spatialized
              audio) and enables the full range of communication cues (eye
              contact, hand gestures, and body language), yet does not require
              special glasses or body-worn microphones/headphones. The system
              consists of a head-tracked autostereoscopic display,
              high-resolution 3D capture and rendering subsystems, and network
              transmission using compressed color and depth video streams. Other
              contributions include a novel image-based geometry fusion
              algorithm, free-space dereverberation, and talker localization.",
  month    =  "12~" # oct,
  year     =  2021,
  url      = "https://doi.org/10.1145/3478513.3480490",
  file     = "All Papers/My Library/Lawrence et al. 2021 - Project starline - a high-fidelity telepresence system.pdf",
  doi      = "10.1145/3478513.3480490",
  issn     = "0730-0301"
}

@INPROCEEDINGS{Takeuchi2021-nk,
  title     = "{{GIBSON}: {AR/VR} synchronized city walking system}",
  author    = "Takeuchi, Seiichiro and Hashiguchi, Kyoko and Homma, Yuki and
               Kajitani, Kent and Meguro, Shingo",
  booktitle = "{{SIGGRAPH} Asia 2021 {XR}}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--2",
  abstract  = "GIBSON is a novel city walking system that enables distant users
               to walk together as if they are physically in the same city.The
               advancement of virtual reality technology has opened the
               possibility to travel around the world virtually beyond
               geographical limitations, but there is still room for improvement
               to make the experience as realistic as real travel. Unlike
               conventional virtual travel tools and prior multi-user
               collaborative XR studies, we designed our system to evoke both a
               sense of co-presence and a sense of being in the real space. For
               this purpose, we implemented two main functions: (1) function to
               transfer real-time audio-visual information of the surroundings
               and (2) function to transfer body movements of users through
               avatars. We also combined visual positioning system (VPS) and
               SLAM to align the user locations.We conducted user testing to
               verify the experience of cross-AR/VR city walking using GIBSON.
               The result suggests that our system could make people feel as if
               they were walking together in the city even though they are
               physically distanced.",
  series    = "SA '21 XR",
  month     =  dec,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3478514.3487638",
  doi       = "10.1145/3478514.3487638",
  isbn      =  9781450390750
}

@INPROCEEDINGS{Takeuchi2021-xz,
  title     = "{GIBSON: AR/VR synchronized city walking system}",
  author    = "Takeuchi, Seiichiro and Hashiguchi, Kyoko and Homma, Yuki and
               Kajitani, Kent and Meguro, Shingo",
  booktitle = "{SIGGRAPH asia 2021 XR}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "GIBSON is a novel city walking system that enables distant users
               to walk together as if they are physically in the same city.The
               advancement of virtual reality technology has opened the
               possibility to travel around the world virtually beyond
               geographical limitations, but there is still room for improvement
               to make the experience as realistic as real travel. Unlike
               conventional virtual travel tools and prior multi-user
               collaborative XR studies, we designed our system to evoke both a
               sense of co-presence and a sense of being in the real space. For
               this purpose, we implemented two main functions: (1) function to
               transfer real-time audio-visual information of the surroundings
               and (2) function to transfer body movements of users through
               avatars. We also combined visual positioning system (VPS) and
               SLAM to align the user locations.We conducted user testing to
               verify the experience of cross-AR/VR city walking using GIBSON.
               The result suggests that our system could make people feel as if
               they were walking together in the city even though they are
               physically distanced.",
  series    = "SA '21 XR",
  month     =  dec,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3478514.3487638",
  doi       = "10.1145/3478514.3487638",
  isbn      =  9781450390750
}

@ARTICLE{Abdullah2021-ju,
  title     = "{Videoconference and Embodied {VR}: Communication Patterns Across
               Task and Medium}",
  author    = "Abdullah, Ahsan and Kolkmeier, Jan and Lo, Vivian and Neff,
               Michael",
  journal   = "Proc. ACM Hum. -Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "CSCW2",
  pages     = "1--29",
  abstract  = "Videoconference has become the dominant technology for remote
               meetings. Embodied Virtual Reality is a potential alternative
               that employs motion tracking in order to place people in a shared
               virtual environment as avatars. This paper describes a 210
               participant study focused on behavioral measures that compares
               multiparty interaction in videoconference and embodied VR across
               a range of task types: a factual intellective task, a subjective
               judgment task and two negotiation tasks, one with visual
               grounding. It uses state-of-the-art body, face and finger
               tracking to drive the avatars in VR and a carefully matched
               videoconferencing implementation. Significant behavioral
               differences are observed. These include increased activity in
               videoconference related to maintaining the social connection:
               more person directed gaze and increased verbal and nonverbal
               backchannel behavior. Videoconference also had reduced
               conversational overlap, increased self-adaptor gestures and
               reduced deictic gestures as compared with embodied VR. Potential
               explanations and implications are discussed.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3479597",
  file      = "All Papers/Other/Abdullah et al. 2021 - Videoconference and Embodied VR - Communication Patterns Across Task and Medium.pdf",
  keywords  = "remote meetings, nonverbal behavior, virtual reality,
               telepresence, embodied VR, VR, multiparty interaction,
               gaze;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/3479597"
}

@ARTICLE{Abdullah2021-je,
  title     = "{Videoconference and Embodied {VR}: Communication Patterns Across
               Task and Medium}",
  author    = "Abdullah, Ahsan and Kolkmeier, Jan and Lo, Vivian and Neff,
               Michael",
  journal   = "Proc. ACM Hum. -Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "CSCW2",
  pages     = "1--29",
  abstract  = "Videoconference has become the dominant technology for remote
               meetings. Embodied Virtual Reality is a potential alternative
               that employs motion tracking in order to place people in a shared
               virtual environment as avatars. This paper describes a 210
               participant study focused on behavioral measures that compares
               multiparty interaction in videoconference and embodied VR across
               a range of task types: a factual intellective task, a subjective
               judgment task and two negotiation tasks, one with visual
               grounding. It uses state-of-the-art body, face and finger
               tracking to drive the avatars in VR and a carefully matched
               videoconferencing implementation. Significant behavioral
               differences are observed. These include increased activity in
               videoconference related to maintaining the social connection:
               more person directed gaze and increased verbal and nonverbal
               backchannel behavior. Videoconference also had reduced
               conversational overlap, increased self-adaptor gestures and
               reduced deictic gestures as compared with embodied VR. Potential
               explanations and implications are discussed.",
  month     =  oct,
  year      =  2021,
  url       = "http://dx.doi.org/10.1145/3479597",
  file      = "All Papers/Other/Abdullah et al. 2021 - Videoconference and Embodied VR - Communication Patterns Across Task and Medium.pdf",
  keywords  = "VR, embodied VR, virtual reality, gaze, multiparty interaction,
               nonverbal behavior, remote meetings, telepresence",
  doi       = "10.1145/3479597"
}

@ARTICLE{Abdullah2021-fe,
  title    = "{Videoconference and embodied VR: Communication patterns across
              task and medium}",
  author   = "Abdullah, Ahsan and Kolkmeier, Jan and Lo, Vivian and Neff,
              Michael",
  journal  = "Proc. ACM Hum. -Comput. Interact.",
  volume   =  5,
  number   = "CSCW2",
  pages    = "1–29",
  abstract = "Videoconference has become the dominant technology for remote
              meetings. Embodied Virtual Reality is a potential alternative that
              employs motion tracking in order to place people in a shared
              virtual environment as avatars. This paper describes a 210
              participant study focused on behavioral measures that compares
              multiparty interaction in videoconference and embodied VR across a
              range of task types: a factual intellective task, a subjective
              judgment task and two negotiation tasks, one with visual
              grounding. It uses state-of-the-art body, face and finger tracking
              to drive the avatars in VR and a carefully matched
              videoconferencing implementation. Significant behavioral
              differences are observed. These include increased activity in
              videoconference related to maintaining the social connection: more
              person directed gaze and increased verbal and nonverbal
              backchannel behavior. Videoconference also had reduced
              conversational overlap, increased self-adaptor gestures and
              reduced deictic gestures as compared with embodied VR. Potential
              explanations and implications are discussed.",
  month    =  oct,
  year     =  2021,
  url      = "http://dx.doi.org/10.1145/3479597",
  file     = "All Papers/My Library/Abdullah et al. 2021 - Videoconference and embodied VR - Communication patterns across task and medium.pdf",
  doi      = "10.1145/3479597"
}

@INPROCEEDINGS{Lu2021-no,
  title     = "{Exploration of Techniques for Rapid Activation of Glanceable
               Information in Head-Worn Augmented Reality}",
  author    = "Lu, Feiyu and Davari, Shakiba and Bowman, Doug",
  booktitle = "{Proceedings of the 2021 ACM Symposium on Spatial User
               Interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–11",
  abstract  = "Future augmented reality (AR) glasses may provide pervasive and
               continuous access to everyday information. However, it remains
               unclear how to address the issue of virtual information
               overlaying and occluding real-world objects and information that
               are of interest to users. One approach is to keep virtual
               information sources inactive until they are explicitly requested,
               so that the real world remains visible. In this research, we
               explored the design of interaction techniques with which users
               can activate virtual information sources in AR. We studied this
               issue in the context of Glanceable AR, in which virtual
               information resides at the periphery of the user’s view. We
               proposed five techniques and evaluated them in both sitting and
               walking scenarios. Our results demonstrate the usability, user
               preference, and social acceptance of each technique, as well as
               design recommendations to achieve optimal performance. Our
               findings can inform the design of lightweight techniques to
               activate virtual information displays in future everyday AR
               interfaces.",
  series    = "SUI '21",
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3485279.3485286",
  file      = "All Papers/My Library/Lu et al. 2021 - Exploration of Techniques for Rapid Activation of Glanceable Information in Head-Worn Augmented Reality.pdf",
  doi       = "10.1145/3485279.3485286",
  isbn      =  9781450390910
}

@INPROCEEDINGS{Zhao2022-cd,
  title     = "{EyeSayCorrect: Eye gaze and voice based hands-free text
               correction for mobile devices}",
  author    = "Zhao, Maozheng and Huang, Henry and Li, Zhi and Liu, Rui and Cui,
               Wenzhe and Toshniwal, Kajal and Goel, Ananya and Wang, Andrew and
               Zhao, Xia and Rashidian, Sina and Baig, Furqan and Phi, Khiem and
               Zhai, Shumin and Ramakrishnan, I V and Wang, Fusheng and Bi,
               Xiaojun",
  booktitle = "{27th international conference on intelligent user interfaces}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "470–482",
  abstract  = "Text correction on mobile devices usually requires precise and
               repetitive manual control. In this paper, we present
               EyeSayCorrect, an eye gaze and voice based hands-free text
               correction method for mobile devices. To correct text with
               EyeSayCorrect, the user first utilizes the gaze location on the
               screen to select a word, then speaks the new phrase.
               EyeSayCorrect would then infer the user's correction intention
               based on the inputs and the text context. We used a Bayesian
               approach for determining the selected word given an eye-gaze
               trajectory. Given each sampling point in an eye-gaze trajectory,
               the posterior probability of selecting a word is calculated and
               accumulated. The target word would be selected when its
               accumulated interest is larger than a threshold. The misspelt
               words have higher priors. Our user studies showed that using
               priors for misspelt words reduced the task completion time up to
               23.79\% and the text selection time up to 40.35\%, and
               EyeSayCorrect is a feasible hands-free text correction method on
               mobile devices.",
  series    = "IUI '22",
  month     =  "22~" # mar,
  year      =  2022,
  url       = "https://doi.org/10.1145/3490099.3511103",
  file      = "All Papers/My Library/Zhao et al. 2022 - EyeSayCorrect - Eye gaze and voice based hands-free text correction for mobile devices.pdf",
  doi       = "10.1145/3490099.3511103",
  isbn      =  9781450391443
}

@INPROCEEDINGS{Murthy2022-es,
  title     = "{Distraction detection in automotive environment using
               appearance-based gaze estimation}",
  author    = "{Murthy} and Mukhopadhyay, Abhishek and Biswas, Pradipta",
  booktitle = "{27th International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "22~" # mar,
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3490100.3516463",
  file      = "All Papers/Other/Murthy et al. 2022 - Distraction detection in automotive environment using appearance-based gaze estimation.pdf",
  doi       = "10.1145/3490100.3516463"
}

@INPROCEEDINGS{Rekimoto2022-wi,
  title     = "{DualVoice: A speech interaction method using whisper-voice as
               commands}",
  author    = "Rekimoto, Jun",
  booktitle = "{Extended abstracts of the 2022 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–6",
  abstract  = "Applications based on speech recognition have become widely used,
               and speech input is increasingly being utilized to create
               documents. However, it is still difficult to correct
               misrecognition by speech, which makes it necessary to re-edit
               documents by other means such as manual input. It is also
               difficult to input symbols and commands because these may be
               misrecognized as text letters. To deal with these problems, we
               propose a speech interaction method called DualVoice in which
               commands are input in a whispered voice and letters in a normal
               voice. The proposed method does not require any special hardware
               other than a regular microphone, thus enabling a complete
               hands-free interaction, and it can be used in a wide range of
               situations where speech recognition is already available. We
               designed two neural networks, one for discriminating normal
               speech from whispered speech, and the second for recognizing
               whisper speech.",
  series    = "CHI EA '22",
  month     =  "27~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491101.3519700",
  file      = "All Papers/My Library/Rekimoto 2022 - DualVoice - A speech interaction method using whisper-voice as commands.pdf",
  doi       = "10.1145/3491101.3519700",
  isbn      =  9781450391566
}

@INPROCEEDINGS{Lee2022-oe,
  title     = "{Auditory and olfactory stimuli-based attractors to induce
               reorientation in virtual reality forward redirected walking}",
  author    = "Lee, Jieun and Hwang, Seokhyun and Kim, Kyunghwan and Kim,
               Seungjun",
  booktitle = "{Extended abstracts of the 2022 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Redirected walking (RDW) visually manipulates the virtual
               environment to imperceptibly redirect the walkers to keep them in
               the tracking area, and offers a larger space than physical space.
               An attractor is a redirected walking technique that captures the
               walker's attention and manipulates the walker's trajectory
               through rotational gain. However, the attractor visually
               manipulates the walker's virtual environment using a predefined
               rotational gain, and having to constantly gaze at the attractor
               or the attractor frequently appearing until the walker's
               direction matches the desired direction, are problems limiting
               the application of visual attractors. Moreover, when the walker
               is unable to recognize or ignores the attractor, reorientation
               fails. In this study, we designed a human-sense-stimulating
               attractor that utilizes the auditory and olfactory senses to
               improve the rotational gain, naturalness, and immersion and
               decrease the chance of reorientation failure. Although sound and
               scents are invisible, they can be detected through direction;
               however, humans cannot recognize the accurate direction of a
               sound or scent. Based on these characteristics, auditory and
               olfactory attractors are proposed. We measured the amount of
               reorientation induced by the auditory and olfactory attractors
               and calculated the reorientation success rate. Additionally, the
               naturalness and immersion of the attractor were evaluated. The
               auditory attractor has a high reorientation success rate,
               naturalness, and immersion. The olfactory attractor induces more
               turn changes in the walker than other attractors, and a high
               number of turn changes leads to a larger rotational gain.
               Auditory and olfactory-based attractors have the potential to
               overcome the shortcomings of visual attractors such as the
               frequent interventions.",
  series    = "CHI EA '22",
  month     =  "27~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491101.3519719",
  file      = "All Papers/My Library/Lee et al. 2022 - Auditory and olfactory stimuli-based attractors to induce reorientation in virtual reality forward redirected walking.pdf",
  doi       = "10.1145/3491101.3519719",
  isbn      =  9781450391566
}

@INPROCEEDINGS{Gumilar2022-fp,
  title     = "{Inter-brain synchrony and eye gaze direction during
               collaboration in {VR}}",
  author    = "Gumilar, Ihshan and Barde, Amit and Sasikumar, Prasanth and
               Billinghurst, Mark and Hayati, Ashkan F and Lee, Gun and Munarko,
               Yuda and Singh, Sanjit and Momin, Abdul",
  booktitle = "{Extended abstracts of the 2022 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Brain activity sometimes synchronises when people collaborate
               together on real world tasks. Understanding this process could to
               lead to improvements in face to face and remote collaboration. In
               this paper we report on an experiment exploring the relationship
               between eye gaze and inter-brain synchrony in Virtual Reality
               (VR). The experiment recruited pairs who were asked to perform
               finger-tracking exercises in VR with three different gaze
               conditions: averted, direct, and natural, while their brain
               activity was recorded. We found that gaze direction has a
               significant effect on inter-brain synchrony during collaboration
               for this task in VR. This shows that representing natural gaze
               could influence inter-brain synchrony in VR, which may have
               implications for avatar design for social VR. We discuss
               implications of our research and possible directions for future
               work.",
  series    = "CHI EA '22",
  month     =  "27~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491101.3519746",
  doi       = "10.1145/3491101.3519746",
  isbn      =  9781450391566
}

@INPROCEEDINGS{Langner2022-xk,
  title     = "{EyeMeet: A joint attention support system for remote meetings}",
  author    = "Langner, Moritz and Toreini, Peyman and Maedche, Alexander",
  booktitle = "{Extended abstracts of the 2022 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "A major challenge in remote meetings is that awareness cues, such
               as gaze, become degraded despite playing a crucial role in
               communication and establishing joint attention. Eye tracking
               allows overcoming these obstacles by enabling augmentation of
               remote meetings with gaze information. In this project, we
               followed a participatory approach by first distributing a
               scenario-based survey to students (n=79) to uncover their
               preference of eye-based joint attention support (real-time,
               retrospective, real-time \& retrospective, no) for remote
               university meetings. Building on these findings, we developed
               EyeMeet, an eye-based joint attention support system that
               combines state-of-the-art real-time joint attention support with
               a retrospective attention feedback for remote meetings. In a
               four-week study, two student groups worked remotely on course
               assignments using EyeMeet. Our findings of the study highlight
               that EyeMeets supports students in staying more focused on the
               meetings. Complementing real-time joint attention support,
               retrospective joint attention feedback is recognized to provide
               valuable support for reflecting and adapting behavior for
               upcoming meetings.",
  series    = "CHI EA '22",
  month     =  "27~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491101.3519792",
  doi       = "10.1145/3491101.3519792",
  isbn      =  9781450391566
}

@INPROCEEDINGS{Mitchell2022-hv,
  title     = "{Ability-based keyboards for augmentative and alternative
               communication: Understanding how individuals' movement patterns
               translate to more efficient keyboards: Methods to generate
               keyboards tailored to user-specific motor abilities}",
  author    = "Mitchell, Claire and Cler, Gabriel and Fager, Susan and Contessa,
               Paola and Roy, Serge and De Luca, Gianluca and Kline, Joshua and
               Vojtech, Jennifer",
  booktitle = "{Extended abstracts of the 2022 CHI conference on human factors
               in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "This study presents the evaluation of ability-based methods
               extended to keyboard generation for alternative communication in
               people with dexterity impairments due to motor disabilities. Our
               approach characterizes user-specific cursor control abilities
               from a multidirectional point-select task to configure letters on
               a virtual keyboard based on estimated time, distance, and
               direction of movement. These methods were evaluated in three
               individuals with motor disabilities against a generically
               optimized keyboard and the ubiquitous QWERTY keyboard. We
               highlight key observations relating to the heterogeneity of the
               manifestation of motor disabilities, perceived importance of
               communication technology, and quantitative improvements in
               communication performance when characterizing an individual's
               movement abilities to design personalized AAC interfaces.",
  series    = "CHI EA '22",
  month     =  "27~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491101.3519845",
  file      = "All Papers/My Library/Mitchell et al. 2022 - Ability-based keyboards for augmentative and alte ... te to more efficient keyboards - Methods to generate keyboards tailo[...]",
  doi       = "10.1145/3491101.3519845",
  isbn      =  9781450391566
}

@INPROCEEDINGS{Fender2022-xc,
  title     = "{Causality-preserving asynchronous reality}",
  author    = "Fender, Andreas Rene and Holz, Christian",
  booktitle = "{CHI conference on human factors in computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–15",
  abstract  = "Mixed Reality is gaining interest as a platform for collaboration
               and focused work to a point where it may supersede current office
               settings in future workplaces. At the same time, we expect that
               interaction with physical objects and face-to-face communication
               will remain crucial for future work environments, which is a
               particular challenge in fully immersive Virtual Reality. In this
               work, we reconcile those requirements through a user's individual
               Asynchronous Reality, which enables seamless physical interaction
               across time. When a user is unavailable, e.g., focused on a task
               or in a call, our approach captures co-located or remote physical
               events in real-time, constructs a causality graph of co-dependent
               events, and lets immersed users revisit them at a suitable time
               in a causally accurate way. Enabled by our system AsyncReality,
               we present a workplace scenario that includes walk-in
               interruptions during a person's focused work, physical
               deliveries, and transient spoken messages. We then generalize our
               approach to a use-case agnostic concept and system architecture.
               We conclude by discussing the implications of Asynchronous
               Reality for future offices.",
  series    = "CHI '22",
  month     =  apr,
  year      =  2022,
  url       = "http://dx.doi.org/10.1145/3491102.3501836",
  file      = "All Papers/My Library/Fender and Holz 2022 - Causality-preserving asynchronous reality.pdf",
  doi       = "10.1145/3491102.3501836",
  isbn      =  9781450391573
}

@INPROCEEDINGS{He2022-em,
  title     = "{TapGazer: Text entry with finger tapping and gaze-directed word
               selection}",
  author    = "He, Zhenyi and Lutteroth, Christof and Perlin, Ken",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–16",
  abstract  = "While using VR, efficient text entry is a challenge: users cannot
               easily locate standard physical keyboards, and keys are often out
               of reach, e.g. when standing. We present TapGazer, a text entry
               system where users type by tapping their fingers in place. Users
               can tap anywhere as long as the identity of each tapping finger
               can be detected with sensors. Ambiguity between different
               possible input words is resolved by selecting target words with
               gaze. If gaze tracking is unavailable, ambiguity is resolved by
               selecting target words with additional taps. We evaluated
               TapGazer for seated and standing VR: seated novice users using
               touchpads as tap surfaces reached 44.81 words per minute (WPM),
               79.17\% of their QWERTY typing speed. Standing novice users
               tapped on their thighs with touch-sensitive gloves, reaching
               45.26 WPM (71.91\%). We analyze TapGazer with a theoretical
               performance model and discuss its potential for text input in
               future AR scenarios.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3501838",
  file      = "All Papers/My Library/He et al. 2022 - TapGazer - Text entry with finger tapping and gaze-directed word selection.pdf",
  doi       = "10.1145/3491102.3501838",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Nakagaki2022-pv,
  title     = "{(Dis)Appearables: A concept and method for actuated tangible UIs
               to appear and disappear based on stages}",
  author    = "Nakagaki, Ken and Tappa, Jordan L and Zheng, Yi and Forman, Jack
               and Leong, Joanne and Koenig, Sven and Ishii, Hiroshi",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "(Dis)Appearables is an approach for actuated Tangible User
               Interfaces (TUIs) to appear and disappear. This technique is
               supported by Stages: physical platforms inspired by theatrical
               stages. Self-propelled TUI's autonomously move between front and
               back stage allowing them to dynamically appear and disappear from
               users' attention. This platform opens up a novel interaction
               design space for expressive displays with dynamic physical
               affordances. We demonstrate and explore this approach based on a
               proof-of-concept implementation using two-wheeled robots, and
               multiple stage design examples. We have implemented a stage
               design pipeline which allows users to plan and design stages that
               are composed with front and back stages, and transition portals
               such as trap doors or lifts. The pipeline includes control of the
               robots, which guides them on and off stage. With this
               proof-of-concept prototype, we demonstrated a range of
               applications including interactive mobility simulation, self
               re-configuring desktops, remote hockey, and storytelling/gaming.
               Inspired by theatrical stage designs, this is a new take on
               `controlling the existence of matter' for user experience design.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3501906",
  file      = "All Papers/My Library/Nakagaki et al. 2022 - (Dis)Appearables - A concept and method for actuated tangible UIs to appear and disappear based on stages.pdf",
  doi       = "10.1145/3491102.3501906",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Guo2022-bu,
  title     = "{Remote co-teaching in rural classroom: Current practices,
               impacts, and challenges}",
  author    = "Guo, Siling and Sun, Tianchen and Gong, Jiangtao and Lu, Zhicong
               and Zhang, Liuxin and Wang, Qianying",
  booktitle = "{CHI Conference on Human Factors in Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3491102.3501924",
  file      = "All Papers/Other/Guo et al. 2022 - Remote co-teaching in rural classroom - Current practices, impacts, and challenges.pdf",
  doi       = "10.1145/3491102.3501924"
}

@INPROCEEDINGS{Yeh2022-pj,
  title     = "{How to Guide Task-oriented Chatbot Users, and When: A
               Mixed-methods Study of Combinations of Chatbot Guidance Types and
               Timings}",
  author    = "Yeh, Su-Fang and Wu, Meng-Hsin and Chen, Tze-Yu and Lin, Yen-Chun
               and Chang, Xijing and Chiang, You-Hsuan and Chang, Yung-Ju",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–16",
  abstract  = "The popularity of task-oriented chatbots is constantly growing,
               but smooth conversational progress with them remains profoundly
               challenging. In recent years, researchers have argued that
               chatbot systems should include guidance for users on how to
               converse with them. Nevertheless, empirical evidence about what
               to place in such guidance, and when to deliver it, has been
               lacking. Using a mixed-methods approach that integrates results
               from a between-subjects experiment and a reflection session, this
               paper compares the effectiveness of eight combinations of two
               guidance types (example-based and rule-based) at four guidance
               timings (service-onboarding, task-intro, after-failure, and
               upon-request), as measured by users’ task performance,
               improvement on subsequent tasks, and subjective experience. It
               establishes that each guidance type and timing has particular
               strengths and weaknesses, thus that each type/timing combination
               has a unique impact on performance metrics, learning outcomes,
               and user experience. On that basis, it presents guidance-design
               recommendations for future task-oriented chatbots.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3491102.3501941",
  file      = "All Papers/My Library/Yeh et al. 2022 - How to Guide Task-oriented Chatbot Users, and When - A Mixed-methods Study of Combinations of Chatbot Guidance Types and Timings.pdf",
  doi       = "10.1145/3491102.3501941",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Elsden2022-vo,
  title     = "{Zoom obscura: Counterfunctional design for video-conferencing}",
  author    = "Elsden, Chris and Chatting, David and Duggan, Michael and Dwyer,
               Andrew Carl and Thornton, Pip",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–17",
  abstract  = "This paper reports on Zoom Obscura – an artist-based design
               research project, responding to the ubiquity of
               video-conferencing as a technical and cultural phenomenon
               throughout the Covid-19 pandemic. As enterprise software, such as
               Zoom, rapidly came to mediate even the most personal and intimate
               interactions, we supported and collaborated with seven
               independent artists to explore technical and creative
               interventions in video-conferencing. Our call for participation
               sought critical interventions that would help users counter, and
               regain agency in regard to the various ways in which personal
               data is captured, transmitted and processed in video-conferencing
               tools. In this design study, we analyse post-hoc how each of the
               seven projects employed aspects of counterfunctional design to
               achieve these aims. Each project reveals different avenues and
               strategies for counterfunctionality in video-conferencing
               software, as well as opportunities to design critically towards
               interactions and experiences that challenge existing norms and
               expectations around these platforms.",
  series    = "CHI '22",
  month     =  "27~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3501973",
  file      = "All Papers/My Library/Elsden et al. 2022 - Zoom obscura - Counterfunctional design for video-conferencing.pdf",
  doi       = "10.1145/3491102.3501973",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Kim2022-vy,
  title     = "{Lattice menu: A low-error gaze-based marking menu utilizing
               target-assisted gaze gestures on a lattice of visual anchors}",
  author    = "Kim, Taejun and Ham, Auejin and Ahn, Sunggeun and Lee, Geehyuk",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "We present Lattice Menu, a gaze-based marking menu utilizing a
               lattice of visual anchors that helps perform accurate gaze
               pointing for menu item selection. Users who know the location of
               the desired item can leverage target-assisted gaze gestures for
               multilevel item selection by looking at visual anchors over the
               gaze trajectories. Our evaluation showed that Lattice Menu
               exhibits a considerably low error rate ( 1\%) and a quick menu
               selection time (1.3-1.6 s) for expert usage across various menu
               structures (4 × 4 × 4 and 6 × 6 × 6) and sizes (8, 10 and 12°).
               In comparison with a traditional gaze-based marking menu that
               does not utilize visual targets, Lattice Menu showed remarkably (
               5 times) fewer menu selection errors for expert usage. In a
               post-interview, all 12 subjects preferred Lattice Menu, and most
               subjects (8 out of 12) commented that the provisioning of visual
               targets facilitated more stable menu selections with reduced eye
               fatigue.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3501977",
  doi       = "10.1145/3491102.3501977",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Lustig2022-vp,
  title     = "{Designing for the bittersweet: Improving sensitive experiences
               with recommender systems}",
  author    = "Lustig, Caitlin and Konrad, Artie and Brubaker, Jed R",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–18",
  abstract  = "It is difficult to design systems that honor the complex and
               often contradictory emotions that can be surfaced by sensitive
               encounters with recommender systems. To explore the design and
               ethical considerations in this space, we interviewed 20 people
               who had recently seen sensitive content through Facebook's
               Memories feature. Interviewees typically described how (1)
               expectedness, (2) context of viewing, and (3) what we describe as
               “affective sense-making” were important factors for how they
               perceived “bittersweet” content, a sensitizing concept from our
               interviews that we expand upon. To address these user needs, we
               pose provocations to support critical work in this area and we
               suggest that researchers and designers: (1) draw inspiration from
               no/low-technology artifacts, (2) use empirical research to
               identify contextual features that have negative impacts on users,
               and (3) conduct user studies on affective sense-making. CAUTION:
               This paper discusses difficult subject matter related to death
               and relationships.",
  series    = "CHI '22",
  month     =  "27~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3502049",
  file      = "All Papers/My Library/Lustig et al. 2022 - Designing for the bittersweet - Improving sensitive experiences with recommender systems.pdf",
  doi       = "10.1145/3491102.3502049",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Khan2022-vw,
  title     = "{Integrating gaze and speech for enabling implicit interactions}",
  author    = "Khan, Anam Ahmad and Newn, Joshua and Bailey, James and Velloso,
               Eduardo",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–14",
  abstract  = "Gaze and speech are rich contextual sources of information that,
               when combined, can result in effective and rich multimodal
               interactions. This paper proposes a machine learning-based
               pipeline that leverages and combines users' natural gaze
               activity, the semantic knowledge from their vocal utterances and
               the synchronicity between gaze and speech data to facilitate
               users' interaction. We evaluated our proposed approach on an
               existing dataset, which involved 32 participants recording voice
               notes while reading an academic paper. Using a Logistic
               Regression classifier, we demonstrate that our proposed
               multimodal approach maps voice notes with accurate text passages
               with an average F1-Score of 0.90. Our proposed pipeline motivates
               the design of multimodal interfaces that combines natural gaze
               and speech patterns to enable robust interactions.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3502134",
  doi       = "10.1145/3491102.3502134",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Abdrabou2022-ls,
  title     = "{”your eyes tell you have used this password before”: Identifying
               password reuse from gaze and keystroke dynamics}",
  author    = "Abdrabou, Yasmeen and Schütte, Johannes and Shams, Ahmed and
               Pfeuffer, Ken and Buschek, Daniel and Khamis, Mohamed and Alt,
               Florian",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–16",
  abstract  = "A significant drawback of text passwords for end-user
               authentication is password reuse. We propose a novel approach to
               detect password reuse by leveraging gaze as well as typing
               behavior and study its accuracy. We collected gaze and typing
               behavior from 49 users while creating accounts for 1) a webmail
               client and 2) a news website. While most participants came up
               with a new password, 32\% reported having reused an old password
               when setting up their accounts. We then compared different ML
               models to detect password reuse from the collected data. Our
               models achieve an accuracy of up to 87.7\% in detecting password
               reuse from gaze, 75.8\% accuracy from typing, and 88.75\% when
               considering both types of behavior. We demonstrate that using
               gaze, password reuse can already be detected during the
               registration process, before users entered their password. Our
               work paves the road for developing novel interventions to prevent
               password reuse.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3517531",
  file      = "All Papers/My Library/Abdrabou et al. 2022 - ”your eyes tell you have used this password before” - Identifying password reuse from gaze and keystroke dynamics.pdf",
  doi       = "10.1145/3491102.3517531",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Arabi2022-ty,
  title     = "{Mobiot: Augmenting everyday objects into moving IoT devices
               using 3D printed attachments generated by demonstration}",
  author    = "Arabi, Abul Al and Li, Jiahao and Chen, Xiang 'anthony and Kim,
               Jeeeun",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–14",
  abstract  = "Recent advancements in personal fabrication have brought novices
               closer to a reality, where they can automate routine tasks with
               mobilized everyday objects. However, the overall process remains
               challenging- from capturing design requirements and motion
               planning to authoring them to creating 3D models of mechanical
               parts to programming electronics, as it demands expertise. We
               introduce Mobiot, an end-user toolkit to help non-experts capture
               the design and motion requirements of legacy objects by
               demonstration. It then automatically generates 3D printable
               attachments, programs to operate assembled modules, a list of
               off-the-shelf electronics, and assembly tutorials. The authoring
               feature further assists users to fine-tune as well as to reuse
               existing motion libraries and 3D printed mechanisms to adapt to
               other real-world objects with different motions. We validate
               Mobiot through application examples with 8 everyday objects with
               various motions applied, and through technical evaluation to
               measure the accuracy of motion reconstruction.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3517645",
  file      = "All Papers/My Library/Arabi et al. 2022 - Mobiot - Augmenting everyday objects into moving IoT devices using 3D printed attachments generated by demonstration.pdf",
  doi       = "10.1145/3491102.3517645",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Rueben2022-iy,
  title     = "{“I See You!”: A Design Framework for Interface Cues about Agent
               Visual Perception from a Thematic Analysis of Videogames}",
  author    = "Rueben, Matthew and Horrocks, Matthew R and Martinez, Jennifer
               Eleanor and Cormier, Michelle V and LaLone, Nicolas and Fraune,
               Marlena and Toups Dugas, Z",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–22",
  abstract  = "As artificial agents proliferate, there will be more and more
               situations in which they must communicate their capabilities to
               humans, including what they can “see.” Artificial agents have
               existed for decades in the form of computer-controlled agents in
               videogames. We analyze videogames in order to not only inspire
               the design of better agents, but to stop agent designers from
               replicating research that has already been theorized, designed,
               and tested in-depth. We present a qualitative thematic analysis
               of sight cues in videogames and develop a framework to support
               human-agent interaction design. The framework identifies the
               different locations and stimulus types – both visualizations and
               sonifications – available to designers and the types of
               information they can convey as sight cues. Insights from several
               other cue properties are also presented. We close with
               suggestions for implementing such cues with existing technologies
               to improve the safety, privacy, and efficiency of human-agent
               interactions.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3491102.3517699",
  doi       = "10.1145/3491102.3517699",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Yuan2022-iy,
  title     = "{Understanding multi-device usage patterns: Physical device
               configurations and fragmented workflows}",
  author    = "Yuan, Ye and Riche, Nathalie and Marquardt, Nicolai and Nicholas,
               Molly Jane and Seyed, Teddy and Romat, Hugo and Lee, Bongshin and
               Pahud, Michel and Goldstein, Jonathan and Vishkaie, Rojin and
               Holz, Christian and Hinckley, Ken",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–22",
  abstract  = "To better ground technical (systems) investigation and
               interaction design of cross-device experiences, we contribute an
               in-depth survey of existing multi-device practices, including
               fragmented workflows across devices and the way people physically
               organize and configure their workspaces to support such activity.
               Further, this survey documents a historically significant moment
               of transition to a new future of remote work, an existing trend
               dramatically accelerated by the abrupt switch to work-from-home
               (and having to contend with the demands of home-at-work) during
               the COVID-19 pandemic. We surveyed 97 participants, and collected
               photographs of home setups and open-ended answers to 50 questions
               categorized in 5 themes. We characterize the wide range of
               multi-device physical configurations and identify five usage
               patterns, including: partitioning tasks, integrating multi-device
               usage, cloning tasks to other devices, expanding tasks and inputs
               to multiple devices, and migrating between devices. Our analysis
               also sheds light on the benefits and challenges people face when
               their workflow is fragmented across multiple devices. These
               insights have implications for the design of multi-device
               experiences that support people's fragmented workflows.",
  series    = "CHI '22",
  month     =  "28~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3517702",
  file      = "All Papers/My Library/Yuan et al. 2022 - Understanding multi-device usage patterns - Physical device configurations and fragmented workflows.pdf",
  doi       = "10.1145/3491102.3517702",
  isbn      =  9781450391573
}

@ARTICLE{Suzuki2022-vg,
  title    = "{Augmented reality and robotics: A survey and taxonomy for
              AR-enhanced Human-Robot interaction and robotic interfaces}",
  author   = "Suzuki, Ryo and Karim, Adnan and Xia, Tian and Hedayati, Hooman
              and Marquardt, Nicolai",
  journal  = "arXiv [cs.RO]",
  abstract = "This paper contributes to a taxonomy of augmented reality and
              robotics based on a survey of 460 research papers. Augmented and
              mixed reality (AR/MR) have emerged as a new way to enhance
              human-robot interaction (HRI) and robotic interfaces (e.g.,
              actuated and shape-changing interfaces). Recently, an increasing
              number of studies in HCI, HRI, and robotics have demonstrated how
              AR enables better interactions between people and robots. However,
              often research remains focused on individual explorations and key
              design strategies, and research questions are rarely analyzed
              systematically. In this paper, we synthesize and categorize this
              research field in the following dimensions: 1) approaches to
              augmenting reality; 2) characteristics of robots; 3) purposes and
              benefits; 4) classification of presented information; 5) design
              components and strategies for visual augmentation; 6) interaction
              techniques and modalities; 7) application domains; and 8)
              evaluation strategies. We formulate key challenges and
              opportunities to guide and inform future research in AR and
              robotics.",
  month    =  mar,
  year     =  2022,
  url      = "http://dx.doi.org/10.1145/3491102.3517719",
  file     = "All Papers/My Library/Suzuki et al. 2022 - Augmented reality and robotics - A survey and taxonomy for AR-enhanced Human-Robot interaction and robotic interfaces.pdf",
  doi      = "10.1145/3491102.3517719"
}

@INPROCEEDINGS{Choi2022-nz,
  title     = "{Kuiper Belt: Utilizing the ``Out-of-natural Angle'' Region in
               the Eye-gaze Interaction for Virtual Reality}",
  author    = "Choi, Myungguen and Sakamoto, Daisuke and Ono, Tetsuo",
  booktitle = "{{CHI} Conference on Human Factors in Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--17",
  abstract  = "The maximum physical range of horizontal human eye movement is
               approximately 45°. However, in a natural gaze shift, the
               difference in the direction of the gaze relative to the frontal
               direction of the head rarely exceeds 25°. We name this region of
               25° − 45° the ``Kuiper Belt'' in the eye-gaze interaction. We try
               to utilize this region to solve the Midas touch problem to enable
               a search task while reducing false input in the Virtual Reality
               environment. In this work, we conduct two studies to figure out
               the design principle of how we place menu items in the Kuiper
               Belt as an ``out-of-natural angle'' region of the eye-gaze
               movement, and determine the effectiveness and workload of the
               Kuiper Belt-based method. The results indicate that the Kuiper
               Belt-based method facilitated the visual search task while
               reducing false input. Finally, we present example applications
               utilizing the findings of these studies.",
  series    = "CHI '22",
  month     =  apr,
  year      =  2022,
  url       = "http://dx.doi.org/10.1145/3491102.3517725",
  keywords  = "Menu Item Selection, Virtual Reality, Eye-gaze Interface, Eye
               Tracking",
  doi       = "10.1145/3491102.3517725",
  isbn      =  9781450391573
}

@INPROCEEDINGS{Choi2022-df,
  title     = "{Kuiper belt: Utilizing the “Out-of-natural angle” region in the
               eye-gaze interaction for virtual reality}",
  author    = "Choi, Myungguen and Sakamoto, Daisuke and Ono, Tetsuo",
  booktitle = "{Proceedings of the 2022 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–17",
  abstract  = "The maximum physical range of horizontal human eye movement is
               approximately 45°. However, in a natural gaze shift, the
               difference in the direction of the gaze relative to the frontal
               direction of the head rarely exceeds 25°. We name this region of
               25° − 45° the “Kuiper Belt” in the eye-gaze interaction. We try
               to utilize this region to solve the Midas touch problem to enable
               a search task while reducing false input in the Virtual Reality
               environment. In this work, we conduct two studies to figure out
               the design principle of how we place menu items in the Kuiper
               Belt as an “out-of-natural angle” region of the eye-gaze
               movement, and determine the effectiveness and workload of the
               Kuiper Belt-based method. The results indicate that the Kuiper
               Belt-based method facilitated the visual search task while
               reducing false input. Finally, we present example applications
               utilizing the findings of these studies.",
  series    = "CHI '22",
  month     =  "29~" # apr,
  year      =  2022,
  url       = "https://doi.org/10.1145/3491102.3517725",
  file      = "All Papers/My Library/Choi et al. 2022 - Kuiper belt - Utilizing the “Out-of-natural angle” region in the eye-gaze interaction for virtual reality.pdf",
  doi       = "10.1145/3491102.3517725",
  isbn      =  9781450391573
}

@ARTICLE{Plopski2022-rn,
  title    = "{The Eye in Extended Reality: A Survey on Gaze Interaction and Eye
              Tracking in Head-worn Extended Reality}",
  author   = "Plopski, Alexander and Hirzle, Teresa and Norouzi, Nahal and Qian,
              Long and Bruder, Gerd and Langlotz, Tobias",
  journal  = "ACM Computing Surveys",
  volume   =  55,
  number   =  3,
  pages    = "53:1–53:39",
  abstract = "With innovations in the field of gaze and eye tracking, a new
              concentration of research in the area of gaze-tracked systems and
              user interfaces has formed in the field of Extended Reality (XR).
              Eye trackers are being used to explore novel forms of spatial
              human–computer interaction, to understand human attention and
              behavior, and to test expectations and human responses. In this
              article, we review gaze interaction and eye tracking research
              related to XR that has been published since 1985, which includes a
              total of 215 publications. We outline efforts to apply eye gaze
              for direct interaction with virtual content and design of
              attentive interfaces that adapt the presented content based on eye
              gaze behavior and discuss how eye gaze has been utilized to
              improve collaboration in XR. We outline trends and novel
              directions and discuss representative high-impact papers in
              detail.",
  year     =  2022,
  url      = "https://dl.acm.org/doi/10.1145/3491207",
  file     = "All Papers/My Library/Plopski et al. 2022 - The Eye in Extended Reality - A Survey on Gaze Interaction and Eye Tracking in Head-worn Extended Reality.pdf",
  doi      = "10.1145/3491207",
  issn     = "0360-0300"
}

@ARTICLE{Miyashita2022-ym,
  title    = "{Display-Size Dependent Effects of 3D Viewing on Subjective
              Impressions}",
  author   = "Miyashita, Yamato and Sawahata, Yasuhito and Sakai, Akihiro and
              Harasawa, Masamitsu and Hara, Kazuhiro and Morita, Toshiya and
              Komine, Kazuteru",
  journal  = "ACM transactions on applied perception",
  volume   =  19,
  number   =  2,
  pages    = "5:1–5:15",
  abstract = "This paper describes how the screen size of 3D displays affect the
              subjective impressions of 3D-visualized content. The key
              requirement for 3D displays is the presentation of depth cues
              comprising binocular disparities and/or motion parallax; however,
              the development of displays and production of content that include
              these cues leads to an increase in costs. Given the variety of
              screen sizes, it is expected that 3D characteristics are
              experienced differently by viewers depending on the screen size.
              We asked 48 participants to evaluate the 3D experience when using
              three different-sized stereoscopic displays (11.5, 55, and 200
              inches) with head trackers. The participants were asked to score
              presented stimuli on 20 opposite-term pairs based on the semantic
              differential method after viewing each of six stimuli. Using
              factor analysis, we extracted three principal factors: power,
              related to strong three-dimensionality, real, etc.; visibility,
              related to stable, natural, etc.; and space, related to agile,
              open, etc., which had proportions of variances of 0.317, 0.277,
              and 0.251, respectively; their cumulation was 0.844. We confirmed
              that the three different-sized displays did not produce the same
              subjective impressions of the 3D characteristics. In particular,
              on the small-sized display, we found larger effects on power and
              space impressions from motion parallax (η2 = 0.133 and 0.161,
              respectively) than for the other two sizes. We found degradation
              of the visibility impressions from binocular disparities, which
              might be caused by artifacts from stereoscopy. The effects of 3D
              viewing on subjective impression depends on the display size, and
              small-sized displays offer the largest benefits by adding 3D
              characteristics to 2D visualization.",
  year     =  2022,
  url      = "https://dl.acm.org/doi/10.1145/3510461",
  file     = "All Papers/My Library/Miyashita et al. 2022 - Display-Size Dependent Effects of 3D Viewing on Subjective Impressions.pdf",
  doi      = "10.1145/3510461",
  issn     = "1544-3558"
}

@ARTICLE{Sakashita2022-xi,
  title    = "{RemoteCoDe: Robotic embodiment for enhancing peripheral awareness
              in remote collaboration tasks}",
  author   = "Sakashita, Mose and Ricci, E Andy and Arora, Jatin and
              Guimbretière, François",
  journal  = "Proc. ACM Hum.-Comput. Interact.",
  volume   =  6,
  number   = "CSCW1",
  pages    = "1–22",
  abstract = "Collaborative design activities are often centered around physical
              artifacts. Depending on the design activity, this can be the model
              of a building, paper crafts, carving artwork, or a new circuit to
              be debugged and evaluated. In a typical setting, collaborators are
              seated around a table and divide their attention between the
              design artifact under review, at least one laptop supporting
              measurements and information foraging, and of course their
              collaborators. Although these activities involve complex sets of
              tools and configurations, people can easily work together when
              they are present in the same space. This is because the physical
              presence of a partner affords peripheral awareness to inform where
              the partner's attention is and what they are doing. This
              peripheral awareness allows collaborators to coordinate actions
              and manage coupling to achieve a shared task. For example, it is
              quite easy to know when your partner switches their focus from a
              breadboard to you as a request to start a face to face discussion.",
  month    =  "4~" # jul,
  year     =  2022,
  url      = "https://doi.org/10.1145/3512910",
  file     = "All Papers/My Library/Sakashita et al. 2022 - RemoteCoDe - Robotic embodiment for enhancing peripheral awareness in remote collaboration tasks.pdf",
  doi      = "10.1145/3512910"
}

@INPROCEEDINGS{Hirzle2022-rd,
  title     = "{Attention of Many Observers Visualized by Eye Movements}",
  author    = "Hirzle, Teresa and Sauter, Marian and Wagner, Tobias and Hummel,
               Susanne and Rukzio, Enrico and Huckauf, Anke",
  booktitle = "{2022 Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Interacting with a group of people requires to direct the
               attention of the whole group, thus requires feedback about the
               crowd’s attention. In face-to-face interactions, head and eye
               movements serve as indicator for crowd attention. However, when
               interacting online, such indicators are not available. To
               substitute this information, gaze visualizations were adapted for
               a crowd scenario. We developed, implemented, and evaluated four
               types of visualizations of crowd attention in an online study
               with 72 participants using lecture videos enriched with
               audience’s gazes. All participants reported increased
               connectedness to the audience, especially for visualizations
               depicting the whole distribution of gaze including spatial
               information. Visualizations avoiding spatial overlay by depicting
               only the variability were regarded as less helpful, for real-time
               as well as for retrospective analyses of lectures. Improving our
               visualizations of crowd attention has the potential for a broad
               variety of applications, in all kinds of social interaction and
               communication in groups.",
  series    = "ETRA '22",
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3517031.3529235",
  file      = "All Papers/My Library/Hirzle et al. 2022 - Attention of Many Observers Visualized by Eye Movements.pdf",
  doi       = "10.1145/3517031.3529235",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Okano2022-yc,
  title     = "{Research on time series evaluation of cognitive load factors
               using features of eye movement}",
  author    = "Okano, Tomomi and Nakayama, Minoru",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–6",
  abstract  = "The relationship between ocular metrics and factor ratings for
               mental workloads are examined using a Bayesian statistical
               state-space modeling technique. During a visual search task
               experiment, microsaccade frequency and pupil size were observed
               as measures of mental workload. Individual mental workloads were
               measured using a 6 factor NASA-TLX rating scale. The models
               calculated generalized temporal changes of microsaccade frequency
               and pupil size during tasks. The contributions of factors of
               mental workload are examined using effect size. Also,
               chronological analysis was introduced to detect the reactions of
               the metrics during tasks. The results suggest that the response
               selectivity of microsaccades and pupil size can be used as
               factors of mental workload.",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3529236",
  doi       = "10.1145/3517031.3529236",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Smith2022-tm,
  title     = "{Advancing dignity for adaptive wheelchair users via a hybrid eye
               tracking and electromyography training game}",
  author    = "Smith, Peter and Dombrowski, Matt and McLinden, Shea and
               MacDonald, Calvin and Lynn, Devon and Sparkman, John and Courbin,
               Dominique and Manero, Albert",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Maintaining autonomous activities can be challenging for patients
               with neuromuscular disorders or quadriplegia, where control of
               joysticks for powered wheelchairs may not be feasible.
               Advancements in human machine interfaces have resulted in methods
               to capture the intentionality of the individual through
               non-traditional controls and communicating the users desires to a
               robotic interface. This research explores the design of a
               training game that teaches users to control a wheelchair through
               such a device that utilizes electromyography (EMG). The training
               game combines the use of EMG and eye tracking to enhance the
               impression of dignity while building self-efficacy and supporting
               autonomy for users. The system implements both eye tracking and
               surface electromyography, via the temporalis muscles, for
               gamified training and simulation of a novel wheelchair interface.",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3529612",
  doi       = "10.1145/3517031.3529612",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Meyer2022-tu,
  title     = "{A holographic single-pixel stereo camera sensor for
               calibration-free eye-tracking in retinal projection augmented
               reality glasses}",
  author    = "Meyer, Johannes and Wilm, Tobias and Fiess, Reinhold and
               Schlebusch, Thomas and Stork, Wilhelm and Kasneci, Enkelejda",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Eye-tracking is a key technology for future retinal projection
               based AR glasses as it enables techniques such as foveated
               rendering or gaze-driven exit pupil steering, which both
               increases the system's overall performance. However, two of the
               major challenges video oculography systems face are robust gaze
               estimation in the presence of glasses slippage, paired with the
               necessity of frequent sensor calibration. To overcome these
               challenges, we propose a novel, calibration-free eye-tracking
               sensor for AR glasses based on a highly transparent holographic
               optical element (HOE) and a laser scanner. We fabricate a
               segmented HOE generating two stereo images of the eye-region. A
               single-pixel detector in combination with our stereo
               reconstruction algorithm is used to precisely calculate the gaze
               position. In our laboratory setup we demonstrate a
               calibration-free accuracy of 1.35° achieved by our eye-tracking
               sensor; highlighting the sensor's suitability for consumer AR
               glasses.",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3529616",
  doi       = "10.1145/3517031.3529616",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Sauter2022-mu,
  title     = "{Distance between gaze and laser pointer predicts performance in
               video-based e-learning independent of the presence of an
               on-screen instructor}",
  author    = "Sauter, Marian and Wagner, Tobias and Huckauf, Anke",
  booktitle = "{2022 Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–10",
  abstract  = "In online lectures, showing an on-screen instructor gained
               popularity amidst the Covid-19 pandemic. However, evidence in
               favor of this is mixed: they draw attention and may distract from
               the content. In contrast, using signaling (e.g., with a digital
               pointer) provides known benefits for learners. But effects of
               signaling were only researched in absence of an on-screen
               instructor. In the present explorative study, we investigated
               effects of an on-screen instructor on the division of learners´
               attention; specifically, on following a digital pointer signal
               with their gaze. The presence of an instructor led to an
               increased number of fixations in the presenter area. This did
               neither affect learning outcomes nor gaze patterns following the
               pointer. The average distance between the learner's gaze and the
               pointer position predicts the student's quiz performance,
               independent of the presence of an on-screen instructor. This can
               also help in creating automated immediate-feedback systems for
               educational videos.",
  series    = "ETRA '22",
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3517031.3529620",
  file      = "All Papers/My Library/Sauter et al. 2022 - Distance between gaze and laser pointer predicts pe ... sed e-learning independent of the presence of an on-screen instructor.pdf",
  doi       = "10.1145/3517031.3529620",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Arefin2022-xi,
  title     = "{Estimating perceptual depth changes with eye vergence and
               interpupillary distance using an eye tracker in virtual reality}",
  author    = "Arefin, Mohammed Safayet and Swan, II, J Edward and Cohen
               Hoffing, Russell A and Thurman, Steven M",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "Virtual Reality (VR) technology has advanced to include
               eye-tracking, allowing novel research, such as investigating how
               our visual system coordinates eye movements with changes in
               perceptual depth. The purpose of this study was to examine
               whether eye tracking could track perceptual depth changes during
               a visual discrimination task. We derived two depth-dependent
               variables from eye tracker data: eye vergence angle (EVA) and
               interpupillary distance (IPD). As hypothesized, our results
               revealed that shifting gaze from near-to-far depth significantly
               decreased EVA and increased IPD, while the opposite pattern was
               observed while shifting from far-to-near. Importantly, the amount
               of change in these variables tracked closely with relative
               changes in perceptual depth, and supported the hypothesis that
               eye tracker data may be used to infer real-time changes in
               perceptual depth in VR. Our method could be used as a new tool to
               adaptively render information based on depth and improve the VR
               user experience.",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3529632",
  doi       = "10.1145/3517031.3529632",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Stojanov2022-rx,
  title     = "{The benefits of depth information for head-mounted gaze
               estimation}",
  author    = "Stojanov, Stefan and Talathi, Sachin S and Sharma, Abhishek",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "In this work, we investigate the hypothesis that adding 3D
               information of the periocular region to an end-to-end
               gaze-estimation network can improve gaze-estimation accuracy in
               the presence of slippage, which occurs quite commonly for
               head-mounted AR/VR devices. To this end, using UnityEyes we
               generate a simulated dataset with RGB and depth-maps of the eye
               with varying camera placement to simulate slippage artifacts. We
               generate different noise profiles for the depth-maps to simulate
               depth sensor noise artifacts. Using this data, we investigate the
               effects of different fusion techniques for combining image and
               depth information for gaze estimation. Our experiments show that
               under an attention-based fusion scheme, 3D information can
               significantly improve gaze-estimation and compensates well for
               slippage induced variability. Our finding supports augmenting 2D
               cameras with depth-sensors for the development of robust
               end-to-end appearance based gaze-estimation systems.",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3529638",
  file      = "All Papers/My Library/Stojanov et al. 2022 - The benefits of depth information for head-mounted gaze estimation.pdf",
  doi       = "10.1145/3517031.3529638",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Inoue2022-ad,
  title     = "{Gaze estimation with imperceptible marker displayed dynamically
               using polarization}",
  author    = "Inoue, Yutaro and Koshikawa, Koki and Takemura, Kentaro",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–5",
  abstract  = "Conventional eye-tracking methods require NIR-LEDs at the corners
               and edges of displays as references. However, extensive eyeball
               rotation results in the loss of reflections. Therefore, we
               propose imperceptible markers that can be dynamically displayed
               using liquid crystals. Using the characteristics of polarized
               light, the imperceptible markers are shown on a screen as
               references for eye-tracking. Additionally, the marker positions
               can be changed using the eyeball pose in the previous frame. The
               point-of-gaze was determined using the imperceptible markers
               based on model-based eye gaze estimation. The accuracy of the
               estimated PoG obtained using the imperceptible marker was
               approximately 1.69°, higher than that obtained using NIR-LEDs.
               Through experiments, we confirmed the feasibility and
               effectiveness of relocating imperceptible markers on the screen.",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3529640",
  doi       = "10.1145/3517031.3529640",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Isomoto2022-ev,
  title     = "{Interaction Design of Dwell Selection Toward Gaze-based AR/VR
               Interaction}",
  author    = "Isomoto, Toshiya and Yamanaka, Shota and Shizuki, Buntarou",
  booktitle = "{2022 Symposium on Eye Tracking Research and Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "In this paper, we first position the current dwell selection
               among gaze-based interactions and its advantages against
               head-gaze selection, which is the mainstream interface for HMDs.
               Next, we show how dwell selection and head-gaze selection are
               used in an actual interaction situation. By comparing these two
               selection methods, we describe the potential of dwell selection
               as an essential AR/VR interaction.",
  series    = "ETRA '22",
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3517031.3531628",
  file      = "All Papers/My Library/Isomoto et al. 2022 - Interaction Design of Dwell Selection Toward Gaze-based AR - VR Interaction.pdf",
  doi       = "10.1145/3517031.3531628",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Jayawardena2022-ul,
  title     = "{Introducing a real-time advanced eye movements analysis
               pipeline}",
  author    = "Jayawardena, Gavindya",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "Real-Time Advanced Eye Movements Analysis Pipeline (RAEMAP) is an
               advanced pipeline to analyze traditional positional gaze
               measurements as well as advanced eye gaze measurements. The
               proposed implementation of RAEMAP includes real-time analysis of
               fixations, saccades, gaze transition entropy, and low/high index
               of pupillary activity. RAEMAP will also provide visualizations of
               fixations, fixations on AOIs, heatmaps, and dynamic AOI
               generation in real-time. This paper outlines the proposed
               architecture of RAEMAP.",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3532196",
  file      = "All Papers/My Library/Jayawardena 2022 - Introducing a real-time advanced eye movements analysis pipeline.pdf",
  doi       = "10.1145/3517031.3532196",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Gunawardena2022-nn,
  title     = "{Mobile device eye tracking on dynamic visual contents using edge
               computing and deep learning}",
  author    = "Gunawardena, Nishan and Ginige, Anupama and Javadi, Bahman and
               Lui, Gough",
  booktitle = "{2022 symposium on eye tracking research and applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1–3",
  abstract  = "Eye-tracking has been used in various domains, including
               human-computer interaction, psychology, and many others. Compared
               to commercial eye trackers, eye tracking using off-the-shelf
               cameras has many advantages, such as lower cost, pervasiveness,
               and mobility. Quantifying human attention on the mobile device is
               invaluable in human-computer interaction. Like videos and mobile
               games, dynamic visual stimuli require higher attention than
               static visual stimuli such as web pages and images. This research
               aims to develop an accurate eye-tracking algorithm using the
               front-facing camera of mobile devices to identify human attention
               hotspots when viewing video type contents. The shortage of
               computational power in mobile devices becomes a challenge to
               obtain higher user satisfaction. Edge computing moves the
               processing power closer to the source of the data and reduces the
               latency introduced by the cloud computing. Therefore, the
               proposed algorithm will be extended with mobile edge computing to
               provide a real-time eye tracking experience for users",
  series    = "ETRA '22",
  month     =  "6~" # aug,
  year      =  2022,
  url       = "https://doi.org/10.1145/3517031.3532198",
  doi       = "10.1145/3517031.3532198",
  isbn      =  9781450392525
}

@INPROCEEDINGS{Dijkstra-Soudarissanane2022-sl,
  title     = "{Virtual visits: life-size immersive communication}",
  author    = "Dijkstra-Soudarissanane, Sylvie and Gunkel, Simon N B and
               Reinders, Véronne",
  booktitle = "{Proceedings of the 13th ACM Multimedia Systems Conference}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "310–314",
  abstract  = "Elderly people in care homes face a great lack of contact with
               their families and loved ones. Social isolation and loneliness
               are detrimental factors for older people in their health
               condition, cognitive impairment and quality of life. We
               previously presented a communication system that facilitated
               high-quality mediated social contact through an Augmented Reality
               tool with 3D user capture and rendering on an iPad. We have
               further enhanced this system and propose a new demonstrator that
               provides a working communication system between the elderly and
               family members on a life-size display. A complete end-to-end
               chain architecture is presented, where capture, transmission, and
               rendering are enhanced to provide a tool for natural interaction
               and high social presence. Our system is technically ready and
               well-adapted to meet the needs of elderly users.",
  series    = "MMSys '22",
  year      =  2022,
  url       = "https://doi.org/10.1145/3524273.3532903",
  doi       = "10.1145/3524273.3532903",
  isbn      =  9781450392839
}

@INPROCEEDINGS{Yoshida2022-um,
  title     = "{Flexel: A Modular Floor Interface for Room-Scale Tactile
               Sensing}",
  author    = "Yoshida, Takatoshi and Okazaki, Narin and Takaki, Ken and Hirose,
               Masaharu and Kitagawa, Shingo and Inami, Masahiko",
  booktitle = "{Proceedings of the 35th Annual ACM Symposium on User Interface
               Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–12",
  abstract  = "Human environments are physically supported by floors, which
               prevent people and furniture from gravitational pull. Since our
               body motions continuously generate vibrations and loads that
               propagate into the ground, measurement of these expressive
               signals leads to unobtrusive activity sensing. In this study, we
               present Flexel, a modular floor interface for room-scale tactile
               sensing. By paving a room with floor interfaces, our system can
               immediately begin to infer touch locations, track user locations,
               recognize foot gestures, and detect object locations. Through a
               series of exploratory studies, we determined the preferable
               hardware design that adheres to construction conventions, as well
               as the optimal sensor density that mediates the trade-off between
               cost and performance. We summarize our findings into design
               guidelines that are generalizable to other floor interfaces.
               Finally, we provide example applications for room-scale tactile
               sensing enabled by our Flexel system.",
  series    = "UIST '22",
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3526113.3545699",
  doi       = "10.1145/3526113.3545699",
  isbn      =  9781450393201
}

@INPROCEEDINGS{Cross2022-mv,
  title     = "{One-dimensional eye-gaze typing interface for people with
               locked-in syndrome}",
  author    = "Cross, Michael and Qiu, Leping and Zhong, Mingyuan and Wang,
               Yuntao and Shi, Yuanchun",
  booktitle = "{The adjunct publication of the 35th annual ACM symposium on user
               interface software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–3",
  abstract  = "People with Locked-in syndrome (LIS) suffer from complete loss of
               voluntary motor functions for speech or hand-writing. They are
               mentally intact, retaining only the control of vertical eye
               movements and blinking. In this work, we present a
               one-dimensional typing interface controlled exclusively by
               vertical eye movements and dwell-time for them to communicate at
               will. Hidden Markov Model and Bigram Models are used as
               auto-completion on both word and sentence level. We conducted two
               preliminary user studies on non-disabled users. The typing
               interface achieved 3.75 WPM without prediction and 11.36 WPM with
               prediction.",
  series    = "UIST '22 adjunct",
  month     =  "28~" # oct,
  year      =  2022,
  url       = "https://doi.org/10.1145/3526114.3558732",
  doi       = "10.1145/3526114.3558732",
  isbn      =  9781450393218
}

@ARTICLE{Lystbaek2022-ys,
  title    = "{Gaze-Hand Alignment: Combining Eye Gaze and Mid-Air Pointing for
              Interacting with Menus in Augmented Reality}",
  author   = "Lystbæk, Mathias N and Rosenberg, Peter and Pfeuffer, Ken and
              Grønbæk, Jens Emil and Gellersen, Hans",
  journal  = "Proceedings of the ACM on Human-Computer Interaction",
  volume   =  6,
  number   = "ETRA",
  pages    = "145:1–145:18",
  abstract = "Gaze and freehand gestures suit Augmented Reality as users can
              interact with objects at a distance without need for a separate
              input device. We propose Gaze-Hand Alignment as a novel multimodal
              selection principle, defined by concurrent use of both gaze and
              hand for pointing and alignment of their input on an object as
              selection trigger. Gaze naturally precedes manual action and is
              leveraged for pre-selection, and manual crossing of a pre-selected
              target completes the selection. We demonstrate the principle in
              two novel techniques, Gaze\&Finger for input by direct alignment
              of hand and finger raised into the line of sight, and Gaze\&Hand
              for input by indirect alignment of a cursor with relative hand
              movement. In a menu selection experiment, we evaluate the
              techniques in comparison with Gaze\&Pinch and a hands-only
              baseline. The study showed the gaze-assisted techniques to
              outperform hands-only input, and gives insight into trade-offs in
              combining gaze with direct or indirect, and spatial or semantic
              freehand gestures.",
  year     =  2022,
  url      = "https://dl.acm.org/doi/10.1145/3530886",
  file     = "All Papers/My Library/Lystbæk et al. 2022 - Gaze-Hand Alignment - Combining Eye Gaze and Mid-Air Pointing for Interacting with Menus in Augmented Reality.pdf",
  doi      = "10.1145/3530886"
}

@ARTICLE{Rong2022-af,
  title    = "{Where and What: Driver Attention-based Object Detection}",
  author   = "Rong, Yao and Kassautzki, Naemi-Rebecca and Fuhl, Wolfgang and
              Kasneci, Enkelejda",
  journal  = "Proceedings of the ACM on Human-Computer Interaction",
  volume   =  6,
  number   = "ETRA",
  pages    = "146:1–146:22",
  abstract = "Human drivers use their attentional mechanisms to focus on
              critical objects and make decisions while driving. As human
              attention can be revealed from gaze data, capturing and analyzing
              gaze information has emerged in recent years to benefit autonomous
              driving technology. Previous works in this context have primarily
              aimed at predicting ``where'' human drivers look at and lack
              knowledge of ``what'' objects drivers focus on. Our work bridges
              the gap between pixel-level and object-level attention prediction.
              Specifically, we propose to integrate an attention prediction
              module into a pretrained object detection framework and predict
              the attention in a grid-based style. Furthermore, critical objects
              are recognized based on predicted attended-to areas. We evaluate
              our proposed method on two driver attention datasets, BDD-A and
              DR(eye)VE. Our framework achieves competitive state-of-the-art
              performance in the attention prediction on both pixel-level and
              object-level but is far more efficient (75.3 GFLOPs less) in
              computation.",
  year     =  2022,
  url      = "https://dl.acm.org/doi/10.1145/3530887",
  file     = "All Papers/My Library/Rong et al. 2022 - Where and What - Driver Attention-based Object Detection.pdf",
  doi      = "10.1145/3530887"
}

@INPROCEEDINGS{Ye2022-tc,
  title     = "{Body-centric NFC: Body-centric interaction with NFC devices
               through near-field enabled clothing}",
  author    = "Ye, Huizhong and Lee, Chi-Jung and Wu, Te-Yen and Yang, Xing-Dong
               and Chen, Bing-Yu and Liang, Rong-Hao",
  booktitle = "{Designing interactive systems conference}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1626–1639",
  abstract  = "NFC (Near-Field Communication) has been widely applied for
               human-computer interaction (HCI). However, the short sensing
               distance of NFC requires the users to initiate the tasks with
               extra effort mostly using their hands, so it is inconvenient to
               use NFC in hands-busy scenarios. This paper presents an
               investigation of body-centric interactions between the NFC device
               users and their surroundings. The exploration is based on the
               recent development of near-field enabled clothing, which can
               passively extend an NFC-enabled device's reading distance to
               users' body landmarks. We present an accessible method for
               fabricating flexible, extensible, and scalable NFC extenders on
               clothing pieces, and an easy-to-use toolkit for facilitating
               designers to realize the interactive experiences. The method and
               toolkit were tested in technical experiments and in a co-creation
               workshop. The elicited design outcomes and the further
               exploratory makings generated knowledge for future research and
               embodied interaction design opportunities.",
  series    = "DIS '22",
  month     =  "13~" # jun,
  year      =  2022,
  url       = "https://doi.org/10.1145/3532106.3534569",
  file      = "All Papers/My Library/Ye et al. 2022 - Body-centric NFC - Body-centric interaction with NFC devices through near-field enabled clothing.pdf",
  doi       = "10.1145/3532106.3534569",
  isbn      =  9781450393584
}

@INPROCEEDINGS{Ward2000-ul,
  title     = "{Dasher---a data entry interface using continuous gestures and
               language models}",
  author    = "Ward, David J and Blackwell, Alan F and MacKay, David J C",
  booktitle = "{Proceedings of the 13th annual {ACM} symposium on User interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "129--137",
  abstract  = "Existing devices for communicating information to computers are
               bulky, slow to use, or unreliable. Dasher is a new interface
               incorporating language modelling and driven by continuous
               two-dimensional gestures, eg a mouse, touchscreen, or
               eye-tracker. Tests have shown that this device can be used to
               enter text at a rate of up to 34 words per minute, compared with
               typical ten-finger keyboard typing of 40--60 words per minute.",
  series    = "UIST '00",
  month     =  nov,
  year      =  2000,
  url       = "http://dx.doi.org/10.1145/354401.354427",
  file      = "All Papers/Other/Ward et al. 2000 - Dasher-a data entry interface using continuous gestures and language models.pdf",
  keywords  = "modelling, adaptive, text, language, entry;prj-gaze-shorthand",
  doi       = "10.1145/354401.354427",
  isbn      =  9781581132120
}

@INPROCEEDINGS{Ward2000-tg,
  title     = "{Dasher—a data entry interface using continuous gestures and
               language models}",
  author    = "Ward, David J and Blackwell, Alan F and MacKay, David J C",
  booktitle = "{Proceedings of the 13th annual ACM symposium on User interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "129–137",
  abstract  = "Existing devices for communicating information to computers are
               bulky, slow to use, or unreliable. Dasher is a new interface
               incorporating language modelling and driven by continuous
               two-dimensional gestures, eg a mouse, touchscreen, or
               eye-tracker. Tests have shown that this device can be used to
               enter text at a rate of up to 34 words per minute, compared with
               typical ten-finger keyboard typing of 40–60 words per minute.",
  series    = "UIST '00",
  month     =  "11~" # jan,
  year      =  2000,
  url       = "https://doi.org/10.1145/354401.354427",
  file      = "All Papers/My Library/Ward et al. 2000 - Dasher—a data entry interface using continuous gestures and language models.pdf",
  doi       = "10.1145/354401.354427",
  isbn      =  9781581132120
}

@INPROCEEDINGS{Sidenmark2023-qr,
  title     = "{Vergence Matching: Inferring Attention to Objects in 3D
               Environments for Gaze-Assisted Selection}",
  author    = "Sidenmark, Ludwig and Clarke, Christopher and Newn, Joshua and
               Lystbæk, Mathias N and Pfeuffer, Ken and Gellersen, Hans",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–15",
  abstract  = "Gaze pointing is the de facto standard to infer attention and
               interact in 3D environments but is limited by motor and sensor
               limitations. To circumvent these limitations, we propose a
               vergence-based motion correlation method to detect visual
               attention toward very small targets. Smooth depth movements
               relative to the user are induced on 3D objects, which cause slow
               vergence eye movements when looked upon. Using the principle of
               motion correlation, the depth movements of the object and
               vergence eye movements are matched to determine which object the
               user is focussing on. In two user studies, we demonstrate how the
               technique can reliably infer gaze attention on very small
               targets, systematically explore how different stimulus motions
               affect attention detection, and show how the technique can be
               extended to multi-target selection. Finally, we provide example
               applications using the concept and design guidelines for small
               target and accuracy-independent attention detection in 3D
               environments.",
  series    = "CHI '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3580685",
  file      = "All Papers/My Library/Sidenmark et al. 2023 - Vergence Matching - Inferring Attention to Objects in 3D Environments for Gaze-Assisted Selection.pdf",
  doi       = "10.1145/3544548.3580685",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Goncalves2023-js,
  title     = "{``My Zelda Cane'': Strategies Used by Blind Players to Play
               Visual-Centric Digital Games}",
  author    = "Gonçalves, David and Piçarra, Manuel and Pais, Pedro and
               Guerreiro, João and Rodrigues, André",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–15",
  abstract  = "Mainstream games are typically designed around the visual
               experience, with behaviors and interactions highly dependent on
               vision. Despite this, blind people are playing mainstream games
               while dealing with and overcoming inaccessible content, often
               together with friends and audiences. In this work, we analyze
               over 70 hours of YouTube videos, where blind content-creators
               play visual-centric games. We point out the various strategies
               employed by players to overcome barriers that permeate mainstream
               games. We reflect on ways to enable and improve blind players’
               experience with these games, shedding light on the positive and
               negative consequences of apparently benign design choices. Our
               observations underline how game elements are appropriated for
               accessibility, the incidental consequences of audio design, and
               the trade-offs between accessibility, agency, and engagement.",
  series    = "CHI '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3580702",
  file      = "All Papers/My Library/Gonçalves et al. 2023 - 'My Zelda Cane' - Strategies Used by Blind Players to Play Visual-Centric Digital Games.pdf",
  doi       = "10.1145/3544548.3580702",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Namnakani2023-xa,
  title     = "{Comparing dwell time, pursuits and gaze gestures for gaze
               interaction on handheld mobile devices}",
  author    = "Namnakani, Omar and Abdrabou, Yasmeen and Grizou, Jonathan and
               Esteves, Augusto and Khamis, Mohamed",
  booktitle = "{Proceedings of the 2023 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–17",
  abstract  = "Gaze is promising for hands-free interaction on mobile devices.
               However, it is not clear how gaze interaction methods compare to
               each other in mobile settings. This paper presents the first
               experiment in a mobile setting that compares three of the most
               commonly used gaze interaction methods: Dwell time, Pursuits, and
               Gaze gestures. In our study, 24 participants selected one of 2,
               4, 9, 12 and 32 targets via gaze while sitting and while walking.
               Results show that input using Pursuits is faster than Dwell time
               and Gaze gestures especially when there are many targets. Users
               prefer Pursuits when stationary, but prefer Dwell time when
               walking. While selection using Gaze gestures is more demanding
               and slower when there are many targets, it is suitable for
               contexts where accuracy is more important than speed. We conclude
               with guidelines for the design of gaze interaction on handheld
               mobile devices.",
  series    = "CHI '23",
  month     =  "19~" # apr,
  year      =  2023,
  url       = "https://doi.org/10.1145/3544548.3580871",
  file      = "All Papers/My Library/Namnakani et al. 2023 - Comparing dwell time, pursuits and gaze gestures for gaze interaction on handheld mobile devices.pdf",
  doi       = "10.1145/3544548.3580871",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Schroder2023-ym,
  title     = "{Collaborating Across Realities: Analytical Lenses for
               Understanding Dyadic Collaboration in Transitional Interfaces}",
  author    = "Schröder, Jan-Henrik and Schacht, Daniel and Peper, Niklas and
               Hamurculu, Anita Marie and Jetter, Hans-Christian",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–16",
  abstract  = "Transitional Interfaces are a yet underexplored, emerging class
               of cross-reality user interfaces that enable users to freely move
               along the reality-virtuality continuum during collaboration. To
               analyze and understand how such collaboration unfolds, we propose
               four analytical lenses derived from an exploratory study of
               transitional collaboration with 15 dyads. While solving a complex
               spatial optimization task, participants could freely switch
               between three contexts, each with different displays (desktop
               screens, tablet-based augmented reality, head-mounted virtual
               reality), input techniques (mouse, touch, handheld controllers),
               and visual representations (monoscopic and allocentric 2D/3D
               maps, stereoscopic egocentric views). Using the rich qualitative
               and quantitative data from our study, we evaluated participants’
               perceptions of transitional collaboration and identified
               commonalities and differences between dyads. We then derived four
               lenses including metrics and visualizations to analyze key
               aspects of transitional collaboration: (1) place and distance,
               (2) temporal patterns, (3) group use of contexts, (4) individual
               use of contexts.",
  series    = "CHI '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3580879",
  doi       = "10.1145/3544548.3580879",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Zenner2023-vv,
  title     = "{Induce a blink of the eye: Evaluating techniques for triggering
               eye blinks in virtual reality}",
  author    = "Zenner, André and Ullmann, Kristin and Ariza, Oscar and
               Steinicke, Frank and Krüger, Antonio",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "19~" # apr,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3580888",
  file      = "All Papers/Other/Zenner et al. 2023 - Induce a blink of the eye - Evaluating techniques for triggering eye blinks in virtual reality.pdf",
  doi       = "10.1145/3544548.3580888"
}

@INPROCEEDINGS{Wang2023-cu,
  title     = "{Understanding How Low Vision People Read Using Eye Tracking}",
  author    = "Wang, Ru and Zeng, Linxiu and Zhang, Xinyong and Mondal, Sanbrita
               and Zhao, Yuhang",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–17",
  abstract  = "While being able to read with screen magnifiers, low vision
               people have slow and unpleasant reading experiences. Eye tracking
               has the potential to improve their experience by recognizing
               fine-grained gaze behaviors and providing more targeted
               enhancements. To inspire gaze-based low vision technology, we
               investigate the suitable method to collect low vision users’ gaze
               data via commercial eye trackers and thoroughly explore their
               challenges in reading based on their gaze behaviors. With an
               improved calibration interface, we collected the gaze data of 20
               low vision participants and 20 sighted controls who performed
               reading tasks on a computer screen; low vision participants were
               also asked to read with different screen magnifiers. We found
               that, with an accessible calibration interface and data
               collection method, commercial eye trackers can collect gaze data
               of comparable quality from low vision and sighted people. Our
               study identified low vision people’s unique gaze patterns during
               reading, building upon which, we propose design implications for
               gaze-based low vision technology.",
  series    = "CHI '23",
  year      =  2023,
  url       = "https://doi.org/10.1145/3544548.3581213",
  file      = "All Papers/My Library/Wang et al. 2023 - Understanding How Low Vision People Read Using Eye Tracking.pdf",
  doi       = "10.1145/3544548.3581213",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Maddali2023-yf,
  title     = "{Understanding context to capture when reconstructing meaningful
               spaces for remote instruction and connecting in {XR}}",
  author    = "Maddali, Hanuma Teja and Lazar, Amanda",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "19~" # apr,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3581243",
  file      = "All Papers/Other/Maddali and Lazar 2023 - Understanding context to capture when reconstructing meaningful spaces for remote instruction and connecting in XR.pdf",
  doi       = "10.1145/3544548.3581243"
}

@INPROCEEDINGS{Xiao2023-pu,
  title     = "{Inform the Uninformed: Improving Online Informed Consent Reading
               with an AI-Powered Chatbot}",
  author    = "Xiao, Ziang and Li, Tiffany Wenting and Karahalios, Karrie and
               Sundaram, Hari",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–17",
  abstract  = "Informed consent is a core cornerstone of ethics in human subject
               research. Through the informed consent process, participants
               learn about the study procedure, benefits, risks, and more to
               make an informed decision. However, recent studies showed that
               current practices might lead to uninformed decisions and expose
               participants to unknown risks, especially in online studies.
               Without the researcher’s presence and guidance, online
               participants must read a lengthy form on their own with no
               answers to their questions. In this paper, we examined the role
               of an AI-powered chatbot in improving informed consent online. By
               comparing the chatbot with form-based interaction, we found the
               chatbot improved consent form reading, promoted participants’
               feelings of agency, and closed the power gap between the
               participant and the researcher. Our exploratory analysis further
               revealed the altered power dynamic might eventually benefit study
               response quality. We discussed design implications for creating
               AI-powered chatbots to offer effective informed consent in
               broader settings.",
  series    = "CHI '23",
  month     =  "19~" # apr,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3581252",
  file      = "All Papers/My Library/Xiao et al. 2023 - Inform the Uninformed - Improving Online Informed Consent Reading with an AI-Powered Chatbot.pdf",
  doi       = "10.1145/3544548.3581252",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Chen2023-nj,
  title     = "{iBall: Augmenting Basketball Videos with Gaze-moderated Embedded
               Visualizations}",
  author    = "Chen, Zhutian and Yang, Qisen and Shan, Jiarui and Lin, Tica and
               Beyer, Johanna and Xia, Haijun and Pfister, Hanspeter",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "Hamburg Germany",
  pages     = "1--18",
  month     =  "19~" # apr,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3581266",
  file      = "All Papers/My Library/Chen et al. 2023 - iBall - Augmenting Basketball Videos with Gaze-moderated Embedded Visualizations.pdf",
  doi       = "10.1145/3544548.3581266",
  isbn      =  9781450394215,
  language  = "en"
}

@INPROCEEDINGS{Cui2023-qx,
  title     = "{GlanceWriter: Writing Text by Glancing Over Letters with Gaze}",
  author    = "Cui, Wenzhe and Liu, Rui and Li, Zhi and Wang, Yifan and Wang,
               Andrew and Zhao, Xia and Rashidian, Sina and Baig, Furqan and
               Ramakrishnan, I V and Wang, Fusheng and Bi, Xiaojun",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–13",
  abstract  = "Writing text with eye gaze only is an appealing hands-free text
               entry method. However, existing gaze-based text entry methods
               introduce eye fatigue and are slow in typing speed because they
               often require users to dwell on letters of a word, or mark the
               starting and ending positions of a gaze path with extra
               operations for entering a word. In this paper, we propose
               GlanceWriter, a text entry method that allows users to enter text
               by glancing over keys one by one without any need to dwell on any
               keys or specify the starting and ending positions of a gaze path
               when typing a word. To achieve so, GlanceWriter probabilistically
               determines the letters to be typed based on the dynamics of gaze
               movements and gaze locations. Our user studies demonstrate that
               GlanceWriter significantly improves the text entry performance
               over EyeSwipe, a dwell-free input method using “reverse crossing”
               to identify the starting and ending keys. GlanceWriter also
               outperforms the dwell-free gaze input method of Tobii’s
               Communicator 5, a commercial eye gaze-based communication system.
               Overall, GlanceWriter achieves dwell-free and crossing-free text
               entry by probabilistically decoding gaze paths, offering a
               promising gaze-based text entry method.",
  series    = "CHI '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3581269",
  file      = "All Papers/My Library/Cui et al. 2023 - GlanceWriter - Writing Text by Glancing Over Letters with Gaze.pdf",
  doi       = "10.1145/3544548.3581269",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Faridan2023-bc,
  title     = "{ChameleonControl: Teleoperating real human surrogates through
               mixed reality gestural guidance for remote hands-on classrooms}",
  author    = "Faridan, Mehrad and Kumari, Bheesha and Suzuki, Ryo",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--13",
  month     =  "19~" # apr,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3581381",
  file      = "All Papers/Other/Faridan et al. 2023 - ChameleonControl - Teleoperating real human surrog ... hrough mixed reality gestural guidance for remote hands-on classrooms.pdf",
  doi       = "10.1145/3544548.3581381"
}

@INPROCEEDINGS{Wagner2023-ar,
  title     = "{A Fitts’ Law Study of Gaze-Hand Alignment for Selection in 3D
               User Interfaces}",
  author    = "Wagner, Uta and Lystbæk, Mathias N and Manakhov, Pavel and
               Grønbæk, Jens Emil Sloth and Pfeuffer, Ken and Gellersen, Hans",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–15",
  abstract  = "Gaze-Hand Alignment has recently been proposed for multimodal
               selection in 3D. The technique takes advantage of gaze for target
               pre-selection, as it naturally precedes manual input. Selection
               is then completed when manual input aligns with gaze on the
               target, without need for an additional click method. In this work
               we evaluate two alignment techniques, Gaze\&Finger and
               Gaze\&Handray, combining gaze with image plane pointing versus
               raycasting, in comparison with hands-only baselines and
               Gaze\&Pinch as established multimodal technique. We used Fitts’
               Law study design with targets presented at different depths in
               the visual scene, to assess effect of parallax on performance.
               The alignment techniques outperformed their respective hands-only
               baselines. Gaze\&Finger is efficient when targets are close to
               the image plane but less performant with increasing target depth
               due to parallax.",
  series    = "CHI '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3581423",
  file      = "All Papers/My Library/Wagner et al. 2023 - A Fitts’ Law Study of Gaze-Hand Alignment for Selection in 3D User Interfaces.pdf",
  doi       = "10.1145/3544548.3581423",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Su2023-hb,
  title     = "{LipLearner: Customizable Silent Speech Interactions on Mobile
               Devices}",
  author    = "Su, Zixiong and Fang, Shitao and Rekimoto, Jun",
  booktitle = "{Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–21",
  abstract  = "Silent speech interface is a promising technology that enables
               private communications in natural language. However, previous
               approaches only support a small and inflexible vocabulary, which
               leads to limited expressiveness. We leverage contrastive learning
               to learn efficient lipreading representations, enabling few-shot
               command customization with minimal user effort. Our model
               exhibits high robustness to different lighting, posture, and
               gesture conditions on an in-the-wild dataset. For 25-command
               classification, an F1-score of 0.8947 is achievable only using
               one shot, and its performance can be further boosted by
               adaptively learning from more data. This generalizability allowed
               us to develop a mobile silent speech interface empowered with
               on-device fine-tuning and visual keyword spotting. A user study
               demonstrated that with LipLearner, users could define their own
               commands with high reliability guaranteed by an online
               incremental learning scheme. Subjective feedback indicated that
               our system provides essential functionalities for customizable
               silent speech interactions with high usability and learnability.",
  series    = "CHI '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544548.3581465",
  file      = "All Papers/My Library/Su et al. 2023 - LipLearner - Customizable Silent Speech Interactions on Mobile Devices.pdf",
  doi       = "10.1145/3544548.3581465",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Shultz2023-iq,
  title     = "{Flat panel haptics: Embedded electroosmotic pumps for scalable
               shape displays}",
  author    = "Shultz, Craig and Harrison, Chris",
  booktitle = "{Proceedings of the 2023 CHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–16",
  abstract  = "Flat touch interfaces, with or without screens, pervade the
               modern world. However, their haptic feedback is minimal,
               prompting much research into haptic and shape-changing display
               technologies which are self-contained, fast acting, and offer
               millimeters of displacement while only being only millimeters
               thick. We present a new, miniaturizable type of shape-changing
               display using embedded electroosmotic pumps (EEOPs). Our pumps,
               controlled and powered directly by applied voltage, are 1.5mm in
               thickness, and allow complete stackups under 5mm. Nonetheless,
               they can move their entire volume's worth of fluid in 1 second,
               and generate pressures of +/-50kPa, enough to create dynamic,
               millimeter-scale tactile features on a surface that can withstand
               typical interaction forces (¡1N). These are the requisite
               technical ingredients to enable, for example, a pop-up keyboard
               on a flat smartphone. We experimentally quantify the mechanical
               and psychophysical performance of our displays and conclude with
               a set of example interfaces.",
  series    = "CHI '23",
  month     =  "19~" # apr,
  year      =  2023,
  url       = "https://doi.org/10.1145/3544548.3581547",
  file      = "All Papers/My Library/Shultz and Harrison 2023 - Flat panel haptics - Embedded electroosmotic pumps for scalable shape displays.pdf",
  doi       = "10.1145/3544548.3581547",
  isbn      =  9781450394215
}

@INPROCEEDINGS{Bacchin2023-lu,
  title     = "{Gaze-based Metrics of Cognitive Load in a Conjunctive Visual
               Memory Task}",
  author    = "Bacchin, Davide and Gehrer, Nina A and Krejtz, Krzysztof and
               Duchowski, Andrew T and Gamberini, Luciano",
  booktitle = "{Extended Abstracts of the 2023 CHI Conference on Human Factors
               in Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Measurement of Cognitive Load (CL) is of considerable importance
               to Human-Computer Interaction (HCI) as it relates to ease of
               learn- and usability. Numerous methods have been used for this
               purpose, both subjective and objective. The former relies on
               perception of expended effort subject to confounding factors.
               Among objective measures, eye tracking has demonstrated its
               effectiveness as a precise and non-invasive alternative. Since
               eye-tracking indices have mainly been tested in tasks involving
               single visual properties (e.g., shape), this work aims to compare
               their sensitivity to conjunctive features. Specifically, this
               study evaluates the Low/High Index of Pupillary Activity (LHIPA)
               and microsaccade magnitude in the Color Visual Short-Term Memory
               (CVSTM) task versus the previously validated n-back task. Results
               show LHIPA and microsaccade magnitude are as effective in
               discerning baseline and task in CVSTM as in n-back, replicating
               earlier results and extending their reliability for evaluation of
               CL in tasks involving multivalent conjunctive features.",
  series    = "CHI EA '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3544549.3585650",
  file      = "All Papers/My Library/Bacchin et al. 2023 - Gaze-based Metrics of Cognitive Load in a Conjunctive Visual Memory Task.pdf",
  doi       = "10.1145/3544549.3585650",
  isbn      =  9781450394222
}

@ARTICLE{Gunawardena2022-pp,
  title    = "{Eye-tracking technologies in mobile devices using edge computing:
              A systematic review}",
  author   = "Gunawardena, Nishan and Ginige, Jeewani Anupama and Javadi, Bahman",
  journal  = "ACM computing surveys",
  abstract = "Eye-tracking provides invaluable insight into the cognitive
              activities underlying a wide range of human behaviours.
              Identifying cognitive activities provide valuable perceptions of
              human learning patterns and signs of cognitive diseases like
              Alzheimer's, Parkinson's, autism. Also, mobile devices have
              changed the way that we experience daily life and become a
              pervasive part. This systematic review provides a detailed
              analysis of mobile device eye-tracking technology reported in 36
              studies published in high ranked scientific journals from 2010 to
              2020 (September), along with several reports from grey literature.
              The review provides in-depth analysis on algorithms, additional
              apparatus, calibration methods, computational systems, and metrics
              applied to measure the performance of the proposed solutions.
              Also, the review presents a comprehensive classification of mobile
              device eye-tracking applications used across various domains such
              as healthcare, education, road safety, news and human
              authentication. We have outlined the shortcomings identified in
              the literature and the limitations of the current mobile device
              eye-tracking technologies, such as using the front-facing mobile
              camera. Further, we have proposed an edge computing driven eye
              tracking solution to achieve the real-time eye tracking
              experience. Based on the findings, the paper outlines various
              research gaps and future opportunities that are expected to be of
              significant value for improving the work in the eye-tracking
              domain.",
  month    =  "7~" # jul,
  year     =  2022,
  url      = "https://dl.acm.org/doi/10.1145/3546938",
  doi      = "10.1145/3546938",
  issn     = "0360-0300,1557-7341",
  language = "en"
}

@INPROCEEDINGS{Hyrskykari2000-cc,
  title     = "{Design issues of iDICT: a gaze-assisted translation aid}",
  author    = "Hyrskykari, Aulikki and Majaranta, Päivi and Aaltonen, Antti and
               Räihä, Kari-Jouko",
  booktitle = "{Proceedings of the 2000 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "9–14",
  abstract  = "Eye-aware applications have existed for long, but mostly for very
               special and restricted target populations. We have designed and
               are currently implementing an eye-aware application, called
               iDict, which is a general-purpose translation aid aimed at mass
               markets. iDict monitors the user's gaze path while s/he is
               reading text written in a foreign language. When the reader
               encounters difficulties, iDict steps in and provides assistance
               with the translation. To accomplish this, the system makes use of
               information obtained from reading research, a language model, and
               the user profile. This paper describes the idea of the iDict
               application, the design problems and the key solutions for
               resolving these problems.",
  series    = "ETRA '00",
  month     =  "11~" # aug,
  year      =  2000,
  url       = "https://doi.org/10.1145/355017.355019",
  file      = "All Papers/My Library/Hyrskykari et al. 2000 - Design issues of iDICT - a gaze-assisted translation aid.pdf",
  doi       = "10.1145/355017.355019",
  isbn      =  9781581132809
}

@INPROCEEDINGS{Lankford2000-eg,
  title     = "{Effective eye-gaze input into Windows}",
  author    = "Lankford, Chris",
  booktitle = "{Proceedings of the 2000 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "23–27",
  abstract  = "The Eye-gaze Response Interface Computer Aid (ERICA) is a
               computer system developed at the University of Virginia that
               tracks eye movement. To allow true integration into the Windows
               environment, an effective methodology for performing the full
               range of mouse actions and for typing with the eye needed to be
               constructed. With the methods described in this paper,
               individuals can reliably perform all actions of the mouse and the
               keyboard with their eye.",
  series    = "ETRA '00",
  month     =  "11~" # aug,
  year      =  2000,
  url       = "https://doi.org/10.1145/355017.355021",
  file      = "All Papers/My Library/Lankford 2000 - Effective eye-gaze input into Windows.pdf",
  doi       = "10.1145/355017.355021",
  isbn      =  9781581132809
}

@INPROCEEDINGS{Salvucci2000-qy,
  title     = "{Identifying fixations and saccades in eye-tracking protocols}",
  author    = "Salvucci, Dario D and Goldberg, Joseph H",
  booktitle = "{Proceedings of the 2000 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "71--78",
  abstract  = "The process of fixation identification---separating and labeling
               fixations and saccades in eye-tracking protocols---is an
               essential part of eye-movement data analysis and can have a
               dramatic impact on higher-level analyses. However, algorithms for
               performing fixation identification are often described informally
               and rarely compared in a meaningful way. In this paper we propose
               a taxonomy of fixation identification algorithms that classifies
               algorithms in terms of how they utilize spatial and temporal
               information in eye-tracking protocols. Using this taxonomy, we
               describe five algorithms that are representative of different
               classes in the taxonomy and are based on commonly employed
               techniques. We then evaluate and compare these algorithms with
               respect to a number of qualitative characteristics. The results
               of these comparisons offer interesting implications for the use
               of the various algorithms in future work.",
  series    = "ETRA '00",
  month     =  nov,
  year      =  2000,
  url       = "http://dx.doi.org/10.1145/355017.355028",
  file      = "All Papers/Other/Salvucci and Goldberg 2000 - Identifying fixations and saccades in eye-tracking protocols.pdf",
  keywords  = "fixation identification, eye tracking, data analysis algorithms",
  doi       = "10.1145/355017.355028",
  isbn      =  9781581132809
}

@INPROCEEDINGS{Salvucci2000-xn,
  title     = "{Identifying fixations and saccades in eye-tracking protocols}",
  author    = "Salvucci, Dario D and Goldberg, Joseph H",
  booktitle = "{Proceedings of the 2000 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "71–78",
  abstract  = "The process of fixation identification—separating and labeling
               fixations and saccades in eye-tracking protocols—is an essential
               part of eye-movement data analysis and can have a dramatic impact
               on higher-level analyses. However, algorithms for performing
               fixation identification are often described informally and rarely
               compared in a meaningful way. In this paper we propose a taxonomy
               of fixation identification algorithms that classifies algorithms
               in terms of how they utilize spatial and temporal information in
               eye-tracking protocols. Using this taxonomy, we describe five
               algorithms that are representative of different classes in the
               taxonomy and are based on commonly employed techniques. We then
               evaluate and compare these algorithms with respect to a number of
               qualitative characteristics. The results of these comparisons
               offer interesting implications for the use of the various
               algorithms in future work.",
  series    = "ETRA '00",
  month     =  "11~" # aug,
  year      =  2000,
  url       = "https://doi.org/10.1145/355017.355028",
  file      = "All Papers/My Library/Salvucci and Goldberg 2000 - Identifying fixations and saccades in eye-tracking protocols.pdf",
  doi       = "10.1145/355017.355028",
  isbn      =  9781581132809
}

@ARTICLE{Isomoto2022-lj,
  title    = "{Dwell Selection with ML-based Intent Prediction Using Only Gaze
              Data}",
  author   = "Isomoto, Toshiya and Yamanaka, Shota and Shizuki, Buntarou",
  journal  = "Proceedings of the ACM on Interactive, Mobile, Wearable and
              Ubiquitous Technologies",
  volume   =  6,
  number   =  3,
  pages    = "1--21",
  abstract = "We developed a dwell selection system with ML-based prediction of
              a user's intent to select. Because a user perceives visual
              information through the eyes, precise prediction of a user's
              intent will be essential to the establishment of gaze-based
              interaction. Our system first detects a dwell to roughly screen
              the user's intent to select and then predicts the intent by using
              an ML-based prediction model. We created the intent prediction
              model from the results of an experiment with five different
              gaze-only tasks representing everyday situations. The intent
              prediction model resulted in an overall area under the curve (AUC)
              of the receiver operator characteristic curve of 0.903. Moreover,
              it could perform independently of the user (AUC=0.898) and the
              eye-tracker (AUC=0.880). In a performance evaluation experiment
              with real interactive situations, our dwell selection method had
              both higher qualitative and quantitative performance than
              previously proposed dwell selection methods.",
  month    =  "6~" # sep,
  year     =  2022,
  url      = "https://dl.acm.org/doi/10.1145/3550301",
  doi      = "10.1145/3550301",
  issn     = "2474-9567",
  language = "en"
}

@INPROCEEDINGS{Ozawa2022-ap,
  title     = "{Computational Alternative Photographic Process toward
               Sustainable Printing}",
  author    = "Ozawa, Chinatsu and Yamamoto, Kenta and Izumi, Kazuya and Ochiai,
               Yoichi",
  booktitle = "{SIGGRAPH Asia 2022 Technical Communications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–4",
  abstract  = "We propose a computational alternative photographic process that
               integrates computer processing with the conventional printing
               method, particularly cyanotype. Cyanotype is a non-silver-halide
               process that has a lower environmental impact and is known for
               its availability to produce tri-color prints; however, the
               tri-color process is complex and time-consuming. To simplify this
               heavy printing process, our framework provides a user interface
               for image editing and optimization based on color simulation
               within a printable color gamut. We demonstrate image editing and
               tri-color cyanotype printing using our framework, indicating that
               it can reduce the number of user trials and errors.",
  series    = "SA '22",
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3550340.3564219",
  doi       = "10.1145/3550340.3564219",
  isbn      =  9781450394659
}

@ARTICLE{Lamb2022-cn,
  title    = "{DeepJoin: Learning a joint occupancy, signed distance, and normal
              field function for shape repair}",
  author   = "Lamb, Nikolas and Banerjee, Sean and Banerjee, Natasha Kholgade",
  journal  = "ACM transactions on graphics",
  volume   =  41,
  number   =  6,
  pages    = "1–10",
  abstract = "We introduce DeepJoin, an automated approach to generate
              high-resolution repairs for fractured shapes using deep neural
              networks. Existing approaches to perform automated shape repair
              operate exclusively on symmetric objects, require a complete proxy
              shape, or predict restoration shapes using low-resolution voxels
              which are too coarse for physical repair. We generate a
              high-resolution restoration shape by inferring a corresponding
              complete shape and a break surface from an input fractured shape.
              We present a novel implicit shape representation for fractured
              shape repair that combines the occupancy function, signed distance
              function, and normal field. We demonstrate repairs using our
              approach for synthetically fractured objects from ShapeNet, 3D
              scans from the Google Scanned Objects dataset, objects in the
              style of ancient Greek pottery from the QP Cultural Heritage
              dataset, and real fractured objects. We outperform six baseline
              approaches in terms of chamfer distance and normal consistency.
              Unlike existing approaches and restorations generated using
              subtraction, DeepJoin restorations do not exhibit surface
              artifacts and join closely to the fractured region of the
              fractured shape. Our code is available at:
              https://github.com/Terascale-All-sensing-Research-Studio/DeepJoin.",
  month    =  "30~" # nov,
  year     =  2022,
  url      = "https://doi.org/10.1145/3550454.3555470",
  file     = "All Papers/My Library/Lamb et al. 2022 - DeepJoin - Learning a joint occupancy, signed distance, and normal field function for shape repair.pdf",
  doi      = "10.1145/3550454.3555470",
  issn     = "0730-0301"
}

@INPROCEEDINGS{Arabi2022-zw,
  title     = "{Augmenting everyday objects into personal robotic devices}",
  author    = "Arabi, Abul Al and Kim, Jeeeun",
  booktitle = "{SIGGRAPH asia 2022 emerging technologies}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "Augmenting familiar physical objects has presented great
               potential in upgrading their functions by automation, granting
               aesthetics, and even changing access. The recent celebration of
               success in personal fabrication has brought novices where they
               can augment everyday objects, from automating routine tasks with
               mobilized smart devices to devising self-sustaining smart objects
               by harvesting energy from involved daily interactions, for
               example. While the overall process involves a line of steps of
               capturing specifications, design mechanisms and fabricating the
               parts, it remains challenging for non-experts as it demands
               domain expertise in robotics, design, programming, and even
               mechanical engineering. We introduce a series of augmented
               robots, smart domestic devices that are augmented from everyday
               objects, leveraging personal fabrication to assist daily
               essential interactions. Through user-demonstration of desired
               motions, 3D printed attachment mechanisms are auto-generated to
               build personal robotic devices that automate routine tasks and
               harvest energy.",
  series    = "SA '22",
  month     =  "22~" # nov,
  year      =  2022,
  url       = "https://doi.org/10.1145/3550471.3564763",
  file      = "All Papers/My Library/Arabi and Kim 2022 - Augmenting everyday objects into personal robotic devices.pdf",
  doi       = "10.1145/3550471.3564763",
  isbn      =  9781450394727
}

@INPROCEEDINGS{Uchihashi2022-oj,
  title     = "{TeleStick: Video Recording and Playing System Optimized for
               Tactile Interaction using a Stick}",
  author    = "Uchihashi, Ryoto and Otsuka, Takumi and Murakami, Yuya and
               Yoshizawa, Ayaka and Kawashima, Takuya and Yamaguchi, Kaito and
               Ono, Genta and Matsuhashi, Tsukina and Yamada, Saki and Waguri,
               Manoka and Kamiyama, Youichi and Watanabe, Keita",
  booktitle = "{Proceedings of the 28th ACM Symposium on Virtual Reality
               Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "TeleStick is a system that records and replicates tactile
               experience and visual information using common types of camera
               and video monitor environment. TeleStick uses a stick-type device
               with a tactile microphone attached to a camera so that it is
               consistently visible in the video, and records video and with
               tactile and audio information using stereo 2ch. The users can
               feel as if they were in the video as they watch it while holding
               a stick-type device with a speaker and a vibrator.",
  series    = "VRST '22",
  year      =  2022,
  url       = "https://doi.org/10.1145/3562939.3565653",
  doi       = "10.1145/3562939.3565653",
  isbn      =  9781450398893
}

@ARTICLE{Nagai2022-ly,
  title    = "{HandyGaze: A gaze tracking technique for room-scale environments
              using a single smartphone}",
  author   = "Nagai, Takahiro and Fujita, Kazuyuki and Takashima, Kazuki and
              Kitamura, Yoshifumi",
  journal  = "Proceedings of the ACM on human-computer interaction",
  volume   =  6,
  number   = "ISS",
  pages    = "143–160",
  abstract = "We propose HandyGaze, a 6-DoF gaze tracking technique for
              room-scale environments that can be carried out by simply holding
              a smartphone naturally without installing any sensors or markers
              in the environment. Our technique simultaneously employs the
              smartphone's front and rear cameras: The front camera estimates
              the user's gaze vector relative to the smartphone, while the rear
              camera (and depth sensor, if available) performs self-localization
              by reconstructing a pre-obtained 3D map of the environment. To
              achieve this, we implemented a prototype that works on iOS
              smartphones by running an ARKit-based algorithm for estimating the
              user's 6-DoF head orientation. We additionally implemented a novel
              calibration method that offsets the user-specific deviation
              between the head and gaze orientations. We then conducted a user
              study (N=10) that measured our technique's positional accuracy to
              the gaze target under four conditions, based on combinations of
              use with and without a depth sensor and calibration. The results
              show that our calibration method was able to reduce the mean
              absolute error of the gaze point by 27\%, with an error of 0.53 m
              when using the depth sensor. We also report the target size
              required to avoid erroneous inputs. Finally, we suggest possible
              applications such as a gaze-based guidance application for
              museums.",
  month    =  "14~" # nov,
  year     =  2022,
  url      = "http://dx.doi.org/10.1145/3567715",
  file     = "All Papers/My Library/Nagai et al. 2022 - HandyGaze - A gaze tracking technique for room-scale environments using a single smartphone.pdf",
  doi      = "10.1145/3567715",
  issn     = "2573-0142",
  language = "en"
}

@INPROCEEDINGS{Beyeler2023-fd,
  title     = "{Cross-Device Shortcuts: Seamless Attention-guided Content
               Transfer via Opportunistic Deep Links between Apps and Devices}",
  author    = "Beyeler, Marilou and Cheng, Yi Fei and Holz, Christian",
  booktitle = "{Proceedings of the 25th International Conference on Multimodal
               Interaction}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "125–134",
  abstract  = "Although users increasingly spread their activities across
               multiple devices—even to accomplish a single task—information
               transfer between apps on separate devices still incurs
               non-negligible effort and time overhead. These interaction flows
               would considerably benefit from more seamless cross-device
               interaction that directly connects the information flow between
               the involved apps across devices. In this paper, we propose
               cross-device shortcuts, an interaction technique that enables
               direct and discoverable content exchange between apps on
               different devices. When users switch their attention between
               multiple engaged devices as part of a workflow, our system
               establishes a cross-device shortcut—a deep link between apps on
               separate devices that presents itself through feed-forward
               previews, inviting and facilitating quick content transfer. We
               explore the use of this technique in four scenarios spanning
               multiple devices and applications, and highlight the potential,
               limitations, and challenges of its design with a preliminary
               evaluation.",
  series    = "ICMI '23",
  year      =  2023,
  url       = "https://doi.org/10.1145/3577190.3614145",
  doi       = "10.1145/3577190.3614145"
}

@INPROCEEDINGS{Zhao2023-vm,
  title     = "{Gaze speedup: Eye gaze assisted gesture typing in virtual
               reality}",
  author    = "Zhao, Maozheng and Pierce, Alec M and Tan, Ran and Zhang, Ting
               and Wang, Tianyi and Jonker, Tanya R and Benko, Hrvoje and Gupta,
               Aakar",
  booktitle = "{Proceedings of the 28th International Conference on Intelligent
               User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "27~" # mar,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3581641.3584072",
  file      = "All Papers/Other/Zhao et al. 2023 - Gaze speedup - Eye gaze assisted gesture typing in virtual reality.pdf",
  doi       = "10.1145/3581641.3584072"
}

@INPROCEEDINGS{Kopacsi2023-ma,
  title     = "{IMETA: An interactive mobile eye tracking annotation method for
               semi-automatic fixation-to-AOI mapping}",
  author    = "Kopácsi, László and Barz, Michael and Bhatti, Omair Shahzad and
               Sonntag, Daniel",
  booktitle = "{28th International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "27~" # mar,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3581754.3584125",
  file      = "All Papers/Other/Kopácsi et al. 2023 - IMETA - An interactive mobile eye tracking annotation method for semi-automatic fixation-to-AOI mapping.pdf",
  doi       = "10.1145/3581754.3584125"
}

@INPROCEEDINGS{Xiao2023-ht,
  title     = "{Supporting qualitative analysis with large language models:
               Combining codebook with GPT-3 for deductive coding}",
  author    = "Xiao, Ziang and Yuan, Xingdi and Liao, Q Vera and Abdelghani,
               Rania and Oudeyer, Pierre-Yves",
  booktitle = "{28th International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "27~" # mar,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3581754.3584136",
  file      = "All Papers/Other/Xiao et al. 2023 - Supporting qualitative analysis with large language models - Combining codebook with GPT-3 for deductive coding.pdf",
  doi       = "10.1145/3581754.3584136"
}

@INPROCEEDINGS{Brinkley2023-ht,
  title     = "{Reproducibility as a Stepping Stone to intelligent assistants
               for data analysis: Recreating a study of physical activity,
               sleep, and work shift in nurses}",
  author    = "Brinkley, Detravious Jamari and Johnson, Emmanuel and Feng,
               Tiantian and Gil, Yolanda",
  booktitle = "{28th International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "27~" # mar,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3581754.3584173",
  doi       = "10.1145/3581754.3584173"
}

@INPROCEEDINGS{Barz2023-xe,
  title     = "{Interactive fixation-to-AOI mapping for mobile eye tracking data
               based on few-shot image classification}",
  author    = "Barz, Michael and Bhatti, Omair Shahzad and Alam, Hasan Md
               Tusfiqur and Nguyen, Duy Minh Ho and Sonntag, Daniel",
  booktitle = "{28th International Conference on Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "27~" # mar,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3581754.3584179",
  file      = "All Papers/Other/Barz et al. 2023 - Interactive fixation-to-AOI mapping for mobile eye tracking data based on few-shot image classification.pdf",
  doi       = "10.1145/3581754.3584179"
}

@INPROCEEDINGS{Poupyrev2023-lw,
  title     = "{The Ultimate Interface}",
  author    = "Poupyrev, Ivan",
  booktitle = "{Adjunct Proceedings of the 36th Annual ACM Symposium on User
               Interface Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "This talk will explore the possibilities of what emerging AI
               architectures could mean for interaction. My perspective and
               examples are based on personal opinions, informed by observations
               and experience gained from building advanced interactions over
               the last few decades. While I aim to provide insights,
               speculations, and suggest some essential conditions for creating
               such an ultimate interface, my insights may not be exhaustive. I
               invite open discussion and debate on this subject, recognizing
               that the conditions that I propose might be necessary but perhaps
               not sufficient in themselves.",
  series    = "UIST '23 Adjunct",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3586182.3624511",
  doi       = "10.1145/3586182.3624511"
}

@INPROCEEDINGS{Park2023-ln,
  title     = "{Generative Agents: Interactive Simulacra of Human Behavior}",
  author    = "Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and
               Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S",
  booktitle = "{Proceedings of the 36th Annual ACM Symposium on User Interface
               Software and Technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–22",
  abstract  = "Believable proxies of human behavior can empower interactive
               applications ranging from immersive environments to rehearsal
               spaces for interpersonal communication to prototyping tools. In
               this paper, we introduce generative agents: computational
               software agents that simulate believable human behavior.
               Generative agents wake up, cook breakfast, and head to work;
               artists paint, while authors write; they form opinions, notice
               each other, and initiate conversations; they remember and reflect
               on days past as they plan the next day. To enable generative
               agents, we describe an architecture that extends a large language
               model to store a complete record of the agent’s experiences using
               natural language, synthesize those memories over time into
               higher-level reflections, and retrieve them dynamically to plan
               behavior. We instantiate generative agents to populate an
               interactive sandbox environment inspired by The Sims, where end
               users can interact with a small town of twenty-five agents using
               natural language. In an evaluation, these generative agents
               produce believable individual and emergent social behaviors. For
               example, starting with only a single user-specified notion that
               one agent wants to throw a Valentine’s Day party, the agents
               autonomously spread invitations to the party over the next two
               days, make new acquaintances, ask each other out on dates to the
               party, and coordinate to show up for the party together at the
               right time. We demonstrate through ablation that the components
               of our agent architecture—observation, planning, and
               reflection—each contribute critically to the believability of
               agent behavior. By fusing large language models with
               computational interactive agents, this work introduces
               architectural and interaction patterns for enabling believable
               simulations of human behavior.",
  series    = "UIST '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3586183.3606763",
  file      = "All Papers/My Library/Park et al. 2023 - Generative Agents - Interactive Simulacra of Human Behavior.pdf",
  doi       = "10.1145/3586183.3606763"
}

@INPROCEEDINGS{Krzys2023-go,
  title     = "{Predicting the Allocation of Attention: Using contextual
               guidance of eye movements to examine the distribution of
               attention}",
  author    = "Krzys, Karolina J and Mistry, Mubeena and Yan, Tyler Q and
               Castelhano, Monica S",
  booktitle = "{Proceedings of the 2023 Symposium on Eye Tracking Research and
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–10",
  abstract  = "Eye movements are often taken as a marker of where attention is
               allocated, but it is possible that the attentional window can be
               either tightly or broadly focused around the fixation point.
               Using target objects whose location could either be strongly
               predicted by scene context (High Certainty) or not (Low
               Certainty), we examined how attention was initially distributed
               across a scene image during search. To do so, an unexpected
               distractor object suddenly appeared either in the relevant or
               irrelevant scene region for each target type. Distractors will be
               more disruptive where attention is allocated. We found that for
               High Certainty targets, the distractors were fixated
               significantly more often when they appeared in relevant than
               irrelevant regions, but there was no such difference for Low
               Certainty targets. This finding demonstrated differential
               patterns of attentional distribution around the fixation point
               based on the predicted location of target objects within a scene.",
  series    = "ETRA '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3588015.3588405",
  file      = "All Papers/My Library/Krzys et al. 2023 - Predicting the Allocation of Attention - Using contextual guidance of eye movements to examine the distribution of attention.pdf",
  doi       = "10.1145/3588015.3588405"
}

@INPROCEEDINGS{Krakowczyk2023-af,
  title     = "{Bridging the Gap: Gaze Events as Interpretable Concepts to
               Explain Deep Neural Sequence Models}",
  author    = "Krakowczyk, Daniel G and Prasse, Paul and Reich, David R and
               Lapuschkin, Sebastian and Scheffer, Tobias and Jäger, Lena A",
  booktitle = "{Proceedings of the 2023 Symposium on Eye Tracking Research and
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–8",
  abstract  = "Recent work in XAI for eye tracking data has evaluated the
               suitability of feature attribution methods to explain the output
               of deep neural sequence models for the task of oculomotric
               biometric identification. These methods provide saliency maps to
               highlight important input features of a specific eye gaze
               sequence. However, to date, its localization analysis has been
               lacking a quantitative approach across entire datasets. In this
               work, we employ established gaze event detection algorithms for
               fixations and saccades and quantitatively evaluate the impact of
               these events by determining their concept influence. Input
               features that belong to saccades are shown to be substantially
               more important than features that belong to fixations. By
               dissecting saccade events into sub-events, we are able to show
               that gaze samples that are close to the saccadic peak velocity
               are most influential. We further investigate the effect of event
               properties like saccadic amplitude or fixational dispersion on
               the resulting concept influence.",
  series    = "ETRA '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3588015.3588412",
  file      = "All Papers/My Library/Krakowczyk et al. 2023 - Bridging the Gap - Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models.pdf",
  doi       = "10.1145/3588015.3588412"
}

@INPROCEEDINGS{Chen2023-im,
  title     = "{Gazealytics: A Unified and Flexible Visual Toolkit for
               Exploratory and Comparative Gaze Analysis}",
  author    = "Chen, Kun-Ting and Prouzeau, Arnaud and Langmead, Joshua and
               Whitelock-Jones, Ryan T and Lawrence, Lee and Dwyer, Tim and
               Hurter, Christophe and Weiskopf, Daniel and Goodwin, Sarah",
  booktitle = "{Proceedings of the 2023 Symposium on Eye Tracking Research and
               Applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–7",
  abstract  = "We present a novel, web-based visual eye-tracking analytics tool
               called Gazealytics. Our open-source toolkit features a unified
               combination of gaze analytics features that support flexible
               exploratory analysis, along with annotation of areas of interest
               (AOI) and filter options based on multiple criteria to visually
               analyse eye tracking data across time and space. Gazealytics
               features coordinated views unifying spatiotemporal exploration of
               fixations and scanpaths for various analytical tasks. A novel
               matrix representation allows analysis of relationships between
               such spatial or temporal features. Data can be grouped across
               samples, user-defined AOIs or time windows of interest (TWIs) to
               support aggregate or filtered analysis of gaze activity. This
               approach exceeds the capabilities of existing systems by
               supporting flexible comparison between and within subjects,
               hypothesis generation, data analysis and communication of
               insights. We demonstrate in a walkthrough that Gazealytics
               supports multiple types of eye tracking datasets and analytical
               tasks.",
  series    = "ETRA '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3588015.3589844",
  file      = "All Papers/My Library/Chen et al. 2023 - Gazealytics - A Unified and Flexible Visual Toolkit for Exploratory and Comparative Gaze Analysis.pdf",
  doi       = "10.1145/3588015.3589844"
}

@INPROCEEDINGS{Suzuki2023-ln,
  title     = "{ExudedVestibule: Enhancing Mid-air Haptics through Galvanic
               Vestibular Stimulation}",
  author    = "Suzuki, Shieru and Aoyama, Kazuma and Kojima, Ryosei and Izumi,
               Kazuya and Fushimi, Tatsuki and Ochiai, Yoichi",
  booktitle = "{ACM SIGGRAPH 2023 Posters}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "This study presents a novel system that enhances air cannon
               tactile perception using synchronous galvanic vestibular
               stimulation (GVS). We conducted a user study with a
               within-subjects design to evaluate the enhancement effects of
               synchronous GVS on air cannon tactile sensations across multiple
               body locations. Results demonstrated significant improvements
               without affecting the magnitude of physical body sway, suggesting
               potential applications in virtual reality, particularly for
               augmenting existing air vortex ring haptics use cases.",
  series    = "SIGGRAPH '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3588028.3603648",
  doi       = "10.1145/3588028.3603648"
}

@INPROCEEDINGS{Ozawa2023-pc,
  title     = "{Give Life Back to Alternative Process: Exploring Handmade
               Photographic Printing Experiments towards Digital Nature
               Ecosystem}",
  author    = "Ozawa, Chinatsu and Yamamoto, Kenta and Izumi, Kazuya and Ochiai,
               Yoichi",
  booktitle = "{ACM SIGGRAPH 2023 Labs}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1–2",
  abstract  = "The proliferation of smartphones has made it easy for anyone to
               take digital photographs, and the recent popularization of
               text-to-image models has made it easy for anyone to create
               images. In this age, by combining digital technology with the
               tactile experience of handmade processes, we can rediscover the
               joy of creating with our own hands and the emotional connection
               that comes from physically interacting with our work. Previously,
               we proposed a new printing framework that integrated computer
               processing with full-color cyanotype printing. In this work, we
               demonstrate expanding the range of aesthetic expressions with
               computer processing for tone adjustment with several alternative
               processes such as salt print, platinum print, and cyanotype. In
               the installation, we present our printing framework with the user
               interface and exhibit works utilizing our proposed method. The
               use of new media developed after the digital age and the
               integration of computer processing in photo printing may be a way
               to create a new photographic life with the joy of materialising
               scenery.",
  series    = "SIGGRAPH '23",
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3588029.3599735",
  doi       = "10.1145/3588029.3599735"
}

@ARTICLE{Isomoto2023-gz,
  title    = "{Exploring Dwell-time from Human Cognitive Processes for Dwell
              Selection}",
  author   = "Isomoto, Toshiya and Yamanaka, Shota and Shizuki, Buntarou",
  journal  = "Proceedings of the ACM on Human-Computer Interaction",
  volume   =  7,
  number   = "ETRA",
  pages    = "159:1–159:15",
  abstract = "In order to develop future implicit interactions, it is important
              to understand the duration a user needs to recognize a visual
              object. By providing interactions that are triggered after a user
              recognizes an object, confusion resulting from the discrepancy
              between completing a cognitive process, which we define as the
              process from perceiving a visual stimulus to determining a
              selection, and triggering an interaction can be reduced. To
              understand this duration, we developed a model to derive
              dwell-times, allowing dwell selection to be performed after
              completing a cognitive process based on the Model Human Processor
              and the number of fixations. Our model revealed a minimum
              dwell-time of 144.2 ms for a colored target selection task. For an
              image selection task, the minimum dwell-time was 272.5 ms, which
              increased to 835.8 ms when a participant had not previously
              fixated on the object.",
  year     =  2023,
  url      = "https://doi.org/10.1145/3591128",
  doi      = "10.1145/3591128"
}

@ARTICLE{Aljehane2023-qo,
  title    = "{Studying Developer Eye Movements to Measure Cognitive Workload
              and Visual Effort for Expertise Assessment}",
  author   = "Aljehane, Salwa D and Sharif, Bonita and Maletic, Jonathan I",
  journal  = "Proceedings of the ACM on Human-Computer Interaction",
  volume   =  7,
  number   = "ETRA",
  pages    = "166:1–166:18",
  abstract = "Eye movement data provides valuable insights that help test
              hypotheses about a software developer's comprehension process. The
              pupillary response is successfully used to assess mental
              processing effort and attentional focus. Relatively little is
              known about the impact of expertise level in cognitive effort
              during programming tasks. This paper presents a quantitative
              analysis that compares the eye movements of 207 experts and
              novices collected while solving program comprehension tasks. The
              goal is to examine changes of developers' eye movement metrics in
              accordance with their expertise. The results indicate significant
              increase in pupil size with the novice group compared to the
              experts, explaining higher cognitive effort for novices. Novices
              also tend to have a significant number of fixations and higher
              gaze time compared to experts when they comprehend code. Moreover,
              a correlation study found that programming experience is still a
              powerful indicator when explaining expertise in this eye-tracking
              dataset among other expertise variables.",
  year     =  2023,
  url      = "https://dl.acm.org/doi/10.1145/3591135",
  file     = "All Papers/My Library/Aljehane et al. 2023 - Studying Developer Eye Movements to Measure Cognitive Workload and Visual Effort for Expertise Assessment.pdf",
  doi      = "10.1145/3591135"
}

@INPROCEEDINGS{Kim2024-jh,
  title     = "{Understanding Large-Language Model (LLM)-powered Human-Robot
               Interaction}",
  author    = "Kim, Callie Y and Lee, Christine P and Mutlu, Bilge",
  booktitle = "{Proceedings of the 2024 ACM/IEEE International Conference on
               Human-Robot Interaction}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "371--380",
  month     =  "11~" # mar,
  year      =  2024,
  url       = "https://dl.acm.org/doi/abs/10.1145/3610977.3634966",
  file      = "All Papers/Other/Kim et al. 2024 - Understanding Large-Language Model (LLM)-powered Human-Robot Interaction.pdf",
  doi       = "10.1145/3610977.3634966",
  isbn      =  9798400703225
}

@INPROCEEDINGS{Nith2024-sp,
  title     = "{SplitBody: Reducing mental workload while multitasking via
               muscle stimulation}",
  author    = "Nith, Romain and Ho, Yun and Lopes, Pedro",
  booktitle = "{Proceedings of the CHI Conference on Human Factors in Computing
               Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "11~" # may,
  year      =  2024,
  url       = "https://dl.acm.org/doi/10.1145/3613904.3642629",
  file      = "All Papers/Other/Nith et al. 2024 - SplitBody - Reducing mental workload while multitasking via muscle stimulation.pdf",
  doi       = "10.1145/3613904.3642629"
}

@INPROCEEDINGS{Sakashita2024-nr,
  title     = "{SharedNeRF: Leveraging Photorealistic and View-dependent
               Rendering for Real-time and Remote Collaboration}",
  author    = "Sakashita, Mose and Thoravi Kumaravel, Balasaravanan and
               Marquardt, Nicolai and Wilson, Andrew David",
  booktitle = "{Proceedings of the CHI Conference on Human Factors in Computing
               Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--14",
  month     =  "11~" # may,
  year      =  2024,
  url       = "https://dl.acm.org/doi/abs/10.1145/3613904.3642945",
  doi       = "10.1145/3613904.3642945",
  isbn      =  9798400703300
}

@INPROCEEDINGS{Overney2024-mm,
  title     = "{SenseMate: An accessible and beginner-friendly human-AI platform
               for qualitative data analysis}",
  author    = "Overney, Cassandra and Saldías, Belén and Dimitrakopoulou,
               Dimitra and Roy, Deb",
  booktitle = "{Proceedings of the 29th International Conference on Intelligent
               User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "18~" # mar,
  year      =  2024,
  url       = "https://dl.acm.org/doi/10.1145/3640543.3645194",
  file      = "All Papers/Other/Overney et al. 2024 - SenseMate - An accessible and beginner-friendly human-AI platform for qualitative data analysis.pdf",
  doi       = "10.1145/3640543.3645194"
}

@INPROCEEDINGS{Andrews2024-nz,
  title     = "{AiCommentator: A multimodal conversational agent for embedded
               visualization in football viewing}",
  author    = "Andrews, Peter and Nordberg, Oda Elise and Zubicueta Portales,
               Stephanie and Borch, Njål and Guribye, Frode and Fujita, Kazuyuki
               and Fjeld, Morten",
  booktitle = "{Proceedings of the 29th International Conference on Intelligent
               User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "18~" # mar,
  year      =  2024,
  url       = "https://dl.acm.org/doi/10.1145/3640543.3645197",
  file      = "All Papers/Other/Andrews et al. 2024 - AiCommentator - A multimodal conversational agent for embedded visualization in football viewing.pdf",
  doi       = "10.1145/3640543.3645197",
  language  = "en"
}

@INPROCEEDINGS{Muraleedharan2024-dx,
  title     = "{Eye-gaze-enabled assistive robotic stamp printing system for
               individuals with severe speech and motor impairment}",
  author    = "Muraleedharan, Anujith and {Anamika} and Vishwakarma, Himanshu
               and Kashyap, Kudrat and Biswas, Pradipta",
  booktitle = "{Companion Proceedings of the 29th International Conference on
               Intelligent User Interfaces}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "18~" # mar,
  year      =  2024,
  url       = "https://dl.acm.org/doi/10.1145/3640544.3645236",
  file      = "All Papers/Other/Muraleedharan et al. 2024 - Eye-gaze-enabled assistive robotic stamp printing system for individuals with severe speech and motor impairment.pdf",
  doi       = "10.1145/3640544.3645236"
}

@INPROCEEDINGS{Kwak2024-yk,
  title     = "{Saccade-Contingent Rendering}",
  author    = "Kwak, Yuna and Penner, Eric and Wang, Xuan and Saeedpour-Parizi,
               Mohammad R and Mercier, Olivier and Wu, Xiuyun and Murdison,
               Scott and Guan, Phillip",
  booktitle = "{Special Interest Group on Computer Graphics and Interactive
               Techniques Conference Conference Papers '24}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "13~" # jul,
  year      =  2024,
  url       = "https://dl.acm.org/doi/10.1145/3641519.3657420",
  file      = "All Papers/Other/Kwak et al. 2024 - Saccade-Contingent Rendering.pdf",
  doi       = "10.1145/3641519.3657420",
  language  = "en"
}

@INPROCEEDINGS{Tedla2024-nt,
  title     = "{LookToFocus: Image focus via eye tracking}",
  author    = "Tedla, Saikiran Kumar and MacKenzie, Scott and Brown, Michael",
  booktitle = "{Proceedings of the 2024 Symposium on Eye Tracking Research and
               Applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "4~" # jun,
  year      =  2024,
  url       = "https://dl.acm.org/doi/10.1145/3649902.3656358",
  doi       = "10.1145/3649902.3656358"
}

@ARTICLE{Park2024-vk,
  title    = "{A functional usability analysis of appearance-based gaze tracking
              for accessibility}",
  author   = "Park, Youn Soo and Manduchi, Roberto",
  journal  = "Proceedings. Eye Tracking Research \& Applications Symposium",
  volume   =  2024,
  abstract = "Appearance-based gaze tracking algorithms, which compute gaze
              direction from user face images, are an attractive alternative to
              infrared-based external devices. Their accuracy has greatly
              benefited by using powerful machine-learning techniques. The
              performance of appearance-based algorithms is normally evaluated
              on standard benchmarks typically involving users fixating at
              points on the screen. However, these metrics do not easily
              translate into functional usability characteristics. In this work,
              we evaluate a state-of-the-art algorithm, FAZE, in a number of
              tasks of interest to the human-computer interaction community.
              Specifically, we study how gaze measured by FAZE could be used for
              dwell-based selection and reading progression (line identification
              and progression along a line) - key functionalities for users
              facing motor and visual impairments. We compared the gaze data
              quality from 7 participants using FAZE against that from an
              infrared tracker (Tobii Pro Spark). Our analysis highlights the
              usability of appearance-based gaze tracking for such applications.",
  month    =  jun,
  year     =  2024,
  url      = "https://dl.acm.org/doi/10.1145/3649902.3656363",
  file     = "All Papers/Other/Park and Manduchi 2024 - A functional usability analysis of appearance-based gaze tracking for accessibility.pdf",
  keywords = "Dwelling; Gaze Estimation; Text Reading",
  doi      = "10.1145/3649902.3656363",
  pmc      = "PMC11323100",
  pmid     =  39145110,
  language = "en"
}

@INPROCEEDINGS{Estalagem2024-mb,
  title     = "{Between wearable and spatial computing: Exploring four
               interaction techniques at the intersection of smartwatches and
               head-mounted displays}",
  author    = "Estalagem, Nuno and Esteves, Augusto",
  booktitle = "{Proceedings of the 2024 Symposium on Eye Tracking Research and
               Applications}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  "4~" # jun,
  year      =  2024,
  url       = "https://dl.acm.org/doi/10.1145/3649902.3656365",
  file      = "All Papers/Other/Estalagem and Esteves 2024 - Between wearable and spatial computing - Ex ... hniques at the intersection of smartwatches and head-mounted displays.pdf",
  doi       = "10.1145/3649902.3656365"
}

@ARTICLE{Heo2024-yw,
  title    = "{Reading with screen magnification: Eye movement analysis using
              compensated gaze tracks}",
  author   = "Heo, Seongsil and Manduchi, Roberto and Chung, Susana",
  journal  = "Proceedings. Eye Tracking Research \& Applications Symposium",
  volume   =  2024,
  abstract = "Eye movements while reading with screen magnification (which
              requires manual scrolling to center the magnified portion of the
              screen within the viewport) pose interpretation challenges.
              Standard representations in terms of alternating fixations and
              saccades don't apply to this case. This is because, during
              scrolling, eyes often track a moving text element, generating a
              movement akin to smooth pursuit. We propose a new representation
              that uses information from the mouse (which the reader uses to
              move the center of magnification) to undo the effect of
              magnification and scrolling. After this ``compensation''
              operation, gaze tracks can again be described as alternating
              fixations and saccades. We present an analysis of gaze tracks
              obtained by applying this transformation on an existing dataset,
              recorded from low vision readers using two modalities of screen
              magnification. This analysis highlights similarities and
              differences in terms of dynamic properties of compensated gaze
              tracks vis-à-vis gaze during regular reading.",
  month    =  jun,
  year     =  2024,
  url      = "https://dl.acm.org/doi/10.1145/3649902.3656493",
  file     = "All Papers/Other/Heo et al. 2024 - Reading with screen magnification - Eye movement analysis using compensated gaze tracks.pdf",
  keywords = "eye movement; reading; screen magnification; visual impairment",
  doi      = "10.1145/3649902.3656493",
  pmc      = "PMC11257655",
  pmid     =  39026617,
  language = "en"
}

@INPROCEEDINGS{Vertegaal2001-nd,
  title     = "{Eye gaze patterns in conversations: there is more to
               conversational agents than meets the eyes}",
  author    = "Vertegaal, Roel and Slagter, Robert and van der Veer, Gerrit and
               Nijholt, Anton",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "301--308",
  abstract  = "In multi-agent, multi-user environments, users as well as agents
               should have a means of establishing who is talking to whom. In
               this paper, we present an experiment aimed at evaluating whether
               gaze directional cues of users could be used for this purpose.
               Using an eye tracker, we measured subject gaze at the faces of
               conversational partners during four-person conversations. Results
               indicate that when someone is listening or speaking to
               individuals, there is indeed a high probability that the person
               looked at is the person listened (p=88\%) or spoken to (p=77\%).
               We conclude that gaze is an excellent predictor of conversational
               attention in multiparty conversations. As such, it may form a
               reliable source of input for conversational systems that need to
               establish whom the user is speaking or listening to. We
               implemented our findings in FRED, a multi-agent conversational
               system that uses eye input to gauge which agent the user is
               listening or speaking to.",
  series    = "CHI '01",
  month     =  mar,
  year      =  2001,
  url       = "http://dx.doi.org/10.1145/365024.365119",
  keywords  = "tracking, multiparty communication, gaze, attention-based
               interfaces, conversational attention, attentive agents",
  doi       = "10.1145/365024.365119",
  isbn      =  9781581133271
}

@INPROCEEDINGS{Vertegaal2001-ny,
  title     = "{Eye gaze patterns in conversations: there is more to
               conversational agents than meets the eyes}",
  author    = "Vertegaal, Roel and Slagter, Robert and van der Veer, Gerrit and
               Nijholt, Anton",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "301–308",
  abstract  = "In multi-agent, multi-user environments, users as well as agents
               should have a means of establishing who is talking to whom. In
               this paper, we present an experiment aimed at evaluating whether
               gaze directional cues of users could be used for this purpose.
               Using an eye tracker, we measured subject gaze at the faces of
               conversational partners during four-person conversations. Results
               indicate that when someone is listening or speaking to
               individuals, there is indeed a high probability that the person
               looked at is the person listened (p=88\%) or spoken to (p=77\%).
               We conclude that gaze is an excellent predictor of conversational
               attention in multiparty conversations. As such, it may form a
               reliable source of input for conversational systems that need to
               establish whom the user is speaking or listening to. We
               implemented our findings in FRED, a multi-agent conversational
               system that uses eye input to gauge which agent the user is
               listening or speaking to.",
  series    = "CHI '01",
  month     =  mar,
  year      =  2001,
  url       = "http://dx.doi.org/10.1145/365024.365119",
  file      = "All Papers/My Library/Vertegaal et al. 2001 - Eye gaze patterns in conversations - there is more to conversational agents than meets the eyes.pdf",
  doi       = "10.1145/365024.365119",
  isbn      =  9781581133271
}

@INPROCEEDINGS{Garau2001-zn,
  title     = "{The impact of eye gaze on communication using humanoid avatars}",
  author    = "Garau, Maia and Slater, Mel and Bee, Simon and Sasse, Martina
               Angela",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "309--316",
  month     =  mar,
  year      =  2001,
  url       = "https://dl.acm.org/doi/abs/10.1145/365024.365121?casa_token=VVXmEcbgXSEAAAAA:BJHUFYXug1ifBU-teRb1Wo6Uqk1GxqwTTPjJMP0YOnRdYqfJUauqgif6Ndbcfpi0O4awAmXpDMIRMtU",
  doi       = "10.1145/365024.365121",
  isbn      =  9781581133271
}

@INPROCEEDINGS{Chen2002-xy,
  title     = "{Leveraging the asymmetric sensitivity of eye contact for
               videoconference}",
  author    = "Chen, Milton",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "49--56",
  abstract  = "Eye contact is a natural and often essential element in the
               language of visual communication. Unfortunately, perceiving eye
               contact is difficult in most video-conferencing systems and hence
               limits their effectiveness. We conducted experiments to determine
               how accurately people perceive eye contact. We discovered that
               the sensitivity to eye contact is asymmetric, in that we are an
               order of magnitude less sensitive to eye contact when people look
               below our eyes than when they look to the left, right, or above
               our eyes. Additional experiments support a theory that people are
               prone to perceive eye contact, that is, we will think that
               someone is making eye contact with us unless we are certain that
               the person is not looking into our eyes. These experimental
               results suggest parameters for the design of videoconferencing
               systems. As a demonstration, we were able to construct from
               commodity components a simple dyadic videoconferencing prototype
               that supports eye contact",
  series    = "CHI '02",
  month     =  apr,
  year      =  2002,
  url       = "http://dx.doi.org/10.1145/503376.503386",
  keywords  = "eye contact, gaze perception,
               videoconferencing;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/503376.503386",
  isbn      =  9781581134537
}

@INPROCEEDINGS{Chen2002-de,
  title     = "{Leveraging the asymmetric sensitivity of eye contact for
               videoconference}",
  author    = "Chen, Milton",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "49–56",
  abstract  = "Eye contact is a natural and often essential element in the
               language of visual communication. Unfortunately, perceiving eye
               contact is difficult in most video-conferencing systems and hence
               limits their effectiveness. We conducted experiments to determine
               how accurately people perceive eye contact. We discovered that
               the sensitivity to eye contact is asymmetric, in that we are an
               order of magnitude less sensitive to eye contact when people look
               below our eyes than when they look to the left, right, or above
               our eyes. Additional experiments support a theory that people are
               prone to perceive eye contact, that is, we will think that
               someone is making eye contact with us unless we are certain that
               the person is not looking into our eyes. These experimental
               results suggest parameters for the design of videoconferencing
               systems. As a demonstration, we were able to construct from
               commodity components a simple dyadic videoconferencing prototype
               that supports eye contact",
  series    = "CHI '02",
  month     =  apr,
  year      =  2002,
  url       = "http://dx.doi.org/10.1145/503376.503386",
  doi       = "10.1145/503376.503386",
  isbn      =  9781581134537
}

@INPROCEEDINGS{Rekimoto2002-et,
  title     = "{SmartSkin: an infrastructure for freehand manipulation on
               interactive surfaces}",
  author    = "Rekimoto, Jun",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "113–120",
  abstract  = "This paper introduces a new sensor architecture for making
               interactive surfaces that are sensitive to human hand and finger
               gestures. This sensor recognizes multiple hand positions and
               shapes and calculates the distance between the hand and the
               surface by using capacitive sensing and a mesh-shaped antenna. In
               contrast to camera-based gesture recognition systems, all sensing
               elements can be integrated within the surface, and this method
               does not suffer from lighting and occlusion problems. This paper
               describes the sensor architecture, as well as two working
               prototype systems: a table-size system and a tablet-size system.
               It also describes several interaction techniques that would be
               difficult to perform without using this architecture",
  series    = "CHI '02",
  year      =  2002,
  url       = "https://doi.org/10.1145/503376.503397",
  doi       = "10.1145/503376.503397",
  isbn      =  9781581134537
}

@INPROCEEDINGS{Morimoto2002-yn,
  title     = "{Free head motion eye gaze tracking without calibration}",
  author    = "Morimoto, Carlos H and Amir, Arnon and Flickner, Myron",
  booktitle = "{CHI '02 Extended Abstracts on Human Factors in Computing
               Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This paper introduces a novel technique for remote eye gaze
               tracking and detection of point of regard that is specially
               designed for wide use in HCI. It addresses and eliminates two of
               the major problems of commercial remote eye gaze tracking, namely
               the need for user calibration before each session and of accuracy
               degradation with head movement. The new technique uses a single
               calibrated camera, several light sources with known positions and
               a physical model of the eye to estimate the 3D position of the
               eye and its gaze direction. Simulation results using ray tracing
               are used to study the accuracy and robustness of the system, and
               demonstrate its operability.",
  month     =  "20~" # apr,
  year      =  2002,
  url       = "https://dl.acm.org/doi/10.1145/506443.506496",
  doi       = "10.1145/506443.506496",
  isbn      =  9781581134544
}

@INPROCEEDINGS{Majaranta2002-bl,
  title     = "{Twenty years of eye typing: systems and design issues}",
  author    = "Majaranta, Päivi and Räihä, Kari-Jouko",
  booktitle = "{Proceedings of the 2002 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "15--22",
  abstract  = "Eye typing provides a means of communication for severely
               handicapped people, even those who are only capable of moving
               their eyes. This paper considers the features, functionality and
               methods used in the eye typing systems developed in the last
               twenty years. Primary concerned with text production, the paper
               also addresses other communication related issues, among them
               customization and voice output.",
  series    = "ETRA '02",
  month     =  mar,
  year      =  2002,
  url       = "http://dx.doi.org/10.1145/507072.507076",
  keywords  = "Eye typing, alternative communication, eye
               tracking;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/507072.507076",
  isbn      =  9781581134674
}

@INPROCEEDINGS{Majaranta2002-hs,
  title     = "{Twenty years of eye typing: systems and design issues}",
  author    = "Majaranta, Päivi and Räihä, Kari-Jouko",
  booktitle = "{Proceedings of the 2002 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "15–22",
  abstract  = "Eye typing provides a means of communication for severely
               handicapped people, even those who are only capable of moving
               their eyes. This paper considers the features, functionality and
               methods used in the eye typing systems developed in the last
               twenty years. Primary concerned with text production, the paper
               also addresses other communication related issues, among them
               customization and voice output.",
  series    = "ETRA '02",
  month     =  "25~" # mar,
  year      =  2002,
  url       = "https://doi.org/10.1145/507072.507076",
  doi       = "10.1145/507072.507076",
  isbn      =  9781581134674
}

@INPROCEEDINGS{Vertegaal2002-lx,
  title     = "{Explaining effects of eye gaze on mediated group conversations:
               amount or synchronization?}",
  author    = "Vertegaal, Roel and Ding, Yaping",
  booktitle = "{Proceedings of the 2002 {ACM} conference on Computer supported
               cooperative work}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "41--48",
  abstract  = "We present an experiment examining effects of gaze on speech
               during three-person conversations. Understanding such effects is
               crucial for the design of teleconferencing systems and
               Collaborative Virtual Environments (CVEs). Previous findings
               suggest subjects take more turns when they experience more gaze.
               We evaluated whether this is because more gaze allowed them to
               better observe whether they were being addressed. We compared
               speaking behavior between two conditions: (1) in which subjects
               experienced gaze synchronized with conversational attention, and
               (2) in which subjects experienced random gaze. The amount of gaze
               experienced by subjects was a covariate. Results show subjects
               were 22\% more likely to speak when gaze behavior was
               synchronized with conversational attention. However, covariance
               analysis showed these results were due to differences in amount
               of gaze rather than synchronization of gaze, with correlations of
               .62 between amount of gaze and amount of subject speech. Task
               performance was 46\% higher when gaze was synchronized. We
               conclude it is commendable to use synchronized gaze models when
               designing CVEs, but depending on task situation, random models
               generating sufficient amounts of gaze may suffice.",
  series    = "CSCW '02",
  month     =  nov,
  year      =  2002,
  url       = "http://dx.doi.org/10.1145/587078.587085",
  keywords  = "agents, attentive interfaces, avatars, eye tracking, gaze,
               multiparty mediated communication",
  doi       = "10.1145/587078.587085",
  isbn      =  9781581135602
}

@INPROCEEDINGS{Vertegaal2002-ag,
  title     = "{Explaining effects of eye gaze on mediated group conversations:
               amount or synchronization?}",
  author    = "Vertegaal, Roel and Ding, Yaping",
  booktitle = "{Proceedings of the 2002 ACM conference on Computer supported
               cooperative work}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "41–48",
  abstract  = "We present an experiment examining effects of gaze on speech
               during three-person conversations. Understanding such effects is
               crucial for the design of teleconferencing systems and
               Collaborative Virtual Environments (CVEs). Previous findings
               suggest subjects take more turns when they experience more gaze.
               We evaluated whether this is because more gaze allowed them to
               better observe whether they were being addressed. We compared
               speaking behavior between two conditions: (1) in which subjects
               experienced gaze synchronized with conversational attention, and
               (2) in which subjects experienced random gaze. The amount of gaze
               experienced by subjects was a covariate. Results show subjects
               were 22\% more likely to speak when gaze behavior was
               synchronized with conversational attention. However, covariance
               analysis showed these results were due to differences in amount
               of gaze rather than synchronization of gaze, with correlations of
               .62 between amount of gaze and amount of subject speech. Task
               performance was 46\% higher when gaze was synchronized. We
               conclude it is commendable to use synchronized gaze models when
               designing CVEs, but depending on task situation, random models
               generating sufficient amounts of gaze may suffice.",
  series    = "CSCW '02",
  month     =  nov,
  year      =  2002,
  url       = "http://dx.doi.org/10.1145/587078.587085",
  doi       = "10.1145/587078.587085",
  isbn      =  9781581135602
}

@INPROCEEDINGS{Vertegaal2003-cb,
  title     = "{{GAZE-2}: conveying eye contact in group video conferencing
               using eye-controlled camera direction}",
  author    = "Vertegaal, Roel and Weevers, Ivo and Sohn, Changuk and Cheung,
               Chris",
  booktitle = "{Proceedings of the {SIGCHI} Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "521--528",
  abstract  = "GAZE-2 is a novel group video conferencing system that uses
               eye-controlled camera direction to ensure parallax-free
               transmission of eye contact. To convey eye contact, GAZE-2
               employs a video tunnel that allows placement of cameras behind
               participant images on the screen. To avoid parallax, GAZE-2
               automatically directs the cameras in this video tunnel using an
               eye tracker, selecting a single camera closest to where the user
               is looking for broadcast. Images of users are displayed in a
               virtual meeting room, and rotated towards the participant each
               user looks at. This way, eye contact can be conveyed to any
               number of users with only a single video stream per user. We
               empirically evaluated whether eye contact perception is affected
               by automated camera direction, which causes angular shifts in the
               transmitted images. Findings suggest camera shifts do not affect
               eye contact perception, and are not considered highly
               distractive.",
  series    = "CHI '03",
  month     =  apr,
  year      =  2003,
  url       = "http://dx.doi.org/10.1145/642611.642702",
  keywords  = "gaze, eye tracking, multiparty video conferencing, eye contact,
               attentive user interfaces;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1145/642611.642702",
  isbn      =  9781581136302
}

@INPROCEEDINGS{Vertegaal2003-wn,
  title     = "{GAZE-2: conveying eye contact in group video conferencing using
               eye-controlled camera direction}",
  author    = "Vertegaal, Roel and Weevers, Ivo and Sohn, Changuk and Cheung,
               Chris",
  booktitle = "{Proceedings of the SIGCHI conference on human factors in
               computing systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "521–528",
  abstract  = "GAZE-2 is a novel group video conferencing system that uses
               eye-controlled camera direction to ensure parallax-free
               transmission of eye contact. To convey eye contact, GAZE-2
               employs a video tunnel that allows placement of cameras behind
               participant images on the screen. To avoid parallax, GAZE-2
               automatically directs the cameras in this video tunnel using an
               eye tracker, selecting a single camera closest to where the user
               is looking for broadcast. Images of users are displayed in a
               virtual meeting room, and rotated towards the participant each
               user looks at. This way, eye contact can be conveyed to any
               number of users with only a single video stream per user. We
               empirically evaluated whether eye contact perception is affected
               by automated camera direction, which causes angular shifts in the
               transmitted images. Findings suggest camera shifts do not affect
               eye contact perception, and are not considered highly
               distractive.",
  series    = "CHI '03",
  month     =  apr,
  year      =  2003,
  url       = "http://dx.doi.org/10.1145/642611.642702",
  doi       = "10.1145/642611.642702",
  isbn      =  9781581136302
}

@INPROCEEDINGS{Garau2003-vh,
  title     = "{The impact of avatar realism and eye gaze control on perceived
               quality of communication in a shared immersive virtual
               environment}",
  author    = "Garau, Maia and Slater, Mel and Vinayagamoorthy, Vinoba and
               Brogni, Andrea and Steed, Anthony and Sasse, M Angela",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "529--536",
  month     =  "5~" # apr,
  year      =  2003,
  url       = "https://dl.acm.org/doi/abs/10.1145/642611.642703?casa_token=75y3DUsoGRwAAAAA:qR2V39fmWvohE2hP_zrL_F06mk28aTFZlN7YdBYRtWnk-bAVueRSz2WNCT98QtucXwUPobGo958-2tE",
  doi       = "10.1145/642611.642703",
  isbn      =  9781581136302
}

@ARTICLE{Grayson2003-vg,
  title    = "{Are you looking at me? Eye contact and desktop video
              conferencing}",
  author   = "Grayson, David M and Monk, Andrew F",
  journal  = "ACM transactions on computer-human interaction: a publication of
              the Association for Computing Machinery",
  volume   =  10,
  number   =  3,
  pages    = "221–243",
  abstract = "Mutual gaze is an important conversational resource, but is
              difficult to provide using conventional video conferencing
              equipment due to the disparity between the position of the camera
              and the position of the eyes on the screen. Various elaborate
              inventions have been proposed to get around this problem but none
              have found wide use. The alternative explored here is that these
              expensive alternatives may be unnecessary. Users of conventional
              desktop video equipment may, under the right conditions, be able
              to learn to interpret what is at first sight inappropriate
              apparent gaze direction as signalling that the other person is
              “looking at me.”Data are presented from two experiments where an
              estimator judges where a gazer is looking. The gazer may be
              looking either at the desktop video image of the estimator or some
              point to the side. Experiment 1 compared two image sizes and two
              camera positions. While the size of the image (352 × 288 pixels
              versus 176 × 144) had no significant effect on participants'
              ability to judge where the gazer was looking, horizontally
              offsetting the position of the camera inhibited performance.
              Experiment 2 examined the effect of reducing the image size
              further. The smallest image size (88 × 72 pixels) resulted in
              poorer performance than the intermediate (176 × 144). The results
              show that it is possible for users of low cost desktop video
              conferencing to learn to interpret gaze direction to a very high
              degree of accuracy if the equipment is configured optimally. The
              practical and theoretical implications of these results are
              discussed.",
  month    =  sep,
  year     =  2003,
  url      = "https://doi.org/10.1145%2F937549.937552",
  doi      = "10.1145/937549.937552",
  issn     = "1073-0516,1557-7325",
  language = "en"
}

@INPROCEEDINGS{Wobbrock2003-dn,
  title     = "{{EdgeWrite}: a stylus-based text entry method designed for high
               accuracy and stability of motion}",
  author    = "Wobbrock, Jacob O and Myers, Brad A and Kembel, John A",
  booktitle = "{Proceedings of the 16th annual {ACM} symposium on User interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "61--70",
  abstract  = "EdgeWrite is a new unistroke text entry method for handheld
               devices designed to provide high accuracy and stability of motion
               for people with motor impairments. It is also effective for
               able-bodied people. An EdgeWrite user enters text by traversing
               the edges and diagonals of a square hole imposed over the usual
               text input area. Gesture recognition is accomplished not through
               pattern recognition but through the sequence of corners that are
               hit. This means that the full stroke path is unimportant and
               recognition is highly deterministic, enabling better accuracy
               than other gestural alphabets such as Graffiti. A study of
               able-bodied users showed subjects with no prior experience were
               18\% more accurate during text entry with Edge Write than with
               Graffiti (p>.05), with no significant difference in speed. A
               study of 4 subjects with motor impairments revealed that some of
               them were unable to do Graffiti, but all of them could do Edge
               Write. Those who could do both methods had dramatically better
               accuracy with Edge Write.",
  series    = "UIST '03",
  month     =  nov,
  year      =  2003,
  url       = "http://dx.doi.org/10.1145/964696.964703",
  keywords  = "corners, graffiti, gesture recognition, palm, assistive
               technology, unistrokes, PDAs, computer access, text entry, motor
               impairments, edges, text input, pebbles, handhelds",
  doi       = "10.1145/964696.964703",
  isbn      =  9781581136364
}

@INPROCEEDINGS{Wobbrock2003-at,
  title     = "{EdgeWrite: a stylus-based text entry method designed for high
               accuracy and stability of motion}",
  author    = "Wobbrock, Jacob O and Myers, Brad A and Kembel, John A",
  booktitle = "{Proceedings of the 16th annual ACM symposium on User interface
               software and technology}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "61–70",
  abstract  = "EdgeWrite is a new unistroke text entry method for handheld
               devices designed to provide high accuracy and stability of motion
               for people with motor impairments. It is also effective for
               able-bodied people. An EdgeWrite user enters text by traversing
               the edges and diagonals of a square hole imposed over the usual
               text input area. Gesture recognition is accomplished not through
               pattern recognition but through the sequence of corners that are
               hit. This means that the full stroke path is unimportant and
               recognition is highly deterministic, enabling better accuracy
               than other gestural alphabets such as Graffiti. A study of
               able-bodied users showed subjects with no prior experience were
               18\% more accurate during text entry with Edge Write than with
               Graffiti (p¿.05), with no significant difference in speed. A
               study of 4 subjects with motor impairments revealed that some of
               them were unable to do Graffiti, but all of them could do Edge
               Write. Those who could do both methods had dramatically better
               accuracy with Edge Write.",
  series    = "UIST '03",
  month     =  nov,
  year      =  2003,
  url       = "http://dx.doi.org/10.1145/964696.964703",
  doi       = "10.1145/964696.964703",
  isbn      =  9781581136364
}

@INPROCEEDINGS{Hansen2004-zi,
  title     = "{Gaze typing compared with input by head and hand}",
  author    = "Hansen, John Paulin and Tørning, Kristian and Johansen, Anders
               Sewerin and Itoh, Kenji and Aoki, Hirotaka",
  booktitle = "{Proceedings of the 2004 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "131–138",
  abstract  = "This paper investigates the usability of gaze-typing systems for
               disabled people in a broad perspective that takes into account
               the usage scenarios and the particular users that these systems
               benefit. Design goals for a gaze-typing system are identified:
               productivity above 25 words per minute, robust tracking, high
               availability, and support of multimodal input. A detailed
               investigation of the efficiency and user satisfaction with a
               Danish and a Japanese gaze-typing system compares it to head- and
               mouse (hand) - typing. We found gaze typing to be more erroneous
               than the other two modalities. Gaze typing was just as fast as
               head typing, and both were slower than mouse (hand-) typing.
               Possibilities for design improvements are discussed.",
  series    = "ETRA '04",
  month     =  "22~" # mar,
  year      =  2004,
  url       = "https://doi.org/10.1145/968363.968389",
  doi       = "10.1145/968363.968389",
  isbn      =  9781581138252
}

@INPROCEEDINGS{Majaranta2004-yp,
  title     = "{Effects of feedback on eye typing with a short dwell time}",
  author    = "Majaranta, Päivi and Aula, Anne and Räihä, Kari-Jouko",
  booktitle = "{Proceedings of the 2004 symposium on Eye tracking research \&
               applications}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "139–146",
  abstract  = "Eye typing provides means of communication especially for people
               with severe disabilities. Recent research indicates that the type
               of feedback impacts typing speed, error rate, and the user's need
               to switch her gaze between the on-screen keyboard and the typed
               text field. The current study focuses on the issues of feedback
               when a short dwell time (450 ms vs. 900 ms in a previous study)
               is used. Results show that the findings obtained using longer
               dwell times only partly apply for shorter dwell times. For
               example, with a short dwell time, spoken feedback results in
               slower text entry speed and double entry errors. A short dwell
               time requires sharp and clear feedback that supports the typing
               rhythm.",
  series    = "ETRA '04",
  month     =  "22~" # mar,
  year      =  2004,
  url       = "https://doi.org/10.1145/968363.968390",
  doi       = "10.1145/968363.968390",
  isbn      =  9781581138252
}

@INPROCEEDINGS{Jacob1990-mi,
  title     = "{What you look at is what you get: eye movement-based interaction
               techniques}",
  author    = "Jacob, Robert J K",
  booktitle = "{Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "11–18",
  abstract  = "In seeking hitherto-unused methods by which users and computers
               can communicate, we investigate the usefulness of eye movements
               as a fast and convenient auxiliary user-to-computer communication
               mode. The barrier to exploiting this medium has not been
               eye-tracking technology but the study of interaction techniques
               that incorporate eye movements into the user-computer dialogue in
               a natural and unobtrusive way. This paper discusses some of the
               human factors and technical considerations that arise in trying
               to use eye movements as an input medium, describes our approach
               and the first eye movement-based interaction techniques that we
               have devised and implemented in our laboratory, and reports our
               experiences and observations on them.",
  series    = "CHI '90",
  year      =  1990,
  url       = "https://dl.acm.org/doi/10.1145/97243.97246",
  file      = "All Papers/My Library/Jacob 1990 - What you look at is what you get - eye movement-based interaction techniques.pdf",
  doi       = "10.1145/97243.97246",
  isbn      =  9780201509328
}

@INPROCEEDINGS{Miyauchi2004-ke,
  title     = "{Active eye contact for human-robot communication}",
  author    = "Miyauchi, Dai and Sakurai, Arihiro and Nakamura, Akio and Kuno,
               Yoshinori",
  booktitle = "{CHI '04 Extended Abstracts on Human Factors in Computing
               Systems}",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Eye contact is an effective means of controlling communication
               for humans, such as starting communication. It seems that we can
               make eye contact if we look at each other. However, this alone
               cannot complete eye contact. In addition, we need to be aware of
               being looked by each other. We propose a method of active eye
               contact for human-robot communication considering both
               conditions. The robot changes its facial expressions according to
               the observation results of the human to make eye contact. Then,
               we present a robot that can recognize hand gestures after making
               eye contact with the human to show the effectiveness of eye
               contact as a means of controlling communication.",
  month     =  "24~" # apr,
  year      =  2004,
  url       = "https://dl.acm.org/doi/10.1145/985921.985998",
  doi       = "10.1145/985921.985998",
  isbn      =  9781581137033
}

@ARTICLE{2015-nt,
  title    = "{遠隔瞳孔検出に基づく頭部移動を許容する視線一致型コミュニケーションシステムの提案}",
  author   = "祥之, 山本 and 清剛, 福元 and 嘉伸, 海老澤",
  journal  = "映像情報メディア学会年次大会講演予稿集",
  volume   =  2015,
  pages    = "13C–3",
  abstract = "… べる方法の組合せで，ユーザと相手ユーザの 視線 が 一致 する ように， さらに互いの顔を見やすくする方法を提案し，その
              有効性を示す．(1) 相手ユーザの頭部 が 移動 しても，相手ユー ザの両瞳孔間中点が，ユーザを写す カメラ 位置に相当するユ …",
  year     =  2015,
  url      = "http://dx.doi.org/10.11485/iteac.2015.0_13C-3",
  file     = "All Papers/My Library/祥之 et al. 2015 - 遠隔瞳孔検出に基づく頭部移動を許容する視線一致型コミュニケーションシステムの提案.pdf",
  doi      = "10.11485/iteac.2015.0\_13C-3"
}

@ARTICLE{2015-ft,
  title     = "{遠隔瞳孔検出に基づく頭部移動を許容する視線一致型コミュニケーションシステムの提案}",
  author    = "祥之, 山本 and 清剛, 福元 and 嘉伸, 海老澤",
  journal   = "映像情報メディア学会年次大会講演予稿集",
  publisher = "jstage.jst.go.jp",
  volume    =  2015,
  pages     = "13C--3",
  abstract  = "… べる方法の組合せで，ユーザと相手ユーザの 視線 が 一致 する ように， さらに互いの顔を見やすくする方法を提案し，その
               有効性を示す．(1) 相手ユーザの頭部 が 移動 しても，相手ユー ザの両瞳孔間中点が，ユーザを写す カメラ 位置に相当するユ …",
  year      =  2015,
  url       = "http://dx.doi.org/10.11485/iteac.2015.0_13C-3",
  file      = "All Papers/Other/祥之 et al. 2015 - 遠隔瞳孔検出に基づく頭部移動を許容する視線一致型コミュニケーションシステムの提案.pdf",
  keywords  = "eye contact;telepresence",
  doi       = "10.11485/iteac.2015.0\_13C-3"
}

@MISC{noauthor_undated-vt,
  title        = "{遠隔視覚対話における人間特性の分析とその応用 - 国立国会図書館デジタルコレクション}",
  howpublished = "\url{https://dl.ndl.go.jp/info:ndljp/pid/3188017}",
  note         = "Accessed: 2021-10-20",
  doi          = "10.11501/3188017"
}

@ARTICLE{2015-nn,
  title   = "{熟練者の視線に基づいたデッサン時の比例法学習支援システムの構築}",
  author  = "陽平, 寶井 and 紀文, 渡邊 and 千明, 久保村 and 弘之, 亀田",
  journal = "人工知能学会第二種研究会資料",
  volume  =  2015,
  number  = "KST-26",
  pages   =  05,
  year    =  2015,
  url     = "http://dx.doi.org/10.11517/jsaisigtwo.2015.KST-26_05",
  file    = "All Papers/My Library/陽平 et al. 2015 - 熟練者の視線に基づいたデッサン時の比例法学習支援システムの構築.pdf",
  doi     = "10.11517/jsaisigtwo.2015.KST-26\_05"
}

@ARTICLE{2009-oo,
  title   = "{可動式カメラを用いたメディアスペースの開発}",
  author  = "友樹, 村上 and 英之, 中西 and 慶, 加藤",
  journal = "人工知能学会全国大会論文集",
  volume  = "JSAI2009",
  pages   = "2A32–2A32",
  year    =  2009,
  url     = "http://dx.doi.org/10.11517/pjsai.JSAI2009.0_2A32",
  file    = "All Papers/My Library/友樹 et al. 2009 - 可動式カメラを用いたメディアスペースの開発.pdf",
  doi     = "10.11517/pjsai.JSAI2009.0\_2A32"
}

@ARTICLE{2009-io,
  title    = "{可動式カメラを用いたメディアスペースの開発}",
  author   = "友樹, 村上 and 英之, 中西 and 慶, 加藤",
  journal  = "人工知能学会全国大会論文集",
  volume   = "JSAI2009",
  pages    = "2A32--2A32",
  year     =  2009,
  url      = "http://dx.doi.org/10.11517/pjsai.JSAI2009.0_2A32",
  file     = "All Papers/Other/友樹 et al. 2009 - 可動式カメラを用いたメディアスペースの開発.pdf",
  keywords = "eye contact;telepresence",
  doi      = "10.11517/pjsai.JSAI2009.0\_2A32"
}

@ARTICLE{Beukelman2011-yc,
  title     = "{Communication Support for People with {ALS}}",
  author    = "Beukelman, David and Fager, Susan and Nordness, Amy",
  journal   = "Neurology research international",
  publisher = "hindawi.com",
  volume    =  2011,
  pages     =  714693,
  abstract  = "Almost all people with amyotrophic lateral sclerosis (ALS)
               experience a motor speech disorder, such as dysarthria, as the
               disease progresses. At some point, 80 to 95\% of people with ALS
               are unable to meet their daily communication needs using natural
               speech. Unfortunately, once intelligibility begins to decrease,
               speech performance often deteriorates so rapidly that there is
               little time to implement an appropriate augmentative and
               alternative communication (AAC) intervention; therefore,
               appropriate timing of referral for AAC assessment and
               intervention continues to be a most important clinical
               decision-making issue. AAC acceptance and use have increased
               considerably during the past decade. Many people use AAC until
               within a few weeks of their deaths.",
  month     =  apr,
  year      =  2011,
  url       = "http://dx.doi.org/10.1155/2011/714693",
  file      = "All Papers/Other/Beukelman et al. 2011 - Communication Support for People with ALS.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1155/2011/714693",
  pmc       = "PMC3096454",
  pmid      =  21603029,
  issn      = "2090-1852,2090-1860",
  language  = "en"
}

@ARTICLE{Beukelman2011-og,
  title    = "{Communication support for people with {ALS}}",
  author   = "Beukelman, David and Fager, Susan and Nordness, Amy",
  journal  = "Neurology research international",
  volume   =  2011,
  pages    =  714693,
  abstract = "Almost all people with amyotrophic lateral sclerosis (ALS)
              experience a motor speech disorder, such as dysarthria, as the
              disease progresses. At some point, 80 to 95\% of people with ALS
              are unable to meet their daily communication needs using natural
              speech. Unfortunately, once intelligibility begins to decrease,
              speech performance often deteriorates so rapidly that there is
              little time to implement an appropriate augmentative and
              alternative communication (AAC) intervention; therefore,
              appropriate timing of referral for AAC assessment and intervention
              continues to be a most important clinical decision-making issue.
              AAC acceptance and use have increased considerably during the past
              decade. Many people use AAC until within a few weeks of their
              deaths.",
  month    =  "14~" # apr,
  year     =  2011,
  url      = "http://dx.doi.org/10.1155/2011/714693",
  file     = "All Papers/My Library/Beukelman et al. 2011 - Communication support for people with ALS.pdf",
  doi      = "10.1155/2011/714693",
  issn     = "2090-1852",
  language = "en"
}

@ARTICLE{Huang2021-ug,
  title    = "{Real-Time Precise Human-Computer Interaction System Based on Gaze
              Estimation and Tracking}",
  author   = "Huang, Junhao and Zhang, Zhicheng and Xie, Guoping and He, Hui",
  editor   = "Chen, Chi-Hua",
  journal  = "Proceedings of the ... International Wireless Communications \&
              Mobile Computing Conference / Association for Computing Machinery.
              International Wireless Communications \& Mobile Computing
              Conference",
  volume   =  2021,
  pages    = "1--10",
  abstract = "Noncontact human-computer interaction has an important value in
              wireless sensor networks. This work is aimed at achieving accurate
              interaction on a computer based on auto eye control, using a cheap
              webcam as the video source. A real-time accurate human-computer
              interaction system based on eye state recognition, rough gaze
              estimation, and tracking is proposed. Firstly, binary
              classification of the eye states (opening or closed) is carried on
              using the SVM classification algorithm with HOG features of the
              input eye image. Second, rough appearance-based gaze estimation is
              implemented based on a simple CNN model. And the head pose is
              estimated to judge whether the user is facing the screen or not.
              Based on these recognition results, noncontact mouse control and
              character input methods are designed and developed to replace the
              standard mouse and keyboard hardware. Accuracy and speed of the
              proposed interaction system are evaluated by four subjects. The
              experimental results show that users can use only a common
              monocular camera to achieve gaze estimation and tracking and to
              achieve most functions of real-time precise human-computer
              interaction on the basis of auto eye control.",
  month    =  "8~" # nov,
  year     =  2021,
  url      = "https://www.hindawi.com/journals/wcmc/2021/8213946/",
  file     = "All Papers/My Library/Huang et al. 2021 - Real-Time Precise Human-Computer Interaction System Based on Gaze Estimation and Tracking.pdf",
  doi      = "10.1155/2021/8213946",
  issn     = "1530-8677,1530-8669",
  language = "en"
}

@ARTICLE{Pan2015-ej,
  title    = "{A surround video capture and presentation system for preservation
              of eye-gaze in teleconferencing applications}",
  author   = "Pan, Ye and Oyekoya, Oyewole and Steed, Anthony",
  journal  = "Presence",
  volume   =  24,
  number   =  1,
  pages    = "24–43",
  abstract = "We propose a new video conferencing system that uses an array of
              cameras to capture a remote user and then show the video of that
              person on a spherical display. This telepresence system has two
              key advantages: (i) it can capture a near-correct image for any
              potential observer viewing direction because the cameras surround
              the user horizontally; and (ii) with view-dependent graphical
              representation on the spherical display, it is possible to tell
              where the remote user is looking from any viewpoint, whereas flat
              displays are visible only from the front. As a result, the display
              can more faithfully represent the gaze of the remote user. We
              evaluate this system by measuring the ability of observers to
              accurately judge which targets the actor is gazing at in two
              experiments. Results from the first experiment demonstrate the
              effectiveness of the camera array and spherical display system, in
              that it allows observers at multiple observing positions to
              accurately tell at which targets the remote user is looking. The
              second experiment further compared a spherical display with a
              planar display and provided detailed reasons for the improvement
              of our system in conveying gaze. We found two linear models for
              predicting the distortion introduced by misalignment of capturing
              cameras and the observer's viewing angles in video conferencing
              systems. Those models might be able to enable a correction for
              this distortion in future display configurations.",
  month    =  "2~" # jan,
  year     =  2015,
  url      = "https://doi.org/10.1162%2Fpres_a_00213",
  doi      = "10.1162/pres\_a\_00213",
  issn     = "1054-7460,1531-3263",
  language = "en"
}

@ARTICLE{Stavrova2019-bb,
  title    = "{The cynical genius illusion: Exploring and debunking lay beliefs
              about cynicism and competence}",
  author   = "Stavrova, Olga and Ehlebracht, Daniel",
  journal  = "Personality \& social psychology bulletin",
  volume   =  45,
  number   =  2,
  pages    = "254–269",
  abstract = "Cynicism refers to a negative appraisal of human nature-a belief
              that self-interest is the ultimate motive guiding human behavior.
              We explored laypersons' beliefs about cynicism and competence and
              to what extent these beliefs correspond to reality. Four studies
              showed that laypeople tend to believe in cynical individuals'
              cognitive superiority. A further three studies based on the data
              of about 200,000 individuals from 30 countries debunked these lay
              beliefs as illusionary by revealing that cynical (vs. less
              cynical) individuals generally do worse on cognitive ability and
              academic competency tasks. Cross-cultural analyses showed that
              competent individuals held contingent attitudes and endorsed
              cynicism only if it was warranted in a given sociocultural
              environment. Less competent individuals embraced cynicism
              unconditionally, suggesting that-at low levels of
              competence-holding a cynical worldview might represent an adaptive
              default strategy to avoid the potential costs of falling prey to
              others' cunning.",
  month    =  feb,
  year     =  2019,
  url      = "http://dx.doi.org/10.1177/0146167218783195",
  file     = "All Papers/My Library/Stavrova and Ehlebracht 2019 - The cynical genius illusion - Exploring and debunking lay beliefs about cynicism and competence.pdf",
  doi      = "10.1177/0146167218783195",
  issn     = "0146-1672,1552-7433",
  language = "en"
}

@ARTICLE{Croes2018-vv,
  title   = "{Social attraction in video-mediated communication: The role of
             nonverbal affiliative behavior}",
  author  = "Croes, Emmelyn A J and Antheunis, Marjolijn L and Schouten,
             Alexander P and Krahmer, Emiel J",
  journal = "Journal of social and personal relationships",
  volume  =  36,
  number  =  4,
  pages   = "1210–1232",
  month   =  feb,
  year    =  2018,
  url     = "https://doi.org/10.1177%2F0265407518757382",
  file    = "All Papers/My Library/Croes et al. 2018 - Social attraction in video-mediated communication - The role of nonverbal affiliative behavior.pdf",
  doi     = "10.1177/0265407518757382",
  issn    = "0265-4075"
}

@ARTICLE{Wade2015-gp,
  title    = "{How were eye movements recorded before yarbus?}",
  author   = "Wade, Nicholas J",
  journal  = "Perception",
  volume   =  44,
  number   = "8-9",
  pages    = "851–883",
  abstract = "Alfred Yarbus introduced a new dimension of precision in recording
              how the eyes moved, either when attempts were made to keep them
              stationary or when scanning pictures. Movements of the eyes had
              been remarked upon for millennia, but recording how they move is a
              more recent preoccupation. Emphasis was initially placed on
              abnormalities of oculomotor function (like strabismus) before
              normal features were considered. The interest was in where the
              eyes moved to rather than determining how they got there. The most
              venerable technique for examining ocular stability involved
              comparing the relative motion between an afterimage and a real
              image. In the late 18th century, Wells compared afterimages
              generated before body rotation with real images observed following
              it when dizzy; he described both lateral and torsional nystagmus,
              thereby demonstrating the directional discontinuities in eye
              velocities. At around the same time Erasmus Darwin used
              afterimages as a means of demonstrating ocular instability when
              attempting to fixate steadily. However, the overriding concern in
              the 19th century was with eye position rather than eye movements.
              Thus, the characteristics of nystagmus were recorded before those
              of saccades and fixations. Eye movements during reading were
              described by Hering and by Lamare (working in Javal's laboratory)
              in 1879; both used similar techniques of listening (with tubes
              placed over the eyelids) to the sounds made during contractions of
              the extraocular muscles. Photographic records of eye movements
              during reading were made by Dodge early in the 20th century, and
              this stimulated research using a wider array of patterns. Eye
              movements over pictures were examined by Stratton and later by
              Buswell, who drew attention to the effects of instructions on the
              pattern of eye movements. In midcentury, attention shifted back to
              the stability of the eyes during fixation, with the emphasis on
              involuntary movements. The suction cap methods developed by Yarbus
              were applied with some success to recording the perceptual effects
              of retinal image stabilization. It is an historical irony that the
              accuracy of image stabilization with contact lenses was assessed
              by comparison with the oldest method for examining eye
              movements–afterimages.",
  month    =  "14~" # aug,
  year     =  2015,
  url      = "http://dx.doi.org/10.1177/0301006615594947",
  doi      = "10.1177/0301006615594947",
  issn     = "0301-0066",
  language = "en"
}

@ARTICLE{Van_der_Kleij2009-bv,
  title     = "{How Conversations Change Over Time in {Face-to-Face} and
               {Video-Mediated} Communication}",
  author    = "van der Kleij, Rick and Maarten Schraagen, Jan and Werkhoven,
               Peter and De Dreu, Carsten K W",
  journal   = "Small Group Research",
  publisher = "SAGE Publications Inc",
  volume    =  40,
  number    =  4,
  pages     = "355--381",
  abstract  = "An experiment was conducted to examine how communication patterns
               and task performance differ as a function of the group's
               communication environment and how these processes change over
               time. In a longitudinal design, three-person groups had to select
               and argue the correct answer out of a set of three alternatives
               for ten questions. Compared with face-to-face groups,
               video-teleconferencing groups took fewer turns, required more
               time for turns, and interrupted each other less. Listeners
               appeared to be more polite, waiting for a speaker to finish
               before making their conversational contribution. Although groups
               were able to maintain comparable performance scores across
               communication conditions, initial differences between conditions
               in communication patterns disappeared over time, indicating that
               the video-teleconferencing groups adapted to the newness and
               limitations of their communication environment. Moreover, because
               of increased experience with the task and the group, groups in
               both conditions needed less conversation to complete the task at
               later rounds. Implications are discussed for practice, training,
               and possibilities for future research.",
  month     =  aug,
  year      =  2009,
  url       = "http://dx.doi.org/10.1177/1046496409333724",
  doi       = "10.1177/1046496409333724",
  issn      = "1046-4964"
}

@ARTICLE{Van_der_Kleij2009-bv,
  title    = "{How conversations change over time in Face-to-Face and
              Video-Mediated communication}",
  author   = "van der Kleij, Rick and Maarten Schraagen, Jan and Werkhoven,
              Peter and De Dreu, Carsten K W",
  journal  = "Small Group Research",
  volume   =  40,
  number   =  4,
  pages    = "355–381",
  abstract = "An experiment was conducted to examine how communication patterns
              and task performance differ as a function of the group's
              communication environment and how these processes change over
              time. In a longitudinal design, three-person groups had to select
              and argue the correct answer out of a set of three alternatives
              for ten questions. Compared with face-to-face groups,
              video-teleconferencing groups took fewer turns, required more time
              for turns, and interrupted each other less. Listeners appeared to
              be more polite, waiting for a speaker to finish before making
              their conversational contribution. Although groups were able to
              maintain comparable performance scores across communication
              conditions, initial differences between conditions in
              communication patterns disappeared over time, indicating that the
              video-teleconferencing groups adapted to the newness and
              limitations of their communication environment. Moreover, because
              of increased experience with the task and the group, groups in
              both conditions needed less conversation to complete the task at
              later rounds. Implications are discussed for practice, training,
              and possibilities for future research.",
  month    =  "8~" # jan,
  year     =  2009,
  url      = "https://doi.org/10.1177/1046496409333724",
  file     = "All Papers/My Library/van der Kleij et al. 2009 - How conversations change over time in Face-to-Face and Video-Mediated communication.pdf",
  doi      = "10.1177/1046496409333724",
  issn     = "1046-4964"
}

@ARTICLE{Aagaard2022-nk,
  title     = "{On the dynamics of Zoom fatigue}",
  author    = "Aagaard, Jesper",
  journal   = "Convergence The International Journal of Research into New Media
               Technologies",
  publisher = "SAGE Publications",
  volume    =  28,
  number    =  6,
  pages     = "1878--1891",
  abstract  = "The COVID-19 pandemic has made videoconferencing tools an
               essential part of our lives as these tools are what allowed us to
               keep in touch in a time of social distancing. Having said that,
               however, many people have found virtual interactions to be
               surprisingly exhausting. This has given rise to the concept of
               Zoom fatigue. The purpose of this article is to explore the
               dynamics that give rise to this peculiar phenomenon. The article
               first discusses the concept of Zoom fatigue and critiques the
               brain-centrism of current explanations. It then proposes a more
               embodied approach to interaction, discusses the mediating role of
               technology in videoconferencing, and proceeds to presents a list
               of five videoconferencing dynamics that may induce Zoom fatigue:
               Awkward turn-taking, inhibited spontaneity, restricted motility,
               lack of eye contact and increased self-awareness. Finally, it is
               argued that these dynamics should make us temper our collective
               expectations about the hybrid future.",
  month     =  dec,
  year      =  2022,
  url       = "https://consensus.apphttps://consensus.app/papers/dynamics-zoom-fatigue-aagaard/acecc9ed268051f983652923116c678c/?q=research+that+eye+tracking+for+estimate+eye+contact+and++zoom+fatigue+etc.+on+videoconferencing&copilot=on",
  file      = "All Papers/Other/Aagaard 2022 - On the dynamics of Zoom fatigue.pdf",
  doi       = "10.1177/13548565221099711",
  issn      = "1354-8565,1748-7382",
  language  = "en"
}

@ARTICLE{Monk2002-fu,
  title    = "{A look is worth a thousand words: Full gaze awareness in
              Video-Mediated conversation}",
  author   = "Monk, Andrew F and Gale, Caroline",
  journal  = "Discourse processes",
  volume   =  33,
  number   =  3,
  pages    = "257–278",
  abstract = "Full gaze awareness, defined here as knowing what someone is
              looking at, might be expected to be a powerful communicative
              resource when the conversation concerns some object of common
              interest in the environment. This article sets out to demonstrate
              this possibility in the context of video-mediated communication.
              An experiment is reported in which pairs complete a communication
              task using a novel apparatus that supports full gaze awareness
              (GA) and mutual gaze (eye contact). This “GA display” was
              contrasted with 2 control conditions, mutual gaze without full
              gaze awareness and audio only. The GA display reduced the number
              of turns and number of words required to complete the task by
              about 1/2 in comparison with the 2 control conditions. The results
              of a subsequent conversational games analysis suggest that at
              least part of this saving comes about because full gaze awareness
              provides an alternative nonlinguistic channel for checking one's
              own and the other person's understanding of what was said.",
  month    =  may,
  year     =  2002,
  url      = "http://dx.doi.org/10.1207/S15326950DP3303_4",
  file     = "All Papers/My Library/Monk and Gale 2002 - A look is worth a thousand words - Full gaze awareness in Video-Mediated conversation.pdf",
  doi      = "10.1207/S15326950DP3303\_4",
  issn     = "0163-853X"
}

@ARTICLE{Monk2002-vn,
  title     = "{A Look Is Worth a Thousand Words: Full Gaze Awareness in
               {Video-Mediated} Conversation}",
  author    = "Monk, Andrew F and Gale, Caroline",
  journal   = "Discourse processes",
  publisher = "Routledge",
  volume    =  33,
  number    =  3,
  pages     = "257--278",
  abstract  = "Full gaze awareness, defined here as knowing what someone is
               looking at, might be expected to be a powerful communicative
               resource when the conversation concerns some object of common
               interest in the environment. This article sets out to demonstrate
               this possibility in the context of video-mediated communication.
               An experiment is reported in which pairs complete a communication
               task using a novel apparatus that supports full gaze awareness
               (GA) and mutual gaze (eye contact). This ``GA display'' was
               contrasted with 2 control conditions, mutual gaze without full
               gaze awareness and audio only. The GA display reduced the number
               of turns and number of words required to complete the task by
               about 1/2 in comparison with the 2 control conditions. The
               results of a subsequent conversational games analysis suggest
               that at least part of this saving comes about because full gaze
               awareness provides an alternative nonlinguistic channel for
               checking one's own and the other person's understanding of what
               was said.",
  month     =  may,
  year      =  2002,
  url       = "http://dx.doi.org/10.1207/S15326950DP3303_4",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.1207/S15326950DP3303\_4",
  issn      = "0163-853X"
}

@ARTICLE{Sellen1995-yc,
  title    = "{Remote conversations: The effects of mediating talk with
              technology}",
  author   = "Sellen, Abigail J",
  journal  = "Human–Computer Interaction",
  volume   =  10,
  number   =  4,
  pages    = "401–444",
  abstract = "Three different videoconferencing systems for supporting
              multiparty, remote conversations are described and evaluated
              experimentally. The three systems differed by how many
              participants were visible at once, their spatial arrangement, and
              control over who was seen. Conversations using these systems were
              compared to same-room (Experiment 1) and audio-only (Experiment 2)
              conversations. Specialized speech-tracking equipment recorded the
              on-off patterns of speech that allowed objective measurement of
              structural aspects of the conversations, such as turn length,
              pauses, and interruptions. Questionnaires and interviews also
              documented participants' opinions and perceptions in the various
              settings. Contrary to expectation, systems in which visual cues
              such as selective gaze were absent produced no differences in
              turn-taking or in any other aspect of the structure of
              conversation. In fact, turn-taking was unaffected even when visual
              information was completely absent. Overall, only the same-room
              condition showed any significant differences from any other
              condition; people in the same room produced more interruptions and
              fewer formal handovers of the floor than in any of the
              technology-mediated conditions. In this respect, the audio-only
              and video systems examined in these studies were equivalent.
              However, analyses of participants' perceptions showed that
              participants felt that visual access in mediated conversations was
              both important and beneficial in conversation. Further, there were
              indications that the particular design of the different video
              systems did affect some aspects of conversational behavior, such
              as the ability to hold side and parallel conversations.",
  month    =  "12~" # jan,
  year     =  1995,
  url      = "https://www.tandfonline.com/doi/abs/10.1207/s15327051hci1004_2",
  doi      = "10.1207/s15327051hci1004\_2",
  issn     = "0737-0024"
}

@ARTICLE{Sellen1995-ja,
  title     = "{Remote conversations: the effects of mediating talk with
               technology}",
  author    = "Sellen, Abigail J",
  journal   = "Hum. -Comput. Interact.",
  publisher = "L. Erlbaum Associates Inc.",
  address   = "USA",
  volume    =  10,
  number    =  4,
  pages     = "401--444",
  abstract  = "Three different videoconferencing systems for supporting
               multiparty, remote conversations are described and evaluated
               experimentally. The three systems differed by how many
               participants were visible at once, their spatial arrangement, and
               control over who was seen. Conversations using these systems were
               compared to same-room (Experiment 1) and audio-only (Experiment
               2) conversations. Specialized speech-tracking equipment recorded
               the on-off patterns of speech that allowed objective measurement
               of structural aspects of the conversations, such as turn length,
               pauses, and interruptions. Questionnaires and interviews also
               documented participants' opinions and perceptions in the various
               settings.Contrary to expectation, systems in which visual cues
               such as selective gaze were absent produced no differences in
               turn-taking or in any other aspect of the structure of
               conversation. In fact, turn-taking was unaffected even when
               visual information was completely absent. Overall, only the
               same-room condition showed any significant differences from any
               other condition; people in the same room produced more
               interruptions and fewer formal handovers of the floor than in any
               of the technology-mediated conditions. In this respect, the
               audio-only and video systems examined in these studies were
               equivalent. However, analyses of participants' perceptions showed
               that participants felt that visual access in mediated
               conversations was both important and beneficial in conversation.
               Further, there were indications that the particular design of the
               different video systems did affect some aspects of conversational
               behavior, such as the ability to hold side and parallel
               conversations.",
  month     =  dec,
  year      =  1995,
  url       = "http://dx.doi.org/10.1207/s15327051hci1004_2",
  doi       = "10.1207/s15327051hci1004\_2",
  issn      = "0737-0024"
}

@ARTICLE{Harris2009-it,
  title     = "{The {ERICA} eye gaze system versus manual letter board to aid
               communication in {ALS/{MND}}}",
  author    = "Harris, Donna and Goren, Mark",
  journal   = "British Journal of Neuroscience Nursing",
  publisher = "Mark Allen Group",
  volume    =  5,
  number    =  5,
  pages     = "227--230",
  abstract  = "Communication is essential to providing individuals with
               effective health care, but degenerative neurological conditions
               such as amyotrophic lateral sclerosis or motor neurone disease
               (ALS/MND) impair patients' physical ability to communicate to get
               their needs met. New technologies are giving people with motor
               disabilities alternative means of communicating with caregivers
               and health professionals. Aim: The aim of this study was to
               determine whether ERICA, a hands-free, eye-tracking communication
               device, is practical for use by people with ALS/MND. The ERICA
               eye gaze system was compared with a manual letter board and
               evaluated for speed and ease of use. Method: Six patients were
               asked to spell out a simple sentence with the assistance of a
               caregiver, first using a manual letter board, then using the
               ERICA eye gaze system. Results: Four out of the six patients
               communicated more quickly using the manual letter board but four
               out of the six patients rated the ERICA eye gaze system as less
               difficult to use. Conclusions: It is recommended that health
               professionals use both low-tech and high-tech solutions to
               address the loss of speech experienced by people with ALS/MND
               during disease progression.",
  month     =  may,
  year      =  2009,
  url       = "http://dx.doi.org/10.12968/bjnn.2009.5.5.42128",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.12968/bjnn.2009.5.5.42128",
  issn      = "1747-0307"
}

@ARTICLE{Harris2009-kk,
  title    = "{The ERICA eye gaze system versus manual letter board to aid
              communication in ALS/{MND}}",
  author   = "Harris, Donna and Goren, Mark",
  journal  = "British Journal of Neuroscience Nursing",
  volume   =  5,
  number   =  5,
  pages    = "227–230",
  abstract = "Communication is essential to providing individuals with effective
              health care, but degenerative neurological conditions such as
              amyotrophic lateral sclerosis or motor neurone disease (ALS/MND)
              impair patients' physical ability to communicate to get their
              needs met. New technologies are giving people with motor
              disabilities alternative means of communicating with caregivers
              and health professionals. Aim: The aim of this study was to
              determine whether ERICA, a hands-free, eye-tracking communication
              device, is practical for use by people with ALS/MND. The ERICA eye
              gaze system was compared with a manual letter board and evaluated
              for speed and ease of use. Method: Six patients were asked to
              spell out a simple sentence with the assistance of a caregiver,
              first using a manual letter board, then using the ERICA eye gaze
              system. Results: Four out of the six patients communicated more
              quickly using the manual letter board but four out of the six
              patients rated the ERICA eye gaze system as less difficult to use.
              Conclusions: It is recommended that health professionals use both
              low-tech and high-tech solutions to address the loss of speech
              experienced by people with ALS/MND during disease progression.",
  month    =  "5~" # jan,
  year     =  2009,
  url      = "https://doi.org/10.12968/bjnn.2009.5.5.42128",
  doi      = "10.12968/bjnn.2009.5.5.42128",
  issn     = "1747-0307"
}

@ARTICLE{Benvenuto2013-aa,
  title     = "{The gaze of the blind: Notes on cézanne and cubism}",
  author    = "Benvenuto, Sergio",
  journal   = "The American imago; a psychoanalytic journal for the arts and
               sciences",
  publisher = "Johns Hopkins University Press",
  volume    =  70,
  number    =  3,
  pages     = "385--406",
  abstract  = "… Let us begin by trying to understand what expressionism does,
               focusing on edward Munch's The Scream (1893), a paint ing so
               famous it is a national symbol of Norway as well as a type …",
  year      =  2013,
  url       = "http://dx.doi.org/10.1353/aim.2013.0022",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.1353/aim.2013.0022",
  issn      = "0065-860X,1085-7931"
}

@ARTICLE{Benvenuto2013-yg,
  title    = "{The gaze of the blind: Notes on cézanne and cubism}",
  author   = "Benvenuto, Sergio",
  journal  = "The American imago; a psychoanalytic journal for the arts and
              sciences",
  volume   =  70,
  number   =  3,
  pages    = "385–406",
  abstract = "ARRAY(0x55f987d20c00)",
  year     =  2013,
  url      = "https://muse.jhu.edu/article/522599",
  doi      = "10.1353/aim.2013.0022",
  issn     = "0065-860X,1085-7931"
}

@ARTICLE{Hassoumi2019-jo,
  title     = "{Improving eye-tracking calibration accuracy using symbolic
               regression}",
  author    = "Hassoumi, Almoctar and Peysakhovich, Vsevolod and Hurter,
               Christophe",
  journal   = "PloS one",
  publisher = "Public Library of Science (PLoS)",
  volume    =  14,
  number    =  3,
  pages     = "e0213675",
  abstract  = "Eye tracking systems have recently experienced a diversity of
               novel calibration procedures, including smooth pursuit and
               vestibulo-ocular reflex based calibrations. These approaches
               allowed collecting more data compared to the standard 9-point
               calibration. However, the computation of the mapping function
               which provides planar gaze positions from pupil features given as
               input is mostly based on polynomial regressions, and little work
               has investigated alternative approaches. This paper fills this
               gap by providing a new calibration computation method based on
               symbolic regression. Instead of making prior assumptions on the
               polynomial transfer function between input and output records,
               symbolic regression seeks an optimal model among different types
               of functions and their combinations. This approach offers an
               interesting perspective in terms of flexibility and accuracy.
               Therefore, we designed two experiments in which we collected
               ground truth data to compare vestibulo-ocular and smooth pursuit
               calibrations based on symbolic regression, both using a marker or
               a finger as a target, resulting in four different calibrations.
               As a result, we improved calibration accuracy by more than 30\%,
               with reasonable extra computation time.",
  month     =  "15~" # mar,
  year      =  2019,
  url       = "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0213675&type=printable",
  file      = "All Papers/Other/Hassoumi et al. 2019 - Improving eye-tracking calibration accuracy using symbolic regression.pdf",
  doi       = "10.1371/journal.pone.0213675",
  pmc       = "PMC6420251",
  pmid      =  30875387,
  issn      = "1932-6203",
  language  = "en"
}

@ARTICLE{Hosp2021-yt,
  title    = "{Soccer goalkeeper expertise identification based on eye
              movements}",
  author   = "Hosp, Benedikt W and Schultz, Florian and Höner, Oliver and
              Kasneci, Enkelejda",
  journal  = "PloS one",
  volume   =  16,
  number   =  5,
  pages    = "e0251070",
  abstract = "By focusing on high experimental control and realistic
              presentation, the latest research in expertise assessment of
              soccer players demonstrates the importance of perceptual skills,
              especially in decision making. Our work captured omnidirectional
              in-field scenes displayed through virtual reality glasses to 12
              expert players (picked by DFB), 10 regional league intermediate
              players, and13 novice soccer goalkeepers in order to assess the
              perceptual skills of athletes in an optimized manner. All scenes
              were shown from the perspective of the same natural goalkeeper and
              ended after the return pass to that goalkeeper. Based on the gaze
              behavior of each player, we classified their expertise with common
              machine learning techniques. Our results show that eye movements
              contain highly informative features and thus enable a
              classification of goalkeepers between three stages of expertise,
              namely elite youth player, regional league player, and novice, at
              a high accuracy of 78.2\%. This research underscores the
              importance of eye tracking and machine learning in perceptual
              expertise research and paves the way for perceptual-cognitive
              diagnosis as well as future training systems.",
  month    =  "19~" # may,
  year     =  2021,
  url      = "http://dx.doi.org/10.1371/journal.pone.0251070",
  file     = "All Papers/My Library/Hosp et al. 2021 - Soccer goalkeeper expertise identification based on eye movements.pdf",
  doi      = "10.1371/journal.pone.0251070",
  issn     = "1932-6203",
  language = "en"
}

@INPROCEEDINGS{Creed2016-ej,
  title     = "{Eye gaze interaction for supporting creative work with disabled
               artists}",
  author    = "Creed, Dr Chris",
  publisher = "BCS Learning \& Development",
  abstract  = "… These are easy to manipulate with a mouse (if you do not have a
               physical impairment ), but … issues when using eye gaze
               technology as users will be directing their gaze away from the …",
  month     =  jul,
  year      =  2016,
  url       = "http://dx.doi.org/10.14236/ewic/hci2016.98",
  file      = "All Papers/Other/Creed 2016 - Eye gaze interaction for supporting creative work with disabled artists.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.14236/ewic/hci2016.98"
}

@INPROCEEDINGS{Creed2016-wh,
  title     = "{Eye gaze interaction for supporting creative work with disabled
               artists}",
  author    = "Creed, Dr Chris",
  publisher = "BCS Learning \& Development",
  abstract  = "Disabled artists with physical impairments can experience
               significant issues when using artistic tools such as brushes,
               pencils, and other materials. Traditional assistive tools (e.g.
               head wands and mouth sticks) can help facilitate artistic work,
               but also introduce further issues such as chronic neck strain and
               damage to teeth. Moreover, research has shown that people with
               severe physical impairments need significant support from others
               when doing creative work around setting up tools, moving
               materials, and being available to make any further adjustments
               (Perera et al., 2009). This lack of independence can be
               particularly frustrating for disabled artists and can stifle
               creative flow.",
  month     =  "7~" # jan,
  year      =  2016,
  url       = "http://dx.doi.org/10.14236/ewic/hci2016.98",
  file      = "All Papers/My Library/Creed 2016 - Eye gaze interaction for supporting creative work with disabled artists.pdf",
  doi       = "10.14236/ewic/hci2016.98"
}

@ARTICLE{2009-et,
  title    = "{遠隔授業における視線一致の必要性とその問題点解決のための一手法}",
  author   = "博司, 田上",
  journal  = "教育システム情報学会誌",
  volume   =  25,
  number   =  4,
  pages    = "394--402",
  year     =  2009,
  url      = "http://dx.doi.org/10.14926/jsise.25.394",
  file     = "All Papers/Other/博司 2009 - 遠隔授業における視線一致の必要性とその問題点解決のための一手法.pdf",
  keywords = "eye contact;telepresence",
  doi      = "10.14926/jsise.25.394"
}

@ARTICLE{2009-em,
  title   = "{遠隔授業における視線一致の必要性とその問題点解決のための一手法}",
  author  = "博司, 田上",
  journal = "教育システム情報学会誌",
  volume  =  25,
  number  =  4,
  pages   = "394–402",
  year    =  2009,
  url     = "http://dx.doi.org/10.14926/jsise.25.394",
  file    = "All Papers/My Library/博司 2009 - 遠隔授業における視線一致の必要性とその問題点解決のための一手法.pdf",
  doi     = "10.14926/jsise.25.394"
}

@ARTICLE{Pieters2004-os,
  title    = "{Attention Capture and Transfer in Advertising: Brand, Pictorial,
              and Text-Size Effects}",
  author   = "Pieters, Rik and Wedel, Michel",
  journal  = "Journal of marketing",
  volume   =  68,
  number   =  2,
  pages    = "36--50",
  abstract = "The three key ad elements (brand, pictorial, and text) each have
              unique superiority effects on attention to advertisements, which
              are on par with many commonly held ideas in marketing practice.
              This is the main conclusion of an analysis of 1363 print
              advertisements tested with infrared eye-tracking methodology on
              more than 3600 consumers. The pictorial is superior in capturing
              attention, independent of its size. The text element best captures
              attention in direct proportion to its surface size. The brand
              element most effectively transfers attention to the other
              elements. Only increments in the text element's surface size
              produce a net gain in attention to the advertisement as a whole.
              The authors discuss how their findings can be used to render more
              effective decisions in advertising.",
  month    =  "1~" # apr,
  year     =  2004,
  url      = "https://doi.org/10.1509/jmkg.68.2.36.27794",
  doi      = "10.1509/jmkg.68.2.36.27794",
  issn     = "0022-2429",
  language = "en"
}

@ARTICLE{Brohl2023-tl,
  title     = "{Detection of spatially localized sounds is robust to saccades
               and concurrent eye movement-related eardrum oscillations
               (EMREOs)}",
  author    = "Bröhl, Felix and Kayser, Christoph",
  journal   = "The Journal of neuroscience: the official journal of the Society
               for Neuroscience",
  publisher = "Society for Neuroscience",
  volume    =  43,
  number    =  45,
  pages     = "7668--7677",
  abstract  = "Hearing is an active process, and recent studies show that even
               the ear is affected by cognitive states or motor actions. One
               example are movements of the eardrum induced by saccadic eye
               movements, known as ``eye movement-related eardrum oscillations''
               (EMREOs). While these are systematically shaped by the direction
               and size of saccades, the consequences of saccadic eye movements
               and their resulting EMREOs for hearing remain unclear. We here
               studied their implications for the detection of near-threshold
               clicks in human participants. Across three experiments, sound
               detection was not affected by their time of presentation relative
               to saccade onset, by saccade amplitude or direction. While the
               EMREOs were shaped by the direction and amplitude of the saccadic
               movement, inducing covert shifts in spatial attention did not
               affect the EMREO, suggesting that this signature of active
               sensing is restricted to overt changes in visual focus.
               Importantly, in our experiments, fluctuations in the EMREO
               amplitude were not related to detection performance, at least
               when monaural cues are sufficient. Hence, while eye movements may
               shape the transduction of acoustic information, the behavioral
               implications remain to be understood.SIGNIFICANCE STATEMENT
               Previous studies suggest that oculomotor behavior may influence
               how we perceive spatially localized sounds. Recent work has
               introduced a new perspective on this question by showing that eye
               movements can directly modulate the eardrum. Yet, it remains
               unclear whether this signature of active hearing accounts for
               behavioral effects. We here show that overt but not covert
               changes in visual attention modulate the eardrum, but these
               modulations do not interfere with the detection of sounds. Our
               results provide a starting point to obtain a deeper understanding
               about the interplay of oculomotor behavior and the active ear.",
  month     =  "8~" # nov,
  year      =  2023,
  url       = "https://www.jneurosci.org/content/43/45/7668",
  file      = "All Papers/Other/Bröhl and Kayser 2023 - Detection of spatially localized sounds is robus ... des and concurrent eye movement-related eardrum oscillations (EMREOs).pdf",
  keywords  = "auditory pathway; eardrum; eye movements; hearing; spatial
               detection",
  doi       = "10.1523/JNEUROSCI.0818-23.2023",
  pmc       = "PMC10634546",
  pmid      =  37734948,
  issn      = "0270-6474,1529-2401",
  language  = "en"
}

@ARTICLE{2018-ps,
  title    = "{テレビ会議話者間の視線一致知覚のための目領域合成手法}",
  author   = "卓弥, 井上 and 高嗣, 平山 and 友和, 高橋 and 康友, 川西 and 大輔, 出口 and 一郎, 井手 and
              洋, 村瀬 and 隆行, 黒住 and 邦夫, 柏野",
  journal  = "電気学会論文誌Ｃ（電子・情報・システム部門誌）",
  volume   =  138,
  number   =  11,
  pages    = "1399--1409",
  year     =  2018,
  url      = "http://dx.doi.org/10.1541/ieejeiss.138.1399",
  file     = "All Papers/Other/卓弥 et al. 2018 - テレビ会議話者間の視線一致知覚のための目領域合成手法.pdf",
  keywords = "eye contact;telepresence",
  doi      = "10.1541/ieejeiss.138.1399"
}

@ARTICLE{2018-fm,
  title   = "{テレビ会議話者間の視線一致知覚のための目領域合成手法}",
  author  = "卓弥, 井上 and 高嗣, 平山 and 友和, 高橋 and 康友, 川西 and 大輔, 出口 and 一郎, 井手 and
             洋, 村瀬 and 隆行, 黒住 and 邦夫, 柏野",
  journal = "電気学会論文誌Ｃ（電子・情報・システム部門誌）",
  volume  =  138,
  number  =  11,
  pages   = "1399–1409",
  year    =  2018,
  url     = "http://dx.doi.org/10.1541/ieejeiss.138.1399",
  file    = "All Papers/My Library/卓弥 et al. 2018 - テレビ会議話者間の視線一致知覚のための目領域合成手法.pdf",
  doi     = "10.1541/ieejeiss.138.1399"
}

@ARTICLE{Thu2017-mt,
  title   = "{Deep-pipelined FPGA implementation of real-time object tracking
             using a particle filter}",
  author  = "Thu, Theint Theint and Hayashida, Yoshiki and Tahara, Akane and
             Shibata, Yuichiro and Oguri, Kiyoshi",
  journal = "International Journal of High Performance Computing and Networking",
  volume  =  7,
  number  =  2,
  pages   = "372–386",
  year    =  2017,
  url     = "http://dx.doi.org/10.15803/ijnc.7.2_372",
  file    = "All Papers/My Library/Thu et al. 2017 - Deep-pipelined FPGA implementation of real-time object tracking using a particle filter.pdf",
  doi     = "10.15803/ijnc.7.2\_372"
}

@ARTICLE{Zhe2020-xh,
  title    = "{Calibration-free gaze interfaces based on linear smooth pursuit}",
  author   = "Zhe, Zeng and Siebert, Felix Wilhelm and Venjakob, Antje Christine
              and Roetting, Matthias",
  journal  = "Journal of eye movement research",
  volume   =  13,
  number   =  1,
  abstract = "Since smooth pursuit eye movements can be used without calibration
              in spontaneous gaze interaction, the intuitiveness of the gaze
              interface design has been a topic of great interest in the
              human-computer interaction field. However, since most related
              research focuses on curved smooth-pursuit trajectories, the design
              issues of linear trajectories are poorly understood. Hence, this
              study evaluated the user performance of gaze interfaces based on
              linear smooth pursuit eye movements. We conducted an experiment to
              investigate how the number of objects (6, 8, 10, 12, or 15) and
              object moving speed (7.73 ˚/s vs. 12.89 ˚/s) affect the user
              performance in a gaze-based interface. Results show that the
              number and speed of the displayed objects influence users'
              performance with the interface. The number of objects
              significantly affected the correct and false detection rates when
              selecting objects in the display. Participants' performance was
              highest on interfaces containing 6 and 8 objects and decreased for
              interfaces with 10, 12, and 15 objects. Detection rates and
              orientation error were significantly influenced by the moving
              speed of displayed objects. Faster moving speed (12.89 ˚/s)
              resulted in higher detection rates and smaller orientation error
              compared to slower moving speeds (7.73 ˚/s). Our findings can help
              to enable a calibration-free accessible interaction with gaze
              interfaces.",
  month    =  "3~" # oct,
  year     =  2020,
  url      = "http://dx.doi.org/10.16910/jemr.13.1.3",
  file     = "All Papers/My Library/Zhe et al. 2020 - Calibration-free gaze interfaces based on linear smooth pursuit.pdf",
  doi      = "10.16910/jemr.13.1.3",
  issn     = "1995-8692",
  language = "en"
}

@ARTICLE{Pannasch2008-ie,
  title     = "{Eye typing in application: A comparison of two systems with
               {ALS} patients}",
  author    = "Pannasch, Sebastian and Helmert, Jens R and Malischke, Susann and
               Storch, Alexander and Velichkovsky, Boris M",
  journal   = "Journal of eye movement research",
  publisher = "University of Bern",
  volume    =  2,
  number    =  4,
  abstract  = "A variety of eye typing systems has been developed during the
               last decades. Such systems can provide support for people who
               lost the ability to communicate, e.g. patients suffering from
               motor neuron diseases such as amyotrophic lateral sclerosis
               (ALS). In the current retrospective analysis, two eye typing
               applications were tested (EyeGaze, GazeTalk) by ALS patients (N =
               4) in order to analyze objective performance measures and
               subjective ratings. An advantage of the EyeGaze system was found
               for most of the evaluated criteria. The results are discussed in
               respect of the special target population and in relation to
               requirements of eye tracking devices.",
  month     =  nov,
  year      =  2008,
  url       = "http://dx.doi.org/10.16910/jemr.2.4.6",
  file      = "All Papers/Other/Pannasch et al. 2008 - Eye typing in application - A comparison of two systems with ALS patients.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.16910/jemr.2.4.6",
  issn      = "1995-8692"
}

@ARTICLE{Pannasch2008-ao,
  title    = "{Eye typing in application: A comparison of two systems with ALS
              patients}",
  author   = "Pannasch, Sebastian and Helmert, Jens R and Malischke, Susann and
              Storch, Alexander and Velichkovsky, Boris M",
  journal  = "Journal of eye movement research",
  volume   =  2,
  number   =  4,
  abstract = "A variety of eye typing systems has been developed during the last
              decades. Such systems can provide support for people who lost the
              ability to communicate, e.g. patients suffering from motor neuron
              diseases such as amyotrophic lateral sclerosis (ALS). In the
              current retrospective analysis, two eye typing applications were
              tested (EyeGaze, GazeTalk) by ALS patients (N = 4) in order to
              analyze objective performance measures and subjective ratings. An
              advantage of the EyeGaze system was found for most of the
              evaluated criteria. The results are discussed in respect of the
              special target population and in relation to requirements of eye
              tracking devices.",
  month    =  "26~" # nov,
  year     =  2008,
  url      = "http://dx.doi.org/10.16910/jemr.2.4.6",
  file     = "All Papers/My Library/Pannasch et al. 2008 - Eye typing in application - A comparison of two systems with ALS patients.pdf",
  doi      = "10.16910/jemr.2.4.6",
  issn     = "1995-8692"
}

@ARTICLE{Regenbrecht2015-gl,
  title     = "{Mutual gaze support in videoconferencing reviewed}",
  author    = "Regenbrecht, Holger and Langlotz, Tobias",
  journal   = "Commun. Assoc. Inf. Syst.",
  publisher = "Association for Information Systems",
  volume    =  37,
  pages     =  45,
  abstract  = "This review gives decision makers, researchers, and developers a
               tool to systematically apply and further develop
               videoconferencing systems in ``serious'' settings requiring
               mutual gaze, and gives recommendations for future developments.
               Videoconferencing allows geographically dispersed parties to
               communicate by simultaneous audio and video transmissions. It is
               used in a variety of application scenarios with a wide range of
               coordination needs and efforts, such as private chat, discussion
               meetings, and negotiation tasks. In particular, in scenarios
               requiring certain levels of trust and judgement non-verbal
               communication, cues are highly important for effective
               communication. Mutual gaze support plays a central role in those
               high coordination need scenarios but generally lacks adequate
               technical support from videoconferencing systems. In this paper,
               we review technical concepts and implementations for mutual gaze
               support in videoconferencing, classify them, evaluate them
               according to a defined set of criteria, and give recommendations
               for future developments. Our review gives decision makers,
               researchers, and developers a tool to systematically apply and
               further develop videoconferencing systems in ``serious'' settings
               requiring mutual gaze. This should lead to well-informed
               decisions regarding the use and development of this technology
               and to a more widespread exploitation of the benefits of
               videoconferencing in general. For example, if videoconferencing
               systems supported high-quality mutual gaze in an easy-to-set-up
               and easy-to-use way, we could hold more effective and efficient
               recruitment interviews, court hearings, or contract negotiations.",
  year      =  2015,
  url       = "http://dx.doi.org/10.17705/1cais.03745",
  file      = "All Papers/Other/Regenbrecht and Langlotz 2015 - Mutual gaze support in videoconferencing reviewed.pdf",
  keywords  = "uist2022-gaze-design",
  doi       = "10.17705/1cais.03745",
  issn      = "1529-3181",
  language  = "en"
}

@ARTICLE{Regenbrecht2015-jh,
  title    = "{Mutual gaze support in videoconferencing reviewed}",
  author   = "Regenbrecht, Holger and Langlotz, Tobias",
  journal  = "Commun. Assoc. Inf. Syst.",
  volume   =  37,
  pages    =  45,
  abstract = "This review gives decision makers, researchers, and developers a
              tool to systematically apply and further develop videoconferencing
              systems in “serious” settings requiring mutual gaze, and gives
              recommendations for future developments. Videoconferencing allows
              geographically dispersed parties to communicate by simultaneous
              audio and video transmissions. It is used in a variety of
              application scenarios with a wide range of coordination needs and
              efforts, such as private chat, discussion meetings, and
              negotiation tasks. In particular, in scenarios requiring certain
              levels of trust and judgement non-verbal communication, cues are
              highly important for effective communication. Mutual gaze support
              plays a central role in those high coordination need scenarios but
              generally lacks adequate technical support from videoconferencing
              systems. In this paper, we review technical concepts and
              implementations for mutual gaze support in videoconferencing,
              classify them, evaluate them according to a defined set of
              criteria, and give recommendations for future developments. Our
              review gives decision makers, researchers, and developers a tool
              to systematically apply and further develop videoconferencing
              systems in “serious” settings requiring mutual gaze. This should
              lead to well-informed decisions regarding the use and development
              of this technology and to a more widespread exploitation of the
              benefits of videoconferencing in general. For example, if
              videoconferencing systems supported high-quality mutual gaze in an
              easy-to-set-up and easy-to-use way, we could hold more effective
              and efficient recruitment interviews, court hearings, or contract
              negotiations.",
  year     =  2015,
  url      = "http://dx.doi.org/10.17705/1cais.03745",
  file     = "All Papers/My Library/Regenbrecht and Langlotz 2015 - Mutual gaze support in videoconferencing reviewed.pdf",
  doi      = "10.17705/1cais.03745",
  issn     = "1529-3181",
  language = "en"
}

@ARTICLE{2005-id,
  title   = "{視線と存在の擬似アウェアネス機能を有する共有仮想空間コミュニケーションシステム}",
  author  = "俊光, 宮島 and 崇, 下地 and 欣也, 藤田",
  journal = "日本バーチャルリアリティ学会論文誌",
  volume  =  10,
  number  =  1,
  pages   = "71–80",
  year    =  2005,
  url     = "http://dx.doi.org/10.18974/tvrsj.10.1_71",
  file    = "All Papers/My Library/俊光 et al. 2005 - 視線と存在の擬似アウェアネス機能を有する共有仮想空間コミュニケーションシステム.pdf",
  doi     = "10.18974/tvrsj.10.1\_71"
}

@ARTICLE{2005-wx,
  title   = "{視線と存在の擬似アウェアネス機能を有する共有仮想空間コミュニケーションシステム}",
  author  = "俊光, 宮島 and 崇, 下地 and 欣也, 藤田",
  journal = "日本バーチャルリアリティ学会論文誌",
  volume  =  10,
  number  =  1,
  pages   = "71--80",
  year    =  2005,
  url     = "http://dx.doi.org/10.18974/tvrsj.10.1_71",
  file    = "All Papers/Other/俊光 et al. 2005 - 視線と存在の擬似アウェアネス機能を有する共有仮想空間コミュニケーションシステム.pdf",
  doi     = "10.18974/tvrsj.10.1\_71"
}

@ARTICLE{2005-jg,
  title   = "{発話を促す話し合いの場に関する研究 : アイコンタクトに着目して}",
  author  = "康郎, 青山 and 凱惟, 戸北",
  journal = "日本教科教育学会誌",
  volume  =  28,
  number  =  1,
  pages   = "41–50",
  year    =  2005,
  url     = "http://dx.doi.org/10.18993/jcrdajp.28.1_41",
  file    = "All Papers/My Library/康郎 and 凱惟 2005 - 発話を促す話し合いの場に関する研究 - アイコンタクトに着目して.pdf",
  doi     = "10.18993/jcrdajp.28.1\_41"
}

@ARTICLE{2005-xw,
  title    = "{発話を促す話し合いの場に関する研究 : アイコンタクトに着目して}",
  author   = "康郎, 青山 and 凱惟, 戸北",
  journal  = "日本教科教育学会誌",
  volume   =  28,
  number   =  1,
  pages    = "41--50",
  year     =  2005,
  url      = "http://dx.doi.org/10.18993/jcrdajp.28.1_41",
  file     = "All Papers/Other/康郎 and 凱惟 2005 - 発話を促す話し合いの場に関する研究 - アイコンタクトに着目して.pdf",
  keywords = "eye contact;telepresence",
  doi      = "10.18993/jcrdajp.28.1\_41"
}

@ARTICLE{Li2021-ew,
  title    = "{BayesGaze: A Bayesian Approach to Eye-Gaze Based Target
              Selection}",
  author   = "Li, Zhi and Zhao, Maozheng and Wang, Yifan and Rashidian, Sina and
              Baig, Furqan and Liu, Rui and Liu, Wanyu and Beaudouin-Lafon,
              Michel and Ellison, Brooke and Wang, Fusheng and Ramakrishnan, I V
              and Bi, Xiaojun",
  pages    = "10 pages, 832",
  abstract = "Selecting targets accurately and quickly with eye-gaze input
              remains an open research question. In this paper, we introduce
              BayesGaze, a Bayesian approach of determining the selected target
              given an eye-gaze trajectory. This approach views each sampling
              point in an eye-gaze trajectory as a signal for selecting a
              target. It then uses the Bayes' theorem to calculate the posterior
              probability of selecting a target given a sampling point, and
              accumulates the posterior probabilities weighted by sampling
              interval to determine the selected target. The selection results
              are fed back to update the prior distribution of targets, which is
              modeled by a categorical distribution. Our investigation shows
              that BayesGaze improves target selection accuracy and speed over a
              dwell-based selection method, and the Center of Gravity Mapping
              (CM) method. Our research shows that both accumulating posterior
              and incorporating the prior are effective in improving the
              performance of eye-gaze based target selection.",
  year     =  2021,
  url      = "https://graphicsinterface.org/proceedings/gi2021/gi2021-35/",
  doi      = "10.20380/GI2021.35",
  issn     = "0713-5424",
  language = "en"
}

@ARTICLE{Misawa2013-ou,
  title     = "{{LiveMask}: A telepresence surrogate system with a face-shaped
               screen for supporting nonverbal communication}",
  author    = "Misawa, Kana and Ishiguro, Yoshio and Rekimoto, Jun",
  journal   = "J. Inf. Process.",
  publisher = "Information Processing Society of Japan",
  volume    =  21,
  number    =  2,
  pages     = "295--303",
  year      =  2013,
  url       = "http://dx.doi.org/10.2197/ipsjjip.21.295",
  file      = "All Papers/Other/Misawa et al. 2013 - LiveMask - A telepresence surrogate system with a face-shaped screen for supporting nonverbal communication.pdf",
  keywords  = "eye contact;telepresence;prj-gaze-design;uist2022-gaze-design",
  doi       = "10.2197/ipsjjip.21.295",
  issn      = "1882-6652",
  language  = "en"
}

@ARTICLE{Misawa2013-ta,
  title    = "{LiveMask: A telepresence surrogate system with a face-shaped
              screen for supporting nonverbal communication}",
  author   = "Misawa, Kana and Ishiguro, Yoshio and Rekimoto, Jun",
  journal  = "J. Inf. Process.",
  volume   =  21,
  number   =  2,
  pages    = "295–303",
  year     =  2013,
  url      = "http://dx.doi.org/10.2197/ipsjjip.21.295",
  file     = "All Papers/My Library/Misawa et al. 2013 - LiveMask - A telepresence surrogate system with a face-shaped screen for supporting nonverbal communication.pdf",
  doi      = "10.2197/ipsjjip.21.295",
  issn     = "1882-6652",
  language = "en"
}

@MISC{Lee2017-hr,
  title     = "{Improving collaboration in augmented video conference using
               mutually shared gaze}",
  author    = "Lee, Gun A and Kim, Seungwon and Lee, Youngho and Dey, Arindam
               and Piumsomboon, Thammathip and Norman, Mitchell and
               Billinghurst, Mark",
  publisher = "The Eurographics Association",
  abstract  = "To improve remote collaboration in video conferencing systems,
               researchers have been investigating augmenting visual cues onto a
               shared live video stream. In such systems, a person wearing a
               head-mounted display (HMD) and camera can share her view of the
               surrounding real-world with a remote collaborator to receive
               assistance on a real-world task. While this concept of augmented
               video conferencing (AVC) has been actively investigated, there
               has been little research on how sharing gaze cues might affect
               the collaboration in video conferencing. This paper investigates
               how sharing gaze in both directions between a local worker and
               remote helper in an AVC system affects the collaboration and
               communication. Using a prototype AVC system that shares the eye
               gaze of both users, we conducted a user study that compares four
               conditions with different combinations of eye gaze sharing
               between the two users. The results showed that sharing each
               other's gaze significantly improved collaboration and
               communication.",
  year      =  2017,
  url       = "http://dx.doi.org/10.2312/egve.20171359",
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  doi       = "10.2312/egve.20171359"
}

@MISC{Lee2017-jk,
  title     = "{Improving collaboration in augmented video conference using
               mutually shared gaze}",
  author    = "Lee, Gun A and Kim, Seungwon and Lee, Youngho and Dey, Arindam
               and Piumsomboon, Thammathip and Norman, Mitchell and
               Billinghurst, Mark",
  publisher = "The Eurographics Association",
  abstract  = "To improve remote collaboration in video conferencing systems,
               researchers have been investigating augmenting visual cues onto a
               shared live video stream. In such systems, a person wearing a
               head-mounted display (HMD) and camera can share her view of the
               surrounding real-world with a remote collaborator to receive
               assistance on a real-world task. While this concept of augmented
               video conferencing (AVC) has been actively investigated, there
               has been little research on how sharing gaze cues might affect
               the collaboration in video conferencing. This paper investigates
               how sharing gaze in both directions between a local worker and
               remote helper in an AVC system affects the collaboration and
               communication. Using a prototype AVC system that shares the eye
               gaze of both users, we conducted a user study that compares four
               conditions with different combinations of eye gaze sharing
               between the two users. The results showed that sharing each
               other's gaze significantly improved collaboration and
               communication.",
  year      =  2017,
  url       = "http://dx.doi.org/10.2312/egve.20171359",
  doi       = "10.2312/egve.20171359"
}

@INPROCEEDINGS{Liang2016-mt,
  title     = "{A review of multimodal interaction}",
  author    = "Liang, Rongjian and Liang, Bin and Wang, Xueqian and Zhang, Tao
               and Li, Gang and Wang, Kang",
  booktitle = "{Advances in computer science research}",
  publisher = "Atlantis Press",
  year      =  2016,
  url       = "https://doi.org/10.2991%2Femcs-16.2016.173",
  file      = "All Papers/My Library/Liang et al. 2016 - A review of multimodal interaction.pdf",
  doi       = "10.2991/emcs-16.2016.173"
}

@ARTICLE{Borgestig2017-qy,
  title     = "{Gaze-based assistive technology in daily activities in children
               with severe physical impairments--An intervention study}",
  author    = "Borgestig, Maria and Sandqvist, Jan and Ahlsten, Gunnar and
               Falkmer, Torbjörn and Hemmingsson, Helena",
  journal   = "Developmental neurorehabilitation",
  publisher = "Taylor \& Francis",
  volume    =  20,
  number    =  3,
  pages     = "129--141",
  abstract  = "ABSTRACTObjective: To establish the impact of a gaze-based
               assistive technology (AT) intervention on activity repertoire,
               autonomous use, and goal attainment in children with severe
               physical impairments, and to examine parents? satisfaction with
               the gaze-based AT and with services related to the gaze-based AT
               intervention. Methods: Non-experimental multiple case study with
               before, after, and follow-up design. Ten children with severe
               physical impairments without speaking ability (aged 1?15 years)
               participated in gaze-based AT intervention for 9?10 months,
               during which period the gaze-based AT was implemented in daily
               activities. Results: Repertoire of computer activities increased
               for seven children. All children had sustained usage of
               gaze-based AT in daily activities at follow-up, all had attained
               goals, and parents? satisfaction with the AT and with services
               was high. Discussion: The gaze-based AT intervention was
               effective in guiding parents and teachers to continue supporting
               the children to perform activities with the AT after the
               intervention program.",
  month     =  apr,
  year      =  2017,
  url       = "http://dx.doi.org/10.3109/17518423.2015.1132281",
  file      = "All Papers/Other/Borgestig et al. 2017 - Gaze-based assistive technology in daily activities in children with severe physical impairments-An intervention study.pdf",
  keywords  = "prj-gaze-shorthand",
  doi       = "10.3109/17518423.2015.1132281",
  issn      = "1751-8423"
}

@ARTICLE{Borgestig2017-jt,
  title    = "{Gaze-based assistive technology in daily activities in children
              with severe physical impairments-An intervention study}",
  author   = "Borgestig, Maria and Sandqvist, Jan and Ahlsten, Gunnar and
              Falkmer, Torbjörn and Hemmingsson, Helena",
  journal  = "Developmental neurorehabilitation",
  volume   =  20,
  number   =  3,
  pages    = "129–141",
  abstract = "OBJECTIVE: To establish the impact of a gaze-based assistive
              technology (AT) intervention on activity repertoire, autonomous
              use, and goal attainment in children with severe physical
              impairments, and to examine parents' satisfaction with the
              gaze-based AT and with services related to the gaze-based AT
              intervention. METHODS: Non-experimental multiple case study with
              before, after, and follow-up design. Ten children with severe
              physical impairments without speaking ability (aged 1-15 years)
              participated in gaze-based AT intervention for 9-10 months, during
              which period the gaze-based AT was implemented in daily
              activities. RESULTS: Repertoire of computer activities increased
              for seven children. All children had sustained usage of gaze-based
              AT in daily activities at follow-up, all had attained goals, and
              parents' satisfaction with the AT and with services was high.
              DISCUSSION: The gaze-based AT intervention was effective in
              guiding parents and teachers to continue supporting the children
              to perform activities with the AT after the intervention program.",
  month    =  apr,
  year     =  2017,
  url      = "http://dx.doi.org/10.3109/17518423.2015.1132281",
  file     = "All Papers/My Library/Borgestig et al. 2017 - Gaze-based assistive technology in daily activities in children with severe physical impairments-An intervention study.pdf",
  doi      = "10.3109/17518423.2015.1132281",
  issn     = "1751-8423,1751-8431",
  language = "en"
}

@ARTICLE{Tanaka2012-yv,
  title    = "{Support for Acceptable Interaction Start using Ambient
              Information Presentation based on User Uninterruptibility
              Estimation}",
  author   = "Tanaka, Takahiro and Fujita, Kinya",
  journal  = "Journal of Software Maintenance and Evolution: Research and
              Practice",
  volume   =  24,
  number   =  5,
  pages    = "921--932",
  year     =  2012,
  url      = "https://www.jstage.jst.go.jp/article/jsoft/24/5/24_921/_article/-char/ja/",
  file     = "All Papers/My Library/Tanaka and Fujita 2012 - Support for Acceptable Interaction Start using ... Information Presentation based on User Uninterruptibility Estimation.pdf",
  doi      = "10.3156/jsoft.24.921",
  issn     = "1532-060X",
  language = "en"
}

@ARTICLE{2014-dw,
  title   = "{次世代ナチュラルユーザインタフェース『視線入力』}",
  author  = "健一, 蜂巣",
  journal = "映像情報メディア学会誌",
  volume  =  68,
  number  =  8,
  pages   = "636–641",
  year    =  2014,
  url     = "http://dx.doi.org/10.3169/itej.68.636",
  file    = "All Papers/My Library/健一 2014 - 次世代ナチュラルユーザインタフェース『視線入力』.pdf",
  doi     = "10.3169/itej.68.636"
}

@ARTICLE{Myodo2018-eq,
  title     = "{[invited paper] issues and solutions to informal communication
               in working from home using a telepresence robot}",
  author    = "Myodo, Emi and Xu, Jianfeng and Tasaka, Kazuyuki and Yanagihara,
               Hiromasa and Sakazawa, Shigeyuki",
  journal   = "ITE trans. media technol. appl.",
  publisher = "Institute of Image Information and Television Engineers",
  volume    =  6,
  number    =  1,
  pages     = "30--45",
  abstract  = "This paper surveys the systems of media space and telepresence
               robots concerning the advantages, problems, and solutions and
               introduces the experiment suggesting that a robot emulating the
               social greeting process actually promotes informal communication
               compared with no emulation. In a telework scenario, the lack of
               informal communications, such as impromptu meetings, is a
               hard-to-solve problem for those who work at home over the long
               term. To facilitate informal communication, a system called media
               space provided an always-on audiovisual connection between
               distributed workers. Moreover, recent studies suggested that
               telepresence robots featuring audiovisual capabilities like media
               space are effective for informal communication. However, in
               practical use, various problems exist. Therefore, in this paper,
               we survey the systems of media space and telepresence robots
               concerning the advantages, problems, and solutions. We also
               discuss the unresolved issues and required investigations. One of
               the unresolved issues is that a stationary or desktop robot has a
               difficulty in initiating informal communication because it cannot
               move around the office. To tackle the issue, we introduce our
               experiment suggesting that a robot emulating the social greeting
               process actually promotes informal communication compared with no
               emulation.",
  year      =  2018,
  url       = "http://dx.doi.org/10.3169/mta.6.30",
  file      = "All Papers/Other/Myodo et al. 2018 - [invited paper] issues and solutions to informal communication in working from home using a telepresence robot.pdf",
  doi       = "10.3169/mta.6.30",
  issn      = "2186-7364",
  language  = "en"
}

@ARTICLE{Myodo2018-ih,
  title    = "{[invited paper] issues and solutions to informal communication in
              working from home using a telepresence robot}",
  author   = "Myodo, Emi and Xu, Jianfeng and Tasaka, Kazuyuki and Yanagihara,
              Hiromasa and Sakazawa, Shigeyuki",
  journal  = "ITE trans. media technol. appl.",
  volume   =  6,
  number   =  1,
  pages    = "30–45",
  abstract = "This paper surveys the systems of media space and telepresence
              robots concerning the advantages, problems, and solutions and
              introduces the experiment suggesting that a robot emulating the
              social greeting process actually promotes informal communication
              compared with no emulation. In a telework scenario, the lack of
              informal communications, such as impromptu meetings, is a
              hard-to-solve problem for those who work at home over the long
              term. To facilitate informal communication, a system called media
              space provided an always-on audiovisual connection between
              distributed workers. Moreover, recent studies suggested that
              telepresence robots featuring audiovisual capabilities like media
              space are effective for informal communication. However, in
              practical use, various problems exist. Therefore, in this paper,
              we survey the systems of media space and telepresence robots
              concerning the advantages, problems, and solutions. We also
              discuss the unresolved issues and required investigations. One of
              the unresolved issues is that a stationary or desktop robot has a
              difficulty in initiating informal communication because it cannot
              move around the office. To tackle the issue, we introduce our
              experiment suggesting that a robot emulating the social greeting
              process actually promotes informal communication compared with no
              emulation.",
  year     =  2018,
  url      = "http://dx.doi.org/10.3169/mta.6.30",
  file     = "All Papers/My Library/Myodo et al. 2018 - [invited paper] issues and solutions to informal communication in working from home using a telepresence robot.pdf",
  doi      = "10.3169/mta.6.30",
  issn     = "2186-7364",
  language = "en"
}

@ARTICLE{Pluzyczka2018-yo,
  title    = "{The first hundred years: A history of eye tracking as a research
              method}",
  author   = "Płużyczka, Monika and Warszawski, Uniwersytet",
  journal  = "Applied Linguistics Papers",
  volume   = "4/2018",
  number   =  25,
  pages    = "101–116",
  abstract = "The paper describes the first hundred years of the history of eye
              tracking as a research method, dividing it into three phases of
              development. It starts by presenting the studies on tracing eye
              movements in reading in the end of the 19th century and the
              creation of the first eye trackers. The further part presents how
              the eye tracking technology was improved in the time of film
              recordings, ceasing to be invasive for the eyes. It also shows how
              in this time the main focus of research shifted to practical
              aspects due to the development of the behaviourist movement in
              experimental psychology. The third phase starts in the 1970s, when
              researchers turned more towards the dependence between the
              perception and mental processes. It was linked to the
              establishment of a theoretical and methodological basis for
              cognitive psychology.",
  month    =  "27~" # dec,
  year     =  2018,
  url      = "http://alp.uw.edu.pl/wp-content/uploads/sites/315/2019/01/ALP-25_4-9-Monika-P%C5%81U%C5%BBYCZKA.pdf",
  file     = "All Papers/My Library/Płużyczka and Warszawski 2018 - The first hundred years - A history of eye tracking as a research method.pdf",
  doi      = "10.32612/uw.25449354.2018.4.pp.101-116",
  issn     = "2544-9354"
}

@ARTICLE{Favre-Felix2019-mn,
  title    = "{Absolute eye gaze estimation with biosensors in hearing aids}",
  author   = "Favre-Félix, Antoine and Graversen, Carina and Bhuiyan, Tanveer A
              and Skoglund, Martin A and Rotger-Griful, Sergi and Rank, Mike
              Lind and Dau, Torsten and Lunner, Thomas",
  journal  = "Frontiers in neuroscience",
  volume   =  13,
  pages    =  1294,
  abstract = "People with hearing impairment typically have difficulties
              following conversations in multi-talker situations. Previous
              studies have shown that utilizing eye gaze to steer audio through
              beamformers could be a solution for those situations. Recent
              studies have shown that in-ear electrodes that capture
              electrooculography in the ear (EarEOG) can estimate the eye-gaze
              relative to the head, when the head was fixed. The head movement
              can be estimated using motion sensors around the ear to create an
              estimate of the absolute eye-gaze in the room. In this study, an
              experiment was designed to mimic a multi-talker situation in order
              to study and model the EarEOG signal when participants attempted
              to follow a conversation. Eleven hearing impaired participants
              were presented speech from the DAT speech corpus (Bo Nielsen et
              al., 2014), with three targets positioned at -30°, 0° and +30°
              azimuth. The experiment was run in two setups: one where the
              participants had their head fixed in a chinrest, and the other
              where they were free to move their head. The participants' task
              was to focus their visual attention on an LED-indicated target
              that changed regularly. A model was developed for the relative
              eye-gaze estimation, taking saccades, fixations, head movement and
              drift from the electrode-skin half-cell into account. This model
              explained 90.5\% of the variance of the EarEOG when the head was
              fixed, and 82.6\% when the head was free. The absolute eye-gaze
              was also estimated utilizing that model. When the head was fixed,
              the estimation of the absolute eye-gaze was reliable. However, due
              to hardware issues, the estimation of the absolute eye-gaze when
              the head was free had a variance that was too large to reliably
              estimate the attended target. Overall, this study demonstrated the
              potential of estimating absolute eye-gaze using EarEOG and motion
              sensors around the ear.",
  month    =  "12~" # may,
  year     =  2019,
  url      = "http://dx.doi.org/10.3389/fnins.2019.01294",
  file     = "All Papers/My Library/Favre-Félix et al. 2019 - Absolute eye gaze estimation with biosensors in hearing aids.pdf",
  doi      = "10.3389/fnins.2019.01294",
  issn     = "1662-4548,1662-453X",
  language = "en"
}

@ARTICLE{Skoglund2022-uu,
  title    = "{Comparing in-ear EOG for eye-movement estimation with
              eye-tracking: Accuracy, calibration, and speech comprehension}",
  author   = "Skoglund, Martin A and Andersen, Martin and Shiell, Martha M and
              Keidser, Gitte and Rank, Mike Lind and Rotger-Griful, Sergi",
  journal  = "Frontiers in neuroscience",
  volume   =  16,
  pages    =  873201,
  abstract = "This presentation details and evaluates a method for estimating
              the attended speaker during a two-person conversation by means of
              in-ear electro-oculography (EOG). Twenty-five hearing-impaired
              participants were fitted with molds equipped with EOG electrodes
              (in-ear EOG) and wore eye-tracking glasses while watching a video
              of two life-size people in a dialog solving a Diapix task. The
              dialogue was directionally presented and together with background
              noise in the frontal hemisphere at 60 dB SPL. During three
              conditions of steering (none, in-ear EOG, conventional
              eye-tracking), participants' comprehension was periodically
              measured using multiple-choice questions. Based on eye movement
              detection by in-ear EOG or conventional eye-tracking, the
              estimated attended speaker was amplified by 6 dB. In the in-ear
              EOG condition, the estimate was based on one selected channel pair
              of electrodes out of 36 possible electrodes. A novel calibration
              procedure introducing three different metrics was used to select
              the measurement channel. The in-ear EOG attended speaker estimates
              were compared to those of the eye-tracker. Across participants,
              the mean accuracy of in-ear EOG estimation of the attended speaker
              was 68\%, ranging from 50 to 89\%. Based on offline simulation, it
              was established that higher scoring metrics obtained for a channel
              with the calibration procedure were significantly associated with
              better data quality. Results showed a statistically significant
              improvement in comprehension of about 10\% in both steering
              conditions relative to the no-steering condition. Comprehension in
              the two steering conditions was not significantly different.
              Further, better comprehension obtained under the in-ear EOG
              condition was significantly correlated with more accurate
              estimation of the attended speaker. In conclusion, this study
              shows promising results in the use of in-ear EOG for visual
              attention estimation with potential for applicability in hearing
              assistive devices.",
  month    =  "30~" # jun,
  year     =  2022,
  url      = "http://dx.doi.org/10.3389/fnins.2022.873201",
  file     = "All Papers/My Library/Skoglund et al. 2022 - Comparing in-ear EOG for eye-movement estimation with eye-tracking - Accuracy, calibration, and speech comprehension.pdf",
  doi      = "10.3389/fnins.2022.873201",
  issn     = "1662-4548,1662-453X",
  language = "en"
}

@ARTICLE{Rosskamp2021-as,
  title    = "{{UnrealHaptics}: Plugins for Advanced {VR} Interactions in Modern
              Game Engines}",
  author   = "Rosskamp, Janis and Meißenhelter, Hermann and Weller, Rene and
              Rüdel, Marc O and Ganser, Johannes and Zachmann, Gabriel",
  journal  = "Frontiers in Virtual Reality",
  volume   =  2,
  abstract = "We present UnrealHaptics, a plugin-architecture that enables
              advanced virtual reality (VR) interactions, such as haptics or
              grasping in modern game engines. The core is a combination of a
              state-of-the-art collision detection library with support for very
              fast and stable force and torque computations and a general device
              plugin for communication with different input/output hardware
              devices, such as haptic devices or Cybergloves. Our modular and
              lightweight architecture makes it easy for other researchers to
              adapt our plugins to their requirements. We prove the versatility
              of our plugin architecture by providing two use cases implemented
              in the Unreal Engine 4 (UE4). In the first use case, we have
              tested our plugin with a haptic device in different test scenes.
              For the second use case, we show a virtual hand grasping an object
              with precise collision detection and handling multiple contacts.
              We have evaluated the performance in our use cases. The results
              show that our plugin easily meets the requirements of stable force
              rendering at 1 kHz for haptic rendering even in highly non-convex
              scenes, and it can handle the complex contact scenarios of virtual
              grasping.",
  year     =  2021,
  url      = "http://dx.doi.org/10.3389/frvir.2021.640470",
  file     = "All Papers/Other/Rosskamp et al. 2021 - UnrealHaptics - Plugins for Advanced VR Interactions in Modern Game Engines.pdf",
  keywords = "haptics",
  doi      = "10.3389/frvir.2021.640470",
  issn     = "2673-4192"
}

@ARTICLE{Rosskamp2021-qf,
  title    = "{UnrealHaptics: Plugins for advanced VR interactions in modern
              game engines}",
  author   = "Rosskamp, Janis and Meißenhelter, Hermann and Weller, Rene and
              Rüdel, Marc O and Ganser, Johannes and Zachmann, Gabriel",
  journal  = "Frontiers in Virtual Reality",
  volume   =  2,
  abstract = "We present UnrealHaptics, a plugin-architecture that enables
              advanced virtual reality (VR) interactions, such as haptics or
              grasping in modern game engines. The core is a combination of a
              state-of-the-art collision detection library with support for very
              fast and stable force and torque computations and a general device
              plugin for communication with different input/output hardware
              devices, such as haptic devices or Cybergloves. Our modular and
              lightweight architecture makes it easy for other researchers to
              adapt our plugins to their requirements. We prove the versatility
              of our plugin architecture by providing two use cases implemented
              in the Unreal Engine 4 (UE4). In the first use case, we have
              tested our plugin with a haptic device in different test scenes.
              For the second use case, we show a virtual hand grasping an object
              with precise collision detection and handling multiple contacts.
              We have evaluated the performance in our use cases. The results
              show that our plugin easily meets the requirements of stable force
              rendering at 1 kHz for haptic rendering even in highly non-convex
              scenes, and it can handle the complex contact scenarios of virtual
              grasping.",
  year     =  2021,
  url      = "http://dx.doi.org/10.3389/frvir.2021.640470",
  file     = "All Papers/My Library/Rosskamp et al. 2021 - UnrealHaptics - Plugins for advanced VR interactions in modern game engines.pdf",
  doi      = "10.3389/frvir.2021.640470",
  issn     = "2673-4192"
}

@ARTICLE{Hayek2019-dd,
  title    = "{3D point clouds and eye tracking for investigating the perception
              and acceptance of power lines in different landscapes}",
  author   = "Hayek, Ulrike Wissen and Müller, Kilian and Göbel, Fabian and
              Kiefer, Peter and Grêt-Regamey, Adrienne",
  journal  = "Multimodal Technologies and Interaction",
  volume   =  3,
  number   =  2,
  pages    =  40,
  abstract = "The perception of the visual landscape impact is a significant
              factor explaining the public's acceptance of energy infrastructure
              developments. Yet, there is lack of knowledge how people perceive
              and accept power lines in certain landscape types and in
              combination with wind turbines, a required setting to achieve
              goals of the energy turnaround. The goal of this work was to
              demonstrate how 3D point cloud visualizations could be used for an
              eye tracking study to systematically investigate the perception of
              landscape scenarios with power lines. 3D visualizations of
              near-natural and urban landscapes were prepared based on data from
              airborne and terrestrial laser scanning. These scenes were altered
              with varying amounts of the respective infrastructure, and they
              provided the stimuli in a laboratory experiment with 49
              participants. Eye tracking and questionnaires served for measuring
              the participants' responses. The results show that the point
              cloud-based simulations offered suitable stimuli for the eye
              tracking study. Particularly for the analysis of guided
              perceptions, the approach fostered an understanding of disturbing
              landscape elements. A comparative in situ eye tracking study is
              recommended to further evaluate the quality of the point cloud
              simulations, whether they produce similar responses as in the real
              world.",
  month    =  "6~" # apr,
  year     =  2019,
  url      = "https://www.researchgate.net/publication/333619737_3D_Point_Clouds_and_Eye_Tracking_for_Investigating_the_Perception_and_Acceptance_of_Power_Lines_in_Different_Landscapes",
  file     = "All Papers/My Library/Hayek et al. 2019 - 3D point clouds and eye tracking for investigating the perception and acceptance of power lines in different landscapes.pdf",
  doi      = "10.3390/mti3020040",
  issn     = "2414-4088"
}

@ARTICLE{Hyona2019-of,
  title    = "{Eye behavior during multiple object tracking and multiple
              identity tracking}",
  author   = "Hyönä, Jukka and Li, Jie and Oksama, Lauri",
  journal  = "Vision (Basel, Switzerland)",
  volume   =  3,
  number   =  3,
  abstract = "We review all published eye-tracking studies to date that have
              used eye movements to examine multiple object (MOT) or multiple
              identity tracking (MIT). In both tasks, observers dynamically
              track multiple moving objects. In MOT the objects are identical,
              whereas in MIT they have distinct identities. In MOT, observers
              prefer to fixate on blank space, which is often the center of
              gravity formed by the moving targets (centroid). In contrast, in
              MIT observers have a strong preference for the target-switching
              strategy, presumably to refresh and maintain identity-location
              bindings for the targets. To account for the qualitative
              differences between MOT and MIT, two mechanisms have been posited,
              a position tracking (MOT) and an identity tracking (MOT \& MIT)
              mechanism. Eye-tracking studies of MOT have also demonstrated that
              observers execute rescue saccades toward targets in danger of
              becoming occluded or are about to change direction after a
              collision. Crowding attracts the eyes close to it in order to
              increase visual acuity for the crowded objects to prevent target
              loss. It is suggested that future studies should concentrate more
              on MIT, as MIT more closely resembles tracking in the real world.",
  month    =  "31~" # jul,
  year     =  2019,
  url      = "http://dx.doi.org/10.3390/vision3030037",
  file     = "All Papers/My Library/Hyönä et al. 2019 - Eye behavior during multiple object tracking and multiple identity tracking.pdf",
  doi      = "10.3390/vision3030037",
  issn     = "2411-5150",
  language = "en"
}

@ARTICLE{Wheeler1979-mo,
  title     = "{Eye contact and the perception of intelligence}",
  author    = "Wheeler, R Wade and Baron, Joan C and Michell, Susan and
               Ginsburg, Harvey J",
  journal   = "Bulletin of the Psychonomic Society",
  publisher = "Springer Science and Business Media LLC",
  volume    =  13,
  number    =  2,
  pages     = "101--102",
  month     =  feb,
  year      =  1979,
  url       = "http://dx.doi.org/10.3758/bf03335025",
  file      = "All Papers/Other/Wheeler et al. 1979 - Eye contact and the perception of intelligence.pdf",
  keywords  = "eye contact",
  doi       = "10.3758/bf03335025",
  issn      = "0090-5054,2197-9979",
  language  = "en"
}

@ARTICLE{Wheeler1979-dp,
  title    = "{Eye contact and the perception of intelligence}",
  author   = "Wheeler, R Wade and Baron, Joan C and Michell, Susan and Ginsburg,
              Harvey J",
  journal  = "Bulletin of the Psychonomic Society",
  volume   =  13,
  number   =  2,
  pages    = "101–102",
  month    =  feb,
  year     =  1979,
  url      = "http://dx.doi.org/10.3758/bf03335025",
  file     = "All Papers/My Library/Wheeler et al. 1979 - Eye contact and the perception of intelligence.pdf",
  doi      = "10.3758/bf03335025",
  issn     = "0090-5054,2197-9979",
  language = "en"
}

@ARTICLE{Valtakari2021-su,
  title     = "{Eye tracking in human interaction: Possibilities and
               limitations}",
  author    = "Valtakari, Niilo V and Hooge, Ignace T C and Viktorsson,
               Charlotte and Nyström, Pär and Falck-Ytter, Terje and Hessels,
               Roy S",
  journal   = "Behavior research methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  53,
  number    =  4,
  pages     = "1592--1608",
  abstract  = "There is a long history of interest in looking behavior during
               human interaction. With the advance of (wearable) video-based eye
               trackers, it has become possible to measure gaze during many
               different interactions. We outline the different types of
               eye-tracking setups that currently exist to investigate gaze
               during interaction. The setups differ mainly with regard to the
               nature of the eye-tracking signal (head- or world-centered) and
               the freedom of movement allowed for the participants. These
               features place constraints on the research questions that can be
               answered about human interaction. We end with a decision tree to
               help researchers judge the appropriateness of specific setups.",
  month     =  aug,
  year      =  2021,
  url       = "https://consensus.apphttps://consensus.app/papers/tracking-interaction-possibilities-limitations-valtakari/7d45d82e432653aba931d0945190709f/?extracted-answer=Eye+tracking+in+human+interactions+can+measure+gaze+during+various+situations%2C+but+setups+and+freedom+of+movement+play+crucial+roles+in+the+research+questions+that+can+be+answered.&q=research+that+eye+tracking+for+estimate+eye+contact+occurance+on+videoconferencing&copilot=on",
  file      = "All Papers/Other/Valtakari et al. 2021 - Eye tracking in human interaction - Possibilities and limitations.pdf",
  keywords  = "Data analysis; Data quality; Eye tracking; Human interaction;
               Wearable",
  doi       = "10.3758/s13428-020-01517-x",
  pmc       = "PMC7787418",
  pmid      =  33409984,
  issn      = "1554-351X,1554-3528",
  language  = "en"
}

@ARTICLE{Wedel2017-aa,
  title    = "{A review of eye-tracking research in marketing}",
  author   = "{Wedel} and {Pieters}",
  journal  = "Review of marketing research",
  abstract = "… importance of visual marketing in practice, we review eye -
              tracking research to evaluate its … of the application of eye -
              tracking to ad pretesting. We review eye - tracking applications
              in …",
  year     =  2017,
  url      = "https://www.taylorfrancis.com/chapters/edit/10.4324/9781351550932-5/review-eye-tracking-research-marketing-michel-wedel-rik-pieters",
  doi      = "10.4324/9781351550932-5/review-eye-tracking-research-marketing-michel-wedel-rik-pieters",
  issn     = "1548-6435,1944-7035"
}

@ARTICLE{Ge2021-hq,
  title    = "{Improving Intention Detection in Single-Trial Classification
              through Fusion of EEG and Eye-tracker Data}",
  author   = "Ge, Xianliang and Pan, Yunxian and Wang, Sujie and Qian, Linze and
              Yuan, Jingjia and Xu, Jie and Thakor, Nitish and Sun, Yu",
  abstract = "Intention decoding is an indispensable procedure in hands-free
              human-computer interaction (HCI). Conventional eye-tracking system
              using single-model fixation duration possibly issues commands
              ignoring users' real expectation. In the current study, an
              eye-brain hybrid brain-computer interface (BCI) interaction system
              was introduced for intention detection through fusion of
              multi-modal eye-track and ERP (a measurement derived from EEG)
              features. Eye-track and EEG data were recorded from 64 healthy
              participants as they performed a 40-min customized free search
              task of a fixed target icon among 25 icons. The corresponding
              fixation duration of eye-tracking and ERP were extracted. Five
              previously-validated LDA-based classifiers (including RLDA, SWLDA,
              BLDA, SKLDA, and STDA) and the widely-used CNN method were adopted
              to verify the efficacy of feature fusion from both offline and
              pseudo-online analysis, and optimal approach was evaluated through
              modulating the training set and system response duration. Our
              study demonstrated that the input of multi-modal eye-track and ERP
              features achieved superior performance of intention detection in
              the single trial classification of active search task. And
              compared with single-model ERP feature, this new strategy also
              induced congruent accuracy across different classifiers. Moreover,
              in comparison with other classification methods, we found that the
              SKLDA exhibited the superior performance when fusing feature in
              offline test (ACC=0.8783, AUC=0.9004) and online simulation with
              different sample amount and duration length. In sum, the current
              study revealed a novel and effective approach for intention
              classification using eye-brain hybrid BCI, and further supported
              the real-life application of hands-free HCI in a more precise and
              stable manner.",
  month    =  "5~" # dec,
  year     =  2021,
  url      = "http://arxiv.org/abs/2112.02566",
  file     = "All Papers/My Library/Ge et al. 2021 - Improving Intention Detection in Single-Trial Classification through Fusion of EEG and Eye-tracker Data.pdf",
  doi      = "10.48550/arXiv.2112.02566"
}

@ARTICLE{Ververas2023-au,
  title    = "{Generalizing Gaze Estimation with Weak-Supervision from Synthetic
              Views}",
  author   = "Ververas, Evangelos and Gkagkos, Polydefkis and Deng, Jiankang and
              Doukas, Michail Christos and Guo, Jia and Zafeiriou, Stefanos",
  abstract = "Developing gaze estimation models that generalize well to unseen
              domains and in-the-wild conditions remains a challenge with no
              known best solution. This is mostly due to the difficulty of
              acquiring ground truth data that cover the distribution of
              possible faces, head poses and environmental conditions that exist
              in the real world. In this work, we propose to train general gaze
              estimation models based on 3D geometry-aware gaze
              pseudo-annotations which we extract from arbitrary unlabelled face
              images, which are abundantly available in the internet.
              Additionally, we leverage the observation that head, body and hand
              pose estimation benefit from revising them as dense 3D coordinate
              prediction, and similarly express gaze estimation as regression of
              dense 3D eye meshes. We overcome the absence of compatible ground
              truth by fitting rigid 3D eyeballs on existing gaze datasets and
              design a multi-view supervision framework to balance the effect of
              pseudo-labels during training. We test our method in the task of
              gaze generalization, in which we demonstrate improvement of up to
              $30\%$ compared to state-of-the-art when no ground truth data are
              available, and up to $10\%$ when they are. The project material
              will become available for research purposes.",
  month    =  "28~" # mar,
  year     =  2023,
  url      = "http://arxiv.org/abs/2212.02997",
  file     = "All Papers/My Library/Ververas et al. 2023 - Generalizing Gaze Estimation with Weak-Supervision from Synthetic Views.pdf",
  doi      = "10.48550/arXiv.2212.02997",
  annote   = "Comment: 13 pages, 12 figures"
}

@ARTICLE{Belardinelli2023-vx,
  title    = "{Gaze-based intention estimation: principles, methodologies, and
              applications in {HRI}}",
  author   = "Belardinelli, Anna",
  abstract = "Intention prediction has become a relevant field of research in
              Human-Machine and Human-Robot Interaction. Indeed, any artificial
              system (co)-operating with and along humans, designed to assist
              and coordinate its actions with a human partner, would benefit
              from first inferring the human's current intention. To spare the
              user the cognitive burden of explicitly uttering their goals, this
              inference relies mostly on behavioral cues deemed indicative of
              the current action. It has been long known that eye movements are
              highly anticipatory of the single steps unfolding during a task,
              hence they can serve as a very early and reliable behavioural cue
              for intention recognition. This review aims to draw a line between
              insights in the psychological literature on visuomotor control and
              relevant applications of gaze-based intention recognition in
              technical domains, with a focus on teleoperated and assistive
              robotic systems. Starting from the cognitive principles underlying
              the relationship between intentions, eye movements, and action,
              the use of eye tracking and gaze-based models for intent
              recognition in Human-Robot Interaction is considered, with
              prevalent methodologies and their diverse applications. Finally,
              special consideration is given to relevant human factors issues
              and current limitations to be factored in when designing such
              systems.",
  month    =  "9~" # feb,
  year     =  2023,
  url      = "http://arxiv.org/abs/2302.04530",
  file     = "All Papers/My Library/Belardinelli 2023 - Gaze-based intention estimation - principles, methodologies, and applications in HRI.pdf",
  doi      = "10.48550/arXiv.2302.04530",
  annote   = "Comment: submitted to ACM Transactions on Human-Robot Interaction"
}

@ARTICLE{Wang2023-xy,
  title    = "{Voyager: An Open-Ended Embodied Agent with Large Language Models}",
  author   = "Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay
              and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar,
              Anima",
  abstract = "We introduce Voyager, the first LLM-powered embodied lifelong
              learning agent in Minecraft that continuously explores the world,
              acquires diverse skills, and makes novel discoveries without human
              intervention. Voyager consists of three key components: 1) an
              automatic curriculum that maximizes exploration, 2) an
              ever-growing skill library of executable code for storing and
              retrieving complex behaviors, and 3) a new iterative prompting
              mechanism that incorporates environment feedback, execution
              errors, and self-verification for program improvement. Voyager
              interacts with GPT-4 via blackbox queries, which bypasses the need
              for model parameter fine-tuning. The skills developed by Voyager
              are temporally extended, interpretable, and compositional, which
              compounds the agent's abilities rapidly and alleviates
              catastrophic forgetting. Empirically, Voyager shows strong
              in-context lifelong learning capability and exhibits exceptional
              proficiency in playing Minecraft. It obtains 3.3x more unique
              items, travels 2.3x longer distances, and unlocks key tech tree
              milestones up to 15.3x faster than prior SOTA. Voyager is able to
              utilize the learned skill library in a new Minecraft world to
              solve novel tasks from scratch, while other techniques struggle to
              generalize. We open-source our full codebase and prompts at
              https://voyager.minedojo.org/.",
  month    =  "25~" # may,
  year     =  2023,
  url      = "http://arxiv.org/abs/2305.16291",
  file     = "All Papers/My Library/Wang et al. 2023 - Voyager - An Open-Ended Embodied Agent with Large Language Models.pdf",
  doi      = "10.48550/arXiv.2305.16291",
  annote   = "<p>Comment: Project website and open-source codebase:
              https://voyager.minedojo.org/</p>"
}

@ARTICLE{Zeng2023-ln,
  title    = "{GestureGPT: Zero-shot Interactive Gesture Understanding and
              Grounding with Large Language Model Agents}",
  author   = "Zeng, Xin and Wang, Xiaoyu and Zhang, Tengxiang and Yu, Chun and
              Zhao, Shengdong and Chen, Yiqiang",
  abstract = "Current gesture recognition systems primarily focus on identifying
              gestures within a predefined set, leaving a gap in connecting
              these gestures to interactive GUI elements or system functions
              (e.g., linking a 'thumb-up' gesture to a 'like' button). We
              introduce GestureGPT, a novel zero-shot gesture understanding and
              grounding framework leveraging large language models (LLMs).
              Gesture descriptions are formulated based on hand landmark
              coordinates from gesture videos and fed into our dual-agent
              dialogue system. A gesture agent deciphers these descriptions and
              queries about the interaction context (e.g., interface, history,
              gaze data), which a context agent organizes and provides.
              Following iterative exchanges, the gesture agent discerns user
              intent, grounding it to an interactive function. We validated the
              gesture description module using public first-view and third-view
              gesture datasets and tested the whole system in two real-world
              settings: video streaming and smart home IoT control. The highest
              zero-shot Top-5 grounding accuracies are 80.11\% for video
              streaming and 90.78\% for smart home tasks, showing potential of
              the new gesture understanding paradigm.",
  month    =  "29~" # oct,
  year     =  2023,
  url      = "http://arxiv.org/abs/2310.12821",
  file     = "All Papers/My Library/Zeng et al. 2023 - GestureGPT - Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents.pdf",
  doi      = "10.48550/arXiv.2310.12821"
}

@ARTICLE{Liu2023-oi,
  title    = "{SUQL: Conversational Search over Structured and Unstructured Data
              with Large Language Models}",
  author   = "Liu, Shicheng and Xu, Jialiang and Tjangnaka, Wesley and Semnani,
              Sina J and Yu, Chen Jie and Dávid, Gui and Lam, Monica S",
  abstract = "Many knowledge sources consist of both structured information such
              as relational databases as well as unstructured free text.
              Building a conversational interface to such data sources is
              challenging. This paper introduces SUQL, Structured and
              Unstructured Query Language, the first formal executable
              representation that naturally covers compositions of structured
              and unstructured data queries. Specifically, it augments SQL with
              several free-text primitives to form a precise, succinct, and
              expressive representation. This paper also presents a
              conversational search agent based on large language models,
              including a few-shot contextual semantic parser for SUQL. To
              validate our approach, we introduce a dataset consisting of
              crowdsourced questions and conversations about real restaurants.
              Over 51\% of the questions in the dataset require both structured
              and unstructured data, suggesting that it is a common phenomenon.
              We show that our few-shot conversational agent based on SUQL finds
              an entity satisfying all user requirements 89.3\% of the time,
              compared to just 65.0\% for a strong and commonly used baseline.",
  month    =  "16~" # nov,
  year     =  2023,
  url      = "http://arxiv.org/abs/2311.09818",
  file     = "All Papers/My Library/Liu et al. 2023 - SUQL - Conversational Search over Structured and Unstructured Data with Large Language Models.pdf",
  doi      = "10.48550/arXiv.2311.09818"
}

@ARTICLE{Fu2024-ns,
  title    = "{Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost
              Whole-Body Teleoperation}",
  author   = "Fu, Zipeng and Zhao, Tony Z and Finn, Chelsea",
  abstract = "Imitation learning from human demonstrations has shown impressive
              performance in robotics. However, most results focus on table-top
              manipulation, lacking the mobility and dexterity necessary for
              generally useful tasks. In this work, we develop a system for
              imitating mobile manipulation tasks that are bimanual and require
              whole-body control. We first present Mobile ALOHA, a low-cost and
              whole-body teleoperation system for data collection. It augments
              the ALOHA system with a mobile base, and a whole-body
              teleoperation interface. Using data collected with Mobile ALOHA,
              we then perform supervised behavior cloning and find that
              co-training with existing static ALOHA datasets boosts performance
              on mobile manipulation tasks. With 50 demonstrations for each
              task, co-training can increase success rates by up to 90\%,
              allowing Mobile ALOHA to autonomously complete complex mobile
              manipulation tasks such as sauteing and serving a piece of shrimp,
              opening a two-door wall cabinet to store heavy cooking pots,
              calling and entering an elevator, and lightly rinsing a used pan
              using a kitchen faucet. Project website:
              https://mobile-aloha.github.io",
  month    =  "4~" # jan,
  year     =  2024,
  url      = "http://arxiv.org/abs/2401.02117",
  file     = "All Papers/My Library/Fu et al. 2024 - Mobile ALOHA - Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation.pdf",
  doi      = "10.48550/arXiv.2401.02117",
  annote   = "Comment: Project website: https://mobile-aloha.github.io (Zipeng
              Fu and Tony Z. Zhao are project co-leads, Chelsea Finn is the
              advisor)"
}

@ARTICLE{Konrad2024-kz,
  title    = "{GazeGPT: Augmenting Human Capabilities using Gaze-contingent
              Contextual AI for Smart Eyewear}",
  author   = "Konrad, Robert and Padmanaban, Nitish and Buckmaster, J Gabriel
              and Boyle, Kevin C and Wetzstein, Gordon",
  abstract = "Multimodal large language models (LMMs) excel in world knowledge
              and problem-solving abilities. Through the use of a world-facing
              camera and contextual AI, emerging smart accessories aim to
              provide a seamless interface between humans and LMMs. Yet, these
              wearable computing systems lack an understanding of the user's
              attention. We introduce GazeGPT as a new user interaction paradigm
              for contextual AI. GazeGPT uses eye tracking to help the LMM
              understand which object in the world-facing camera view a user is
              paying attention to. Using extensive user evaluations, we show
              that this gaze-contingent mechanism is a faster and more accurate
              pointing mechanism than alternatives; that it augments human
              capabilities by significantly improving their accuracy in a
              dog-breed classification task; and that it is consistently ranked
              as more natural than head- or body-driven selection mechanisms for
              contextual AI. Moreover, we prototype a variety of application
              scenarios that suggest GazeGPT could be of significant value to
              users as part of future AI-driven personal assistants.",
  month    =  "31~" # jan,
  year     =  2024,
  url      = "http://arxiv.org/abs/2401.17217",
  file     = "All Papers/My Library/Konrad et al. 2024 - GazeGPT - Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear.pdf",
  doi      = "10.48550/arXiv.2401.17217",
  annote   = "Comment: Project video: https://youtu.be/AuDFHHTK\_m8"
}

@ARTICLE{Jin2024-us,
  title    = "{Position Paper: What Can Large Language Models Tell Us about Time
              Series Analysis}",
  author   = "Jin, Ming and Zhang, Yifan and Chen, Wei and Zhang, Kexin and
              Liang, Yuxuan and Yang, Bin and Wang, Jindong and Pan, Shirui and
              Wen, Qingsong",
  abstract = "Time series analysis is essential for comprehending the
              complexities inherent in various real-world systems and
              applications. Although large language models (LLMs) have recently
              made significant strides, the development of artificial general
              intelligence (AGI) equipped with time series analysis capabilities
              remains in its nascent phase. Most existing time series models
              heavily rely on domain knowledge and extensive model tuning,
              predominantly focusing on prediction tasks. In this paper, we
              argue that current LLMs have the potential to revolutionize time
              series analysis, thereby promoting efficient decision-making and
              advancing towards a more universal form of time series analytical
              intelligence. Such advancement could unlock a wide range of
              possibilities, including modality switching and time series
              question answering. We encourage researchers and practitioners to
              recognize the potential of LLMs in advancing time series analysis
              and emphasize the need for trust in these related efforts.
              Furthermore, we detail the seamless integration of time series
              analysis with existing LLM technologies and outline promising
              avenues for future research.",
  month    =  "4~" # feb,
  year     =  2024,
  url      = "http://arxiv.org/abs/2402.02713",
  file     = "All Papers/My Library/Jin et al. 2024 - Position Paper - What Can Large Language Models Tell Us about Time Series Analysis.pdf",
  doi      = "10.48550/arXiv.2402.02713",
  annote   = "Comment: 16 pages, 8 figures, 1 table"
}

@ARTICLE{1994-bl,
  title   = "{テレビ通信システムにおける会話特質について (1)}",
  author  = "尚幹, 植松 and 正和, 林 and 昭浩, 岩崎 and 明哲, 小松原",
  journal = "人間工学",
  volume  =  30,
  number  = "Supplement",
  pages   = "132–133",
  year    =  1994,
  url     = "http://dx.doi.org/10.5100/jje.30.Supplement_132",
  file    = "All Papers/My Library/尚幹 et al. 1994 - テレビ通信システムにおける会話特質について (1).pdf",
  doi     = "10.5100/jje.30.Supplement\_132"
}

@ARTICLE{1994-dq,
  title    = "{テレビ通信システムにおける会話特質について (1)}",
  author   = "尚幹, 植松 and 正和, 林 and 昭浩, 岩崎 and 明哲, 小松原",
  journal  = "人間工学",
  volume   =  30,
  number   = "Supplement",
  pages    = "132--133",
  year     =  1994,
  url      = "http://dx.doi.org/10.5100/jje.30.Supplement_132",
  file     = "All Papers/Other/尚幹 et al. 1994 - テレビ通信システムにおける会話特質について (1).pdf",
  keywords = "eye contact;telepresence",
  doi      = "10.5100/jje.30.Supplement\_132"
}

@INPROCEEDINGS{Nakamura2020-ju,
  title     = "{A Japanese bimanual flick keyboard for tablets that improves
               display space efficiency}",
  author    = "Nakamura, Yuya and Hosobe, Hiroshi",
  booktitle = "{Proceedings of the 15th international joint conference on
               computer vision, imaging and computer graphics theory and
               applications}",
  publisher = "SCITEPRESS - Science and Technology Publications",
  year      =  2020,
  url       = "http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0008969101700177",
  doi       = "10.5220/0008969101700177",
  isbn      =  9789897584022
}

@ARTICLE{Lu_undated-or,
  title    = "{Glanceable AR: Evaluating Information Access Methods for
              Head-Worn Augmented Reality}",
  author   = "Lu, Feiyu and Tech, Virginia and States, United and Davari,
              Shakiba and Tech, Virginia and States, United and Lisle, Lee and
              Tech, Virginia and States, United and Li, Yuan and Tech, Virginia
              and States, United and Bowman, Doug A and Tech, Virginia and
              States, United",
  file     = "All Papers/My Library/Lu et al. - Glanceable AR - Evaluating Information Access Methods for Head-Worn Augmented Reality.pdf",
  language = "en"
}

@ARTICLE{2007-we,
  title    = "{遠隔操作型コミュニケーションロボットとのインタラクションにおける印象評価}",
  author   = "史享, 山岡 and 崇行, 神田 and 浩, 石黒 and 紀博, 萩田",
  journal  = "情報処理学会論文誌",
  volume   =  48,
  number   =  11,
  pages    = "3577--3587",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  nov,
  year     =  2007,
  keywords = "論文;eye contact;telepresence",
  issn     = "1882-7764"
}

@ARTICLE{2007-ga,
  title    = "{遠隔操作型コミュニケーションロボットとのインタラクションにおける印象評価}",
  author   = "{史享} and {崇行} and {紀博}",
  journal  = "情報処理学会論文誌",
  volume   =  48,
  number   =  11,
  pages    = "3577–3587",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  nov,
  year     =  2007,
  issn     = "1882-7764"
}

@INPROCEEDINGS{Sutherland1965-mp,
  title    = "{The Ultimate Display}",
  author   = "Sutherland, I",
  abstract = "We live in a physical world whose properties we have come to know
              well through long familiarity. We sense an involvement with this
              physical world which gives us the ability to predict its
              properties well. For example, we can predict where objects will
              fall, how well-known shapes look from other angles, and how much
              force is required to push objects against friction. We lack
              corresponding familiarity with the forces on charged particles,
              forces in non-uniform fields, the effects of nonprojective
              geometric transformations, and high-inertia, low friction motion.
              A display connected to a digital computer gives us a chance to
              gain familiarity with concepts not realizable in the physical
              world. It is a looking glass into a mathematical wonderland.
              Computer displays today cover a variety of capabilities. Some have
              only the fundamental ability to plot dots. Displays being sold now
              generally have built in line-drawing capability. An ability to
              draw simple curves would be useful. Some available displays are
              able to plot very short line segments in arbitrary directions, to
              form characters or more complex curves. Each of these abilities
              has a history and a known utility.",
  year     =  1965,
  url      = "https://www.semanticscholar.org/paper/The-Ultimate-Display-Sutherland/dce55f83dd425c68e5d1c1714dd0c8bbb43e54d9",
  file     = "All Papers/My Library/Sutherland 1965 - The Ultimate Display.pdf",
  annote   = "[TLDR] The authors live in a physical world whose properties they
              have come to know well through long familiarity but lack
              corresponding familiarity with the forces on charged particles,
              forces in non-uniform fields, the effects of nonprojective
              geometric transformations, and high-inertia, low friction motion."
}

@ARTICLE{1995-uu,
  title     = "{{複数人同士の視線一致を可能とするテレビ会議システム(MPEC})}",
  author    = "ディシルバ, リヤナゲ and 田原, 光穂 and 相澤, 清晴 and 羽鳥, 光俊",
  journal   = "電子情報通信学会技術研究報告. IE, 画像工学",
  publisher = "一般社団法人電子情報通信学会",
  volume    =  95,
  number    =  181,
  pages     = "15--22",
  abstract  = "
               臨場感を伴う高度な画像通信会議システムの実現には、会議出席者間の視線一致が不可欠であると考えられる。このため、近年、液晶ディスプレイやプロジェクタ等の利用による、さまざまな視線一致の実現手法が提案されている。しかし、各局に複数の会議出席者が存在する場合に、同時に各出席者間の視線の一致を実現するシステムについては検討されていない。本稿では、同時に各出席者間の視線一致を可能とするMPEC(Multiple
               Person Eye
               Contact)テレビ会議システムを提案しその試作を説明する。視線一致の実現のためには、ディスプレイに映された相手の顔の後ろにカメラを配置し、ディスプレイ上の相手の位置から見える画像を取得し、相手に伝送する必要がある。本方式ではハーフミラーとカメラの物理的な配置の工夫により、各局に複数の会議出席者が存在する場合のシステムを実現している。
               In this paper a novel method of providing eye contact for
               multiple participants in a teleconferencing system is presented.
               This method proposes a simple idea to fulfill the requirement for
               eye contact, for two or more local and remote participants
               simultaneously, by using half mirrors and cameras placed at the
               common points of the extended lines of gaze of each participant.
               In contrast to the commonly available one to one eye contact
               video phone systems, which also use half mirrors, the proposed
               system has the advantage of serving more than one person at a
               given location, hence named Multiple Person Eye Contact (MPEC)
               teleconferencing system. We observed the resultant images by
               using a preliminary experiment and also building a prototype
               system. The results from both of these experiments proved the
               merits of the proposed idea. One of the main features of this
               system is that, it gives any participant, a feeling of ``being
               looked at'' if any remote participant is actually gazing at him.
               On the other hand if none of the remote participants are looking
               at him, then he gets the feeling of ``not being looked at'',
               which is also extremely important for effective inter personal
               communication.",
  month     =  jul,
  year      =  1995,
  keywords  = "テレビ会議;視線一致;MPEC(複数人同士の視線一致);ハーフミラーデスプレイ;eye contact;telepresence"
}

@ARTICLE{1995-pw,
  title    = "{複数人同士の視線一致を可能とするテレビ会議システム(MPEC)}",
  author   = "{ディシルバ} and {田原} and {相澤} and {羽鳥}",
  journal  = "電子情報通信学会技術研究報告. IE, 画像工学",
  volume   =  95,
  number   =  181,
  pages    = "15–22",
  abstract = "
              臨場感を伴う高度な画像通信会議システムの実現には、会議出席者間の視線一致が不可欠であると考えられる。このため、近年、液晶ディスプレイやプロジェクタ等の利用による、さまざまな視線一致の実現手法が提案されている。しかし、各局に複数の会議出席者が存在する場合に、同時に各出席者間の視線の一致を実現するシステムについては検討されていない。本稿では、同時に各出席者間の視線一致を可能とするMPEC(Multiple
              Person Eye
              Contact)テレビ会議システムを提案しその試作を説明する。視線一致の実現のためには、ディスプレイに映された相手の顔の後ろにカメラを配置し、ディスプレイ上の相手の位置から見える画像を取得し、相手に伝送する必要がある。本方式ではハーフミラーとカメラの物理的な配置の工夫により、各局に複数の会議出席者が存在する場合のシステムを実現している。
              In this paper a novel method of providing eye contact for multiple
              participants in a teleconferencing system is presented. This
              method proposes a simple idea to fulfill the requirement for eye
              contact, for two or more local and remote participants
              simultaneously, by using half mirrors and cameras placed at the
              common points of the extended lines of gaze of each participant.
              In contrast to the commonly available one to one eye contact video
              phone systems, which also use half mirrors, the proposed system
              has the advantage of serving more than one person at a given
              location, hence named Multiple Person Eye Contact (MPEC)
              teleconferencing system. We observed the resultant images by using
              a preliminary experiment and also building a prototype system. The
              results from both of these experiments proved the merits of the
              proposed idea. One of the main features of this system is that, it
              gives any participant, a feeling of “being looked at” if any
              remote participant is actually gazing at him. On the other hand if
              none of the remote participants are looking at him, then he gets
              the feeling of “not being looked at”, which is also extremely
              important for effective inter personal communication.",
  month    =  jul,
  year     =  1995
}

@ARTICLE{Zhang2021-gc,
  title    = "{VirtualCube: An immersive 3D video communication system}",
  author   = "Zhang, Yizhong and Yang, Jiaolong and Liu, Zhen and Wang, Ruicheng
              and Chen, Guojun and Tong, Xin and Guo, Baining",
  journal  = "arXiv [cs.CV]",
  abstract = "The VirtualCube system is a 3D video conference system that
              attempts to overcome some limitations of conventional
              technologies. The key ingredient is VirtualCube, an abstract
              representation of a real-world cubicle instrumented with RGBD
              cameras for capturing the 3D geometry and texture of a user. We
              design VirtualCube so that the task of data capturing is
              standardized and significantly simplified, and everything can be
              built using off-the-shelf hardware. We use VirtualCubes as the
              basic building blocks of a virtual conferencing environment, and
              we provide each VirtualCube user with a surrounding display
              showing life-size videos of remote participants. To achieve
              real-time rendering of remote participants, we develop the V-Cube
              View algorithm, which uses multi-view stereo for more accurate
              depth estimation and Lumi-Net rendering for better rendering
              quality. The VirtualCube system correctly preserves the mutual eye
              gaze between participants, allowing them to establish eye contact
              and be aware of who is visually paying attention to them. The
              system also allows a participant to have side discussions with
              remote participants as if they were in the same room. Finally, the
              system sheds lights on how to support the shared space of work
              items (e.g., documents and applications) and track the visual
              attention of participants to work items.",
  month    =  dec,
  year     =  2021,
  url      = "http://arxiv.org/abs/2112.06730",
  file     = "All Papers/My Library/Zhang et al. 2021 - VirtualCube - An immersive 3D video communication system.pdf"
}

@ARTICLE{1995-ux,
  title     = "{{複数人の視線一致を可能にするテレビ会議システム(MPEC})}",
  author    = "田原, 光穂 and ディシルバ., リヤナゲ and 相澤, 清晴 and 羽鳥, 光俊",
  journal   = "電子情報通信学会総合大会講演論文集",
  publisher = "一般社団法人電子情報通信学会",
  volume    =  1995,
  pages     =  285,
  abstract  = "臨場感を伴う高度な画像通信会議システムの実現には,会議出席者間の視線一致が不可欠な要素であると考えられる。視線が一致しているか否かは、相手の意図を感じるための重要な要素であり、円滑なコミュニケーションに必修である。現在、視線一致の実現方法としては液晶ディスプレイやプロジェクタ等の利用による様々な方法が提案されているが、その多くは1対1に留まっており、一ケ所に複数人が参加する時、出席者全員の視線一致が同時に支援できないという問題があった。本稿では複数人の視線一致のための手法を提案し、プロトタイプによる実験例を示す。本手法では、ハーフミラーとカメラの物理的な配置を工夫することで、実際の会議と同様な視線一致を実現でき、他者の視線や顔の向きにより会話相手を特定したり、相手の意図を知る手掛かりとすることができる。",
  month     =  mar,
  year      =  1995,
  keywords  = "eye contact;telepresence"
}

@ARTICLE{1995-ya,
  title    = "{複数人の視線一致を可能にするテレビ会議システム(MPEC)}",
  author   = "{田原} and {ディシルバ} and {相澤} and {羽鳥}",
  journal  = "電子情報通信学会総合大会講演論文集",
  volume   =  1995,
  pages    =  285,
  abstract = "臨場感を伴う高度な画像通信会議システムの実現には,会議出席者間の視線一致が不可欠な要素であると考えられる。視線が一致しているか否かは、相手の意図を感じるための重要な要素であり、円滑なコミュニケーションに必修である。現在、視線一致の実現方法としては液晶ディスプレイやプロジェクタ等の利用による様々な方法が提案されているが、その多くは1対1に留まっており、一ケ所に複数人が参加する時、出席者全員の視線一致が同時に支援できないという問題があった。本稿では複数人の視線一致のための手法を提案し、プロトタイプによる実験例を示す。本手法では、ハーフミラーとカメラの物理的な配置を工夫することで、実際の会議と同様な視線一致を実現でき、他者の視線や顔の向きにより会話相手を特定したり、相手の意図を知る手掛かりとすることができる。",
  month    =  mar,
  year     =  1995
}

@MISC{Arakawa_undated-hd,
  title    = "{RGBDGaze: Gaze tracking on smartphones with RGB and depth data}",
  author   = "Arakawa, Riku and Goel, Mayank and Harrison, Chris and Ahuja,
              Karan",
  abstract = "Tracking a user's gaze on smartphones offers the potential for
              accessible and powerful multimodal interactions. However, phones
              are used in a myriad of contexts and state-of-the-art gaze models
              that use only the front-facing RGB cameras are too coarse and do
              not adapt adequately to changes in context. While prior research
              has showcased the efficacy of depth maps for gaze tracking, they
              have been limited to desktop-grade depth cameras, which are more
              capable than the types seen in smartphones, that must be thin and
              low-powered. In this paper, we present a gaze tracking system that
              makes use of today's smartphone depth camera technology to adapt
              to the changes in distance and orientation relative to the user's
              face. Unlike prior efforts that used depth sensors, we do not
              constrain the users to maintain a fixed head position. Our
              approach works across different use contexts in unconstrained
              mobile settings. The results show that our multimodal ML model has
              a mean gaze error of 1.89 cm; a 16.3\% improvement over using RGB
              data alone (2.26 cm error). Our system and dataset offer the first
              benchmark of gaze tracking on smartphones using RGB+Depth data
              under different use contexts. CCS CONCEPTS • Human-centered
              computing → Mobile devices; • Computing methodologies → Computer
              vision.",
  url      = "https://rikky0611.github.io/resource/paper/rgbdgaze_icmi2022_paper.pdf",
  file     = "All Papers/My Library/Arakawa et al. - RGBDGaze - Gaze tracking on smartphones with RGB and depth data.pdf"
}

@ARTICLE{Hessels2018-ss,
  title     = "{Eye contact takes two--autistic and social anxiety traits
               predict gaze behavior in dyadic interaction}",
  author    = "Hessels, R S and Holleman, G A and {others}",
  journal   = "Journal of",
  publisher = "journals.sagepub.com",
  abstract  = "… While two-way gaze is sometimes referred to as ` eye contact ',
               and no eye gaze as `averted gaze', we refrain from using these
               terms, as it may imply something special or intentional about
               these gaze states. For all three gaze states, the frequency of
               occurrences, the mean …",
  year      =  2018,
  file      = "All Papers/Other/Hessels et al. 2018 - Eye contact takes two-autistic and social anxiety traits predict gaze behavior in dyadic interaction.pdf",
  keywords  = "eye contact;telepresence"
}

@ARTICLE{Vickers_undated-to,
  title    = "{Eye-gaze Interaction Techniques for Use in Online Games and
              Environments for Users with Severe Physical Disabilities}",
  author   = "Vickers, Stephen",
  language = "en"
}

@ARTICLE{Follmer2013-ek,
  title    = "{inFORM: Dynamic Physical Affordances and Constraints through
              Shape and Object Actuation}",
  author   = "Follmer, Sean and Leithinger, Daniel and Olwal, Alex and Hogge,
              Akimitsu and Ishii, Hiroshi",
  journal  = "MIT web domain",
  abstract = "Past research on shape displays has primarily focused on rendering
              content and user interface elements through shape output, with
              less emphasis on dynamically changing UIs. We propose utilizing
              shape displays in three different ways to mediate interaction: to
              facilitate by providing dynamic physical affordances through shape
              change, to restrict by guiding users with dynamic physical
              constraints, and to manipulate by actuating physical objects. We
              outline potential interaction techniques and introduce Dynamic
              Physical Affordances and Constraints with our inFORM system, built
              on top of a state-of-the-art shape display, which provides for
              variable stiffness rendering and real-time user input through
              direct touch and tangible interaction. A set of motivating
              examples demonstrates how dynamic affordances, constraints and
              object actuation can create novel interaction possibilities.",
  month    =  oct,
  year     =  2013,
  url      = "https://dspace.mit.edu/handle/1721.1/92273",
  file     = "All Papers/My Library/Follmer et al. 2013 - inFORM - Dynamic Physical Affordances and Constraints through Shape and Object Actuation.pdf",
  language = "en\_US"
}

@ARTICLE{Liu2010-xx,
  title     = "{An {Eye-Gaze} Tracking and Human Computer Interface System for
               People with {ALS} and other Locked-in Diseases}",
  author    = "Liu, Shuo Samuel and Rawicz, Andrew and Ma, Teng and Zhang, Cheng
               and Lin, Kyle and Rezaei, Siavash and Wu, Eion",
  journal   = "CMBES Proc.",
  publisher = "proceedings.cmbes.ca",
  volume    =  33,
  abstract  = "Eye tracking is one of the most important ways for people with
               ALS and other locked-in diseases to communicate. The majority of
               current eye tracking computer input systems are too expensive and
               not very user-friendly. This paper proposes an infrared
               sensor/emitter based eye tracking system called EyeLive, which
               does not use the common camera based approach. The hardware, eye
               tracking algorithm, and user interface are described, discussed,
               and compared with camera based eye tracking systems. The
               performance of the system is presented with experimental data.
               The advantages of the EyeLive system such as low cost, user
               friendliness, portability, and eye strain reduction are also
               discussed.",
  month     =  jun,
  year      =  2010,
  keywords  = "prj-gaze-shorthand",
  issn      = "2371-9516,2371-9516",
  language  = "en"
}

@ARTICLE{Liu2010-dh,
  title    = "{An Eye-Gaze tracking and human computer interface system for
              people with ALS and other locked-in diseases}",
  author   = "{Liu} and {Rawicz} and {Ma} and {Zhang} and {Lin} and {Rezaei} and
              {Wu}",
  journal  = "CMBES Proc.",
  volume   =  33,
  abstract = "Eye tracking is one of the most important ways for people with ALS
              and other locked-in diseases to communicate. The majority of
              current eye tracking computer input systems are too expensive and
              not very user-friendly. This paper proposes an infrared
              sensor/emitter based eye tracking system called EyeLive, which
              does not use the common camera based approach. The hardware, eye
              tracking algorithm, and user interface are described, discussed,
              and compared with camera based eye tracking systems. The
              performance of the system is presented with experimental data. The
              advantages of the EyeLive system such as low cost, user
              friendliness, portability, and eye strain reduction are also
              discussed.",
  month    =  jun,
  year     =  2010,
  issn     = "2371-9516,2371-9516",
  language = "en"
}

@ARTICLE{Wu2021-me,
  title    = "{DOVE: Learning deformable 3D objects by watching videos}",
  author   = "Wu, Shangzhe and Jakab, Tomas and Rupprecht, Christian and
              Vedaldi, Andrea",
  journal  = "arXiv [cs.CV]",
  abstract = "Learning deformable 3D objects from 2D images is an extremely
              ill-posed problem. Existing methods rely on explicit supervision
              to establish multi-view correspondences, such as template shape
              models and keypoint annotations, which restricts their
              applicability on objects “in the wild”. In this paper, we propose
              to use monocular videos, which naturally provide correspondences
              across time, allowing us to learn 3D shapes of deformable object
              categories without explicit keypoints or template shapes.
              Specifically, we present DOVE, which learns to predict 3D
              canonical shape, deformation, viewpoint and texture from a single
              2D image of a bird, given a bird video collection as well as
              automatically obtained silhouettes and optical flows as training
              data. Our method reconstructs temporally consistent 3D shape and
              deformation, which allows us to animate and re-render the bird
              from arbitrary viewpoints from a single image.",
  month    =  jul,
  year     =  2021,
  url      = "http://arxiv.org/abs/2107.10844",
  file     = "All Papers/My Library/Wu et al. 2021 - DOVE - Learning deformable 3D objects by watching videos.pdf"
}

@INCOLLECTION{Kiyohiko2013-lk,
  title     = "{Eye-gaze input system suitable for use under natural light and
               its applications toward a support for {ALS} patients}",
  author    = "Kiyohiko, Abe and Shoichi, Ohi and Minoru, Ohyama",
  booktitle = "{Current Advances in Amyotrophic Lateral Sclerosis}",
  publisher = "IntechOpen",
  abstract  = "… Our research group has developed application programs for eye-
               gaze input to assist ALS … We believe that our new eye- gaze
               input system can ameliorate the QoL of ALS patients. …",
  year      =  2013,
  file      = "All Papers/Other/Kiyohiko et al. 2013 - Eye-gaze input system suitable for use under natural light and its applications toward a support for ALS patients.pdf",
  keywords  = "prj-gaze-shorthand"
}

@INCOLLECTION{Kiyohiko2013-al,
  title     = "{Eye-gaze input system suitable for use under natural light and
               its applications toward a support for ALS patients}",
  author    = "Kiyohiko, Abe and Shoichi, Ohi and Minoru, Ohyama",
  booktitle = "{Current advances in amyotrophic lateral sclerosis}",
  publisher = "InTech",
  abstract  = "Recently, eye-gaze input systems have been developed as novel
               human–machine interfaces [1-10]. Their operation requires only
               eye movements by the user. Based upon such systems, many
               communication aids have been developed for people with severe
               physical disabilities, such as amyotrophic lateral sclerosis
               (ALS). Eye-gaze input systems commonly employ noncontact eye-gaze
               detection for which an incandescent, fluorescent, or LED lamp can
               be used as the source of infrared or natural light. Detection
               based on infrared light can detect eye gaze with a high degree of
               accuracy [1-3] but requires an expensive device. Detection based
               on natural light uses ordinary devices and is therefore
               cost-effective [4,5]. However, an eye-gaze input system for
               natural light has a low degree of accuracy.",
  month     =  "9~" # nov,
  year      =  2013,
  url       = "http://www.intechopen.com/books/current-advances-in-amyotrophic-lateral-sclerosis/eye-gaze-input-system-suitable-for-use-under-natural-light-and-its-applications-toward-a-support-for",
  file      = "All Papers/My Library/Kiyohiko et al. 2013 - Eye-gaze input system suitable for use under natural light and its applications toward a support for ALS patients.pdf",
  isbn      =  9789535111955
}

@ARTICLE{1993-sl,
  title    = "{{TV会議のための多視線一致方式} －応用分野の展望－}",
  author   = "忠彦, 小松 and 新一, 志和",
  journal  = "情報処理学会研究報告グループウェアとネットワークサービス（GN）",
  volume   =  1993,
  number   = "95(1993-GN-004)",
  pages    = "77--84",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  oct,
  year     =  1993,
  keywords = "小松忠彦;eye contact;telepresence"
}

@ARTICLE{1993-vb,
  title    = "{TV会議のための多視線一致方式 －応用分野の展望－}",
  author   = "{忠彦} and {新一}",
  journal  = "情報処理学会研究報告グループウェアとネットワークサービス（GN）",
  volume   =  1993,
  number   = "95(1993-GN-004)",
  pages    = "77–84",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  oct,
  year     =  1993
}

@ARTICLE{2021-oj,
  title   = "{頷きのリアルタイムフィードバックによるビデオ会議支援手法の提案}",
  author  = "{徳原耕亮} and {荒川豊} and {石田繁巳} and {Others}",
  journal = "マルチメディア, 分散協調とモバイルシンポジウム 2021 論文集",
  volume  =  2021,
  number  =  1,
  pages   = "953--959",
  year    =  2021
}

@ARTICLE{undated-yp,
  title  = "{深 澤 遼＊}",
  author = "文郎, 岸 野",
  url    = "https://www.jstage.jst.go.jp/article/itetr/32.22/0/32.22_19/_pdf/-char/ja",
  file   = "All Papers/My Library/文郎 - 深 澤 遼＊.pdf"
}

@ARTICLE{2020-kt,
  title    = "{ビデオ会議のアウェアネス}",
  author   = "{幹文}",
  journal  = "マルチメディア，分散協調とモバイルシンポジウム2184論文集",
  volume   =  2020,
  pages    = "1141–1141",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jun,
  year     =  2020
}

@ARTICLE{2020-uv,
  title    = "{ビデオ会議のアウェアネス}",
  author   = "幹文, 敷田",
  journal  = "マルチメディア，分散協調とモバイルシンポジウム2184論文集",
  volume   =  2020,
  pages    = "1141--1141",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jun,
  year     =  2020,
  keywords = "招待講演"
}

@ARTICLE{2010-ef,
  title    = "{ロボット搭載カメラの移動がテレプレゼンスに与える影響}",
  author   = "{村上友樹} and {中西英之} and {野上大輔} and {石黒浩} and {Others}",
  journal  = "情報処理学会論文誌",
  volume   =  51,
  number   =  1,
  pages    = "54--62",
  year     =  2010,
  keywords = "eye contact;telepresence"
}

@ARTICLE{2010-gg,
  title   = "{ロボット搭載カメラの移動がテレプレゼンスに与える影響}",
  author  = "{村上友樹} and {中西英之} and {野上大輔} and {Others}",
  journal = "情報処理学会論文誌",
  volume  =  51,
  number  =  1,
  pages   = "54–62",
  year    =  2010
}

@ARTICLE{Description_undated-uz,
  title  = "{Determining the Tobii {I-VT} Fixation Filter's Default Values}",
  author = "Description, Method and Discussion, Results"
}

@ARTICLE{Description_undated-rd,
  title  = "{Determining the tobii I-VT fixation filter's default values}",
  author = "Description, Method and Discussion, Results"
}

@ARTICLE{Sharma2020-tx,
  title    = "{Digital twins: State of the art theory and practice, challenges,
              and open research questions}",
  author   = "Sharma, Angira and Kosasih, Edward and Zhang, Jie and Brintrup,
              Alexandra and Calinescu, Anisoara",
  journal  = "arXiv [cs.LG]",
  abstract = "Digital Twin was introduced over a decade ago, as an innovative
              all-encompassing tool, with perceived benefits including real-time
              monitoring, simulation and forecasting. However, the theoretical
              framework and practical implementations of digital twins (DT) are
              still far from this vision. Although successful implementations
              exist, sufficient implementation details are not publicly
              available, therefore it is difficult to assess their
              effectiveness, draw comparisons and jointly advance the DT
              methodology. This work explores the various DT features and
              current approaches, the shortcomings and reasons behind the delay
              in the implementation and adoption of digital twin. Advancements
              in machine learning, internet of things and big data have
              contributed hugely to the improvements in DT with regards to its
              real-time monitoring and forecasting properties. Despite this
              progress and individual company-based efforts, certain research
              gaps exist in the field, which have caused delay in the widespread
              adoption of this concept. We reviewed relevant works and
              identified that the major reasons for this delay are the lack of a
              universal reference framework, domain dependence, security
              concerns of shared data, reliance of digital twin on other
              technologies, and lack of quantitative metrics. We define the
              necessary components of a digital twin required for a universal
              reference framework, which also validate its uniqueness as a
              concept compared to similar concepts like simulation, autonomous
              systems, etc. This work further assesses the digital twin
              applications in different domains and the current state of machine
              learning and big data in it. It thus answers and identifies novel
              research questions, both of which will help to better understand
              and advance the theory and practice of digital twins.",
  month    =  nov,
  year     =  2020,
  url      = "http://arxiv.org/abs/2011.02833",
  file     = "All Papers/My Library/Sharma et al. 2020 - Digital twins - State of the art theory and practice, challenges, and open research questions.pdf"
}

@ARTICLE{2021-bk,
  title   = "{頷きのリアルタイムフィードバックによるビデオ会議支援手法の提案}",
  author  = "{徳原耕亮} and {石田繁巳} and {Others}",
  journal = "マルチメディア, 分散協調とモバイルシンポジウム 2021 論文集",
  volume  =  2021,
  number  =  1,
  pages   = "953–959",
  year    =  2021
}

@ARTICLE{2015-xr,
  title    = "{多地点マルチメディア会議システムにおける視線共有方法の検討}",
  author   = "雅文, 王 and 猛, 梅澤 and 範高, 大澤",
  journal  = "第77回全国大会講演論文集",
  volume   =  2015,
  number   =  1,
  pages    = "171--172",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  mar,
  year     =  2015,
  keywords = "インタフェース;eye contact;telepresence"
}

@ARTICLE{2002-vg,
  title    = "{ビデオ対話に正面顔が必要不可欠か}",
  author   = "{橋本亮一}",
  journal  = "日本バーチャルリアリティ学会第 7 回大会論文集",
  pages    = "283–286",
  abstract = "… 表示方法として、対話相手だけの場合( ビデオ 対話条件 V)、画面左側に自己像の鏡映像を追加 表示する場合（マ
              ルチウインドウ条件 M）、さらに背景を同一にする場合（ハ イパーミラー[7]条件 H）の 3 種類を用意した。実験者の 視線は、
              カメラ 目線 の場合と、画面上の被験者を見る目 線の場合の 2 …",
  year     =  2002
}

@ARTICLE{King2009-be,
  title     = "{Dlib-ml: A Machine Learning Toolkit}",
  author    = "King, Davis E",
  journal   = "Journal of machine learning research: JMLR",
  publisher = "JMLR.org",
  volume    =  10,
  pages     = "1755--1758",
  abstract  = "There are many excellent toolkits which provide support for
               developing machine learning software in Python, R, Matlab, and
               similar environments. Dlib-ml is an open source library, targeted
               at both engineers and research scientists, which aims to provide
               a similarly rich environment for developing machine learning
               software in the C++ language. Towards this end, dlib-ml contains
               an extensible linear algebra toolkit with built in BLAS support.
               It also houses implementations of algorithms for performing
               inference in Bayesian networks and kernel-based methods for
               classification, regression, clustering, anomaly detection, and
               feature ranking. To enable easy use of these tools, the entire
               library has been developed with contract programming, which
               provides complete and precise documentation as well as powerful
               debugging tools.",
  month     =  dec,
  year      =  2009,
  issn      = "1532-4435"
}

@ARTICLE{King2009-lp,
  title    = "{Dlib-ml: A machine learning toolkit}",
  author   = "{King}",
  journal  = "Journal of machine learning research: JMLR",
  volume   =  10,
  pages    = "1755–1758",
  abstract = "There are many excellent toolkits which provide support for
              developing machine learning software in Python, R, Matlab, and
              similar environments. Dlib-ml is an open source library, targeted
              at both engineers and research scientists, which aims to provide a
              similarly rich environment for developing machine learning
              software in the C++ language. Towards this end, dlib-ml contains
              an extensible linear algebra toolkit with built in BLAS support.
              It also houses implementations of algorithms for performing
              inference in Bayesian networks and kernel-based methods for
              classification, regression, clustering, anomaly detection, and
              feature ranking. To enable easy use of these tools, the entire
              library has been developed with contract programming, which
              provides complete and precise documentation as well as powerful
              debugging tools.",
  month    =  dec,
  year     =  2009,
  issn     = "1532-4435,1533-7928"
}

@ARTICLE{Sarcar2013-zw,
  title     = "{Eyeboard++ an enhanced eye gaze-based text entry system in
               Hindi}",
  author    = "Sarcar, S and Panwar, P",
  journal   = "Proceedings of the 11th Asia Pacific Conference on",
  publisher = "dl.acm.org",
  abstract  = "Of late, eye gaze has become an important modality of text entry
               in large and small display digital devices. Despite many tools
               being developed, issues like minimizing dwell time and …",
  year      =  2013,
  keywords  = "prj-gaze-shorthand"
}

@ARTICLE{Sarcar2013-ud,
  title    = "{Eyeboard++ an enhanced eye gaze-based text entry system in Hindi}",
  author   = "{Sarcar} and {Panwar}",
  journal  = "Proceedings of the 11th Asia Pacific Conference on",
  abstract = "Of late, eye gaze has become an important modality of text entry
              in large and small display digital devices. Despite many tools
              being developed, issues like minimizing dwell time and …",
  year     =  2013
}

@ARTICLE{He2019-br,
  title    = "{Exploring the Effectiveness of Face-to-face Mixed Reality for
              Teaching with Chalktalk}",
  author   = "He, Zhenyi and Perlin, K",
  journal  = "undefined",
  abstract = "A Mixed Reality (MR) platform is proposed that places Chalktalk's
              behaviors and simulations within a mirrored virtual world
              environment designed for face-to-face, one-on-one interactions and
              compares its relative effectiveness for learning, retention, and
              level of engagement. Teaching that uses projected presentation
              media such as slide-shows lacks support for dynamic content whose
              form and behaviors require live changes during a lecture. Recent
              software alternatives such as the Chalktalk software platform
              allow the creation of interactive simulations in arbitrary
              sequences and combinations within presentations. These more
              dynamic solutions, however, do not optimize for face-to-face
              interactions: eye-contact, gaze direction, and concurrent
              awareness of another person's movements together with the
              presented content. To explore the extent to which these
              face-to-face interactions may improve learning and engagement
              during a lecture, we propose a Mixed Reality (MR) platform that
              places Chalktalk's behaviors and simulations within a mirrored
              virtual world environment designed for face-to-face, one-on-one
              interactions. We compare our system with projected Chalktalk to
              evaluate its relative effectiveness for learning, retention, and
              level of engagement.",
  year     =  2019,
  language = "en"
}

@ARTICLE{He2019-cz,
  title    = "{Exploring the effectiveness of face-to-face mixed reality for
              teaching with chalktalk}",
  author   = "He, Zhenyi and Perlin, Ken",
  journal  = "arXiv [cs.HC]",
  abstract = "Teaching that uses projected presentation media such as
              slide-shows lacks support for dynamic content whose form and
              behaviors require live changes during a lecture. Recent software
              alternatives such as the Chalktalk software platform allow the
              creation of interactive simulations in arbitrary sequences and
              combinations within presentations. These more dynamic solutions,
              however, do not optimize for face-to-face interactions:
              eye-contact, gaze direction, and concurrent awareness of another
              person's movements together with the presented content. To explore
              the extent to which these face-to-face interactions may improve
              learning and engagement during a lecture, we propose a Mixed
              Reality (MR) platform that places Chalktalk's behaviors and
              simulations within a mirrored virtual world environment designed
              for face-to-face, one-on-one interactions. We compare our system
              with projected Chalktalk to evaluate its relative effectiveness
              for learning, retention, and level of engagement.",
  month    =  dec,
  year     =  2019,
  url      = "http://arxiv.org/abs/1912.03863",
  file     = "All Papers/My Library/He and Perlin 2019 - Exploring the effectiveness of face-to-face mixed reality for teaching with chalktalk.pdf"
}

@ARTICLE{He2019-qk,
  title    = "{Exploring the effectiveness of face-to-face mixed reality for
              teaching with chalktalk}",
  author   = "{He} and {Perlin}",
  journal  = "undefined",
  abstract = "A Mixed Reality (MR) platform is proposed that places Chalktalk's
              behaviors and simulations within a mirrored virtual world
              environment designed for face-to-face, one-on-one interactions and
              compares its relative effectiveness for learning, retention, and
              level of engagement. Teaching that uses projected presentation
              media such as slide-shows lacks support for dynamic content whose
              form and behaviors require live changes during a lecture. Recent
              software alternatives such as the Chalktalk software platform
              allow the creation of interactive simulations in arbitrary
              sequences and combinations within presentations. These more
              dynamic solutions, however, do not optimize for face-to-face
              interactions: eye-contact, gaze direction, and concurrent
              awareness of another person's movements together with the
              presented content. To explore the extent to which these
              face-to-face interactions may improve learning and engagement
              during a lecture, we propose a Mixed Reality (MR) platform that
              places Chalktalk's behaviors and simulations within a mirrored
              virtual world environment designed for face-to-face, one-on-one
              interactions. We compare our system with projected Chalktalk to
              evaluate its relative effectiveness for learning, retention, and
              level of engagement.",
  year     =  2019,
  language = "en"
}

@ARTICLE{Argyle1965-wx,
  title     = "{{EYE-CONTACT}, {DISTANCE} {AND} {AFFILIATION}}",
  author    = "Argyle, M and Dean, J",
  journal   = "Sociometry",
  publisher = "JSTOR",
  volume    =  28,
  pages     = "289--304",
  abstract  = "Previous evidence suggests that eye-contact serves a number of
               different functions in two- person encounters, of which one of
               the most important is gathering feed-back on the other person's
               reactions. It is further postulated that eye-contact is linked to
               affiliate motivation, and that approach and avoidance forces
               produce an equilibrium level of physical proximity, eye contact
               and other aspects of intimacy. If one of these is disturbed,
               compensatory changes may occur along the other dimensions.
               Experiments are reported which suggest that people …",
  month     =  sep,
  year      =  1965,
  url       = "https://www.ncbi.nlm.nih.gov/pubmed/14341239",
  keywords  = "INTERPERSONAL RELATIONS; PSYCHOLOGY, SOCIAL; VISION",
  pmid      =  14341239,
  issn      = "0038-0431",
  language  = "en"
}

@ARTICLE{Argyle1965-zh,
  title    = "{EYE-CONTACT, DISTANCE AND {AFFILIATION}}",
  author   = "Argyle, M and Dean, J",
  journal  = "Sociometry",
  volume   =  28,
  pages    = "289–304",
  abstract = "Previous evidence suggests that eye-contact serves a number of
              different functions in two- person encounters, of which one of the
              most important is gathering feed-back on the other person's
              reactions. It is further postulated that eye-contact is linked to
              affiliate motivation, and that approach and avoidance forces
              produce an equilibrium level of physical proximity, eye contact
              and other aspects of intimacy. If one of these is disturbed,
              compensatory changes may occur along the other dimensions.
              Experiments are reported which suggest that people …",
  month    =  sep,
  year     =  1965,
  url      = "https://www.ncbi.nlm.nih.gov/pubmed/14341239",
  pmid     =  14341239,
  issn     = "0038-0431",
  language = "en"
}

@INPROCEEDINGS{Vertegaal2000-fa,
  title     = "{Effects of gaze on multiparty mediated communication}",
  author    = "Vertegaal, Roel and Van der Veer, Gerrit and Vons, Harro",
  booktitle = "{Graphics interface}",
  publisher = "researchgate.net",
  pages     = "95--102",
  abstract  = "We evaluated effects of gaze direction and other nonY verbal
               visual cues on multiparty mediated communication. Groups of three
               participants (two actors, one subject) solved language puzzles in
               three audiovisual communication conditions. Each condition
               presented a different selection of images of the actors to
               subjects:(1) frontal motion video;(2) motion video with gaze
               directional cues;(3) still images with gaze directional cues.
               Results show that subjects used twice as many deictic references
               to persons when head orientation cues …",
  year      =  2000
}

@INPROCEEDINGS{Vertegaal2000-rr,
  title     = "{Effects of gaze on multiparty mediated communication}",
  author    = "Vertegaal, Roel and Van der Veer, Gerrit and Vons, Harro",
  booktitle = "{Graphics interface}",
  publisher = "researchgate.net",
  pages     = "95–102",
  abstract  = "We evaluated effects of gaze direction and other nonY verbal
               visual cues on multiparty mediated communication. Groups of three
               participants (two actors, one subject) solved language puzzles in
               three audiovisual communication conditions. Each condition
               presented a different selection of images of the actors to
               subjects:(1) frontal motion video;(2) motion video with gaze
               directional cues;(3) still images with gaze directional cues.
               Results show that subjects used twice as many deictic references
               to persons when head orientation cues …",
  year      =  2000
}

@ARTICLE{Papoutsaki2016-gr,
  title    = "{{WebGazer}: Scalable Webcam Eye Tracking Using User Interactions}",
  author   = "Papoutsaki, Alexandra and Sangkloy, Patsorn and Laskey, James and
              Daskalova, N and Huang, Jeff and Hays, James",
  journal  = "IJCAI: proceedings of the conference / sponsored by the
              International Joint Conferences on Artificial Intelligence",
  abstract = "The findings show that WebGazer can learn from user interactions
              and that its accuracy is sufficient for approximating the user's
              gaze. We introduce WebGazer, an online eye tracker that uses
              common webcams already present in laptops and mobile devices to
              infer the eye-gaze locations of web visitors on a page in real
              time. The eye tracking model self-calibrates by watching web
              visitors interact with the web page and trains a mapping between
              features of the eye and positions on the screen. This approach
              aims to provide a natural experience to everyday users that is not
              restricted to laboratories and highly controlled user studies.
              WebGazer has two key components: a pupil detector that can be
              combined with any eye detection library, and a gaze estimator
              using regression analysis informed by user interactions. We
              perform a large remote online study and a small in-person study to
              evaluate WebGazer. The findings show that WebGazer can learn from
              user interactions and that its accuracy is sufficient for
              approximating the user's gaze. As part of this paper, we release
              the first eye tracking library that can be easily integrated in
              any website for real-time gaze interactions, usability studies, or
              web research.",
  year     =  2016,
  keywords = "uist2022-gaze-design",
  issn     = "1045-0823",
  language = "en"
}

@ARTICLE{Papoutsaki2016-jt,
  title    = "{WebGazer: Scalable webcam eye tracking using user interactions}",
  author   = "{Papoutsaki} and {Sangkloy} and {Laskey} and {Daskalova} and
              {Huang} and {Hays}",
  journal  = "IJCAI: proceedings of the conference / sponsored by the
              International Joint Conferences on Artificial Intelligence",
  abstract = "The findings show that WebGazer can learn from user interactions
              and that its accuracy is sufficient for approximating the user's
              gaze. We introduce WebGazer, an online eye tracker that uses
              common webcams already present in laptops and mobile devices to
              infer the eye-gaze locations of web visitors on a page in real
              time. The eye tracking model self-calibrates by watching web
              visitors interact with the web page and trains a mapping between
              features of the eye and positions on the screen. This approach
              aims to provide a natural experience to everyday users that is not
              restricted to laboratories and highly controlled user studies.
              WebGazer has two key components: a pupil detector that can be
              combined with any eye detection library, and a gaze estimator
              using regression analysis informed by user interactions. We
              perform a large remote online study and a small in-person study to
              evaluate WebGazer. The findings show that WebGazer can learn from
              user interactions and that its accuracy is sufficient for
              approximating the user's gaze. As part of this paper, we release
              the first eye tracking library that can be easily integrated in
              any website for real-time gaze interactions, usability studies, or
              web research.",
  year     =  2016,
  issn     = "1045-0823",
  language = "en"
}

@ARTICLE{Vasilchenko2020-kv,
  title     = "{Collaborative learning \& co-creation in {XR}}",
  author    = "Vasilchenko, A and Li, J and Ryskeldiev, B and Sarcar, S and
               {others}",
  journal   = "Extended Abstracts of",
  publisher = "dl.acm.org",
  abstract  = "In this SIG, we aim at gathering researchers and practitioners to
               reflect on using XR technologies to support collaborative
               learning and co-creation, and to foster a joint force by …",
  year      =  2020,
  keywords  = "spatial computing"
}

@ARTICLE{Vasilchenko2020-mr,
  title    = "{Collaborative learning \& co-creation in {XR}}",
  author   = "{Vasilchenko} and {Li} and {Ryskeldiev} and {Sarcar} and {others}",
  journal  = "Extended abstracts on Human factors in computing systems . CHI
              Conference",
  abstract = "In this SIG, we aim at gathering researchers and practitioners to
              reflect on using XR technologies to support collaborative learning
              and co-creation, and to foster a joint force by …",
  year     =  2020,
  file     = "All Papers/My Library/Vasilchenko et al. 2020 - Collaborative learning & co-creation in XR.pdf"
}

@ARTICLE{Regenbrecht2012-pi,
  title    = "{Implementing {Eye-to-Eye} Contact in {Life-Sized}
              Videoconferencing}",
  author   = "Regenbrecht, H and Müller, L and Hoermann, S and Langlotz, T",
  abstract = "Videoconferencing systems available for end users do not allow for
              eye-to-eye contact between participants, also known as a lack of
              mutual gaze. The different locations of video camera and video
              display used for video conferencing makes it impossible to
              directly look into each other's eyes. This, combined with a lack
              of a lifesized video image of the communication partner makes a
              videoconferencing session an artificial experience leading to a
              decreased communication quality, empathy and trust. First, we
              present a survey on possible solutions to implement life-sized
              eye-to-eye contact and discuss briefly their pros and cons. The
              discussion of the characteristics and limitations of each concept
              can be used as a guideline for designing videoconferencing
              systems. Second, we present our own 1:1 scale videoconferencing
              solution, which builds upon advantages of other approaches while
              minimizing their disadvantages. Third and final, we report on the
              experiences made with our system in empirical evaluations. Author",
  year     =  2012,
  language = "en"
}

@ARTICLE{Regenbrecht2012-ts,
  title    = "{Implementing eye-to-eye contact in life-sized videoconferencing}",
  author   = "Regenbrecht, H and Müller, L and Hoermann, S and Langlotz, T and
              Duenser, A",
  journal  = "Inf. Sci. , HCI, Univ. Otago, Dunedin, New Zealand",
  abstract = "Videoconferencing systems available for end users do not allow for
              eye-to-eye contact between participants, also known as a lack of
              mutual gaze. The different locations of video camera and video
              display used for video conferencing makes it impossible to
              directly look into each other's eyes. This, combined with a lack
              of a lifesized video image of the communication partner makes a
              videoconferencing session an artificial experience leading to a
              decreased communication quality, empathy and trust. First, we
              present a survey on possible solutions to implement life-sized
              eye-to-eye contact and discuss briefly their pros and cons. The
              discussion of the characteristics and limitations of each concept
              can be used as a guideline for designing videoconferencing
              systems. Second, we present our own 1:1 scale videoconferencing
              solution, which builds upon advantages of other approaches while
              minimizing their disadvantages. Third and final, we report on the
              experiences made with our system in empirical evaluations. Author",
  year     =  2012,
  language = "en"
}

@ARTICLE{Iwatsuki_undated-ep,
  title  = "{Differences of eye gaze behavior between expert and novices while
            soccer coaching}",
  author = "Iwatsuki, Atsushi and Hirayama, Takatsugu and Morita, Junya and
            Mase, Kenji",
  url    = "https://www.jcss.gr.jp/meetings/JCSS2014/proceedings/pdf/JCSS2014_P2-36.pdf",
  file   = "All Papers/My Library/Iwatsuki et al. - Differences of eye gaze behavior between expert and novices while soccer coaching.pdf"
}

@MISC{Lombard_undated-bs,
  title        = "{Measuring presence: The Temple Presence Inventory}",
  author       = "Lombard, Matthew and Ditton, Theresa B and Weinstein, Lisa",
  abstract     = "This paper describes the development and testing of the Temple
                  Presence Inventory. The TPI questionnaire is a
                  multidimensional, literature-based measure of telepresence
                  that has demonstrated sensitivity to media form and content in
                  studies discussed here.",
  howpublished = "\url{https://academic.csuohio.edu/kneuendorf/frames/LombardDittonWeinstein09.pdf}",
  note         = "Accessed: 2022-3-4"
}

@MISC{Lombard_undated-uz,
  title    = "{Measuring presence: The temple presence inventory}",
  author   = "Lombard, Matthew and Ditton, Theresa B and Weinstein, Lisa",
  abstract = "This paper describes the development and testing of the Temple
              Presence Inventory. The TPI questionnaire is a multidimensional,
              literature-based measure of telepresence that has demonstrated
              sensitivity to media form and content in studies discussed here.",
  url      = "https://academic.csuohio.edu/kneuendorf/frames/LombardDittonWeinstein09.pdf",
  annote   = "Accessed: 2022-3-4"
}

@ARTICLE{Driess2023-ko,
  title    = "{PaLM-E: An Embodied Multimodal Language Model}",
  author   = "Driess, Danny and Xia, Fei and Sajjadi, Mehdi S M and Lynch, Corey
              and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and
              Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang,
              Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth,
              Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman,
              Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and
              Mordatch, Igor and Florence, Pete",
  abstract = "Large language models have been demonstrated to perform complex
              tasks. However, enabling general inference in the real world, e.g.
              for robotics problems, raises the challenge of grounding. We
              propose embodied language models to directly incorporate
              real-world continuous sensor modalities into language models and
              thereby establish the link between words and percepts. Input to
              our embodied language model are multi-modal sentences that
              interleave visual, continuous state estimation, and textual input
              encodings. We train these encodings end-to-end, in conjunction
              with a pretrained large language model, for multiple embodied
              tasks including sequential robotic manipulation planning, visual
              question answering, and captioning. Our evaluations show that
              PaLM-E, a single large embodied multimodal model, can address a
              variety of embodied reasoning tasks, from a variety of observation
              modalities, on multiple embodiments, and further, exhibits
              positive transfer: the model beneﬁts from diverse joint training
              across internet-scale language, vision, and visual-language
              domains. Our largest model, PaLM-E-562B with 562B parameters, in
              addition to being trained on robotics tasks, is a visual-language
              generalist with state-of-the-art performance on OK-VQA, and
              retains generalist language capabilities with increasing scale.",
  month    =  "6~" # mar,
  year     =  2023,
  url      = "http://arxiv.org/abs/2303.03378",
  file     = "All Papers/My Library/Driess et al. 2023 - PaLM-E - An Embodied Multimodal Language Model.pdf",
  language = "en"
}

@ARTICLE{Kapelyukh2022-kz,
  title    = "{DALL-E-Bot: Introducing web-scale diffusion models to robotics}",
  author   = "Kapelyukh, Ivan and Vosylius, Vitalis and Johns, Edward",
  abstract = "We introduce the first work to explore web-scale diffusion models
              for robotics. DALL-E-Bot enables a robot to rearrange objects in a
              scene, by first inferring a text description of those objects,
              then generating an image representing a natural, human-like
              arrangement of those objects, and finally physically arranging the
              objects according to that image. The significance is that we
              achieve this zero-shot using DALL-E, without needing any further
              data collection or training. Encouraging real-world results with
              human studies show that this is an exciting direction for the
              future of web-scale robot learning algorithms. We also propose a
              list of recommendations to the text-to-image community, to align
              further developments of these models with applications to
              robotics. Videos are available at:
              https://www.robot-learning.uk/dall-e-bot",
  month    =  "10~" # may,
  year     =  2022,
  url      = "http://arxiv.org/abs/2210.02438",
  file     = "All Papers/My Library/Kapelyukh et al. 2022 - DALL-E-Bot - Introducing web-scale diffusion models to robotics.pdf"
}

@MISC{Huey_undated-zd,
  title    = "{Developmental changes in the semantic part structure of drawn
              objects}",
  author   = "Huey, Holly and Long, Bria and Yang, Justin and George, Kaylee and
              Fan, Judith E",
  abstract = "Children produce increasingly more recognizable drawings of object
              concepts throughout childhood. What drives this improvement? Here
              we explore the role of children's ability to include relevant
              parts of those objects in their drawings. We crowdsourced part
              tags for every pen stroke in 2,160 drawings of 16 common object
              categories that had been produced by children between 4 and 8
              years old. These part decompositions revealed both substantial
              variation in the number and kind of parts that children
              emphasized, as well as a non-monotonic relationship between the
              number of parts that children drew and how recognizable their
              drawing was. Taken together, our findings advance current
              understanding of how children's part compositionality in their
              drawings both drives and constrains recognition of depicted
              objects. We plan to publicly release these data to catalyze
              further investigation of how children's drawings change across
              development.",
  url      = "https://cogtoolslab.github.io/pdf/hueylong_cogsci_2022.pdf",
  file     = "All Papers/My Library/Huey et al. - Developmental changes in the semantic part structure of drawn objects.pdf",
  annote   = "Accessed: 2022-8-6"
}

@ARTICLE{Bhuiyan2022-oj,
  title    = "{OtherTube: Facilitating content discovery and reflection by
              exchanging YouTube recommendations with strangers}",
  author   = "Bhuiyan, Md Momen and Isaza, Carlos Augusto Bautista and Mitra,
              Tanushree and Lee, Sang Won",
  journal  = "arXiv [cs.HC]",
  abstract = "To promote engagement, recommendation algorithms on platforms like
              YouTube increasingly personalize users' feeds, limiting users'
              exposure to diverse content and depriving them of opportunities to
              reflect on their interests compared to others'. In this work, we
              investigate how exchanging recommendations with strangers can help
              users discover new content and reflect. We tested this idea by
              developing OtherTube – a browser extension for YouTube that
              displays strangers' personalized YouTube recommendations.
              OtherTube allows users to (i) create an anonymized profile for
              social comparison, (ii) share their recommended videos with
              others, and (iii) browse strangers' YouTube recommendations. We
              conducted a 10-day-long user study (n=41) followed by a post-study
              interview (n=11). Our results reveal that users discovered and
              developed new interests from seeing OtherTube recommendations. We
              identified user and content characteristics that affect
              interaction and engagement with exchanged recommendations; for
              example, younger users interacted more with OtherTube, while the
              perceived irrelevance of some content discouraged users from
              watching certain videos. Users reflected on their interests as
              well as others', recognizing similarities and differences. Our
              work shows promise for designs leveraging the exchange of
              personalized recommendations with strangers.",
  month    =  jan,
  year     =  2022,
  url      = "http://arxiv.org/abs/2201.11709",
  file     = "All Papers/My Library/Bhuiyan et al. 2022 - OtherTube - Facilitating content discovery and reflection by exchanging YouTube recommendations with strangers.pdf"
}

@ARTICLE{2002-ip,
  title     = "{ビデオ対話に正面顔が必要不可欠か}",
  author    = "{森川治} and {橋本亮一}",
  journal   = "日本バーチャルリアリティ学会第 7 回大会論文集",
  publisher = "ds0n.cc.yamaguchi-u.ac.jp",
  pages     = "283--286",
  abstract  = "… 表示方法として、対話相手だけの場合( ビデオ 対話条件 V)、画面左側に自己像の鏡映像を追加 表示する場合（マ
               ルチウインドウ条件 M）、さらに背景を同一にする場合（ハ イパーミラー[7]条件 H）の 3 種類を用意した。実験者の 視線は、
               カメラ 目線 の場合と、画面上の被験者を見る目 線の場合の 2 …",
  year      =  2002
}

@ARTICLE{He2021-cw,
  title    = "{LookAtChat: Visualizing gaze awareness for remote Small-Group
              conversations}",
  author   = "He, Zhenyi and Du, Ruofei and Perlin, Ken",
  journal  = "arXiv [cs.HC]",
  abstract = "Video conferences play a vital role in our daily lives. However,
              many nonverbal cues are missing, including gaze and spatial
              information. We introduce LookAtChat, a web-based video
              conferencing system, which empowers remote users to identify gaze
              awareness and spatial relationships in small-group conversations.
              Leveraging real-time eye-tracking technology available with
              ordinary webcams, LookAtChat tracks each user's gaze direction,
              identifies who is looking at whom, and provides corresponding
              spatial cues. Informed by formative interviews with 5 participants
              who regularly use videoconferencing software, we explored the
              design space of gaze visualization in both 2D and 3D layouts. We
              further conducted an exploratory user study (N=20) to evaluate
              LookAtChat in three conditions: baseline layout, 2D directional
              layout, and 3D perspective layout. Our findings demonstrate how
              LookAtChat engages participants in small-group conversations, how
              gaze and spatial information improve conversation quality, and the
              potential benefits and challenges to incorporating gaze awareness
              visualization into existing videoconferencing systems.",
  month    =  jul,
  year     =  2021,
  url      = "http://arxiv.org/abs/2107.06265",
  file     = "All Papers/My Library/He et al. 2021 - LookAtChat - Visualizing gaze awareness for remote Small-Group conversations.pdf"
}

@ARTICLE{Chen2022-hm,
  title    = "{Sporthesia: Augmenting Sports Videos Using Natural Language}",
  author   = "Chen, Zhutian and Yang, Qisen and Xie, Xiao and Beyer, Johanna and
              Xia, Haijun and Wu, Yingcai and Pfister, Hanspeter",
  abstract = "Augmented sports videos, which combine visualizations and video
              effects to present data in actual scenes, can communicate insights
              engagingly and thus have been increasingly popular for sports
              enthusiasts around the world. Yet, creating augmented sports
              videos remains a challenging task, requiring considerable time and
              video editing skills. On the other hand, sports insights are often
              communicated using natural language, such as in commentaries, oral
              presentations, and articles, but usually lack visual cues. Thus,
              this work aims to facilitate the creation of augmented sports
              videos by enabling analysts to directly create visualizations
              embedded in videos using insights expressed in natural language.
              To achieve this goal, we propose a three-step approach - 1)
              detecting visualizable entities in the text, 2) mapping these
              entities into visualizations, and 3) scheduling these
              visualizations to play with the video - and analyzed 155 sports
              video clips and the accompanying commentaries for accomplishing
              these steps. Informed by our analysis, we have designed and
              implemented Sporthesia, a proof-of-concept system that takes
              racket-based sports videos and textual commentaries as the input
              and outputs augmented videos. We demonstrate Sporthesia's
              applicability in two exemplar scenarios, i.e., authoring augmented
              sports videos using text and augmenting historical sports videos
              based on auditory comments. A technical evaluation shows that
              Sporthesia achieves high accuracy (F1-score of 0.9) in detecting
              visualizable entities in the text. An expert evaluation with eight
              sports analysts suggests high utility, effectiveness, and
              satisfaction with our language-driven authoring method and
              provides insights for future improvement and opportunities.",
  month    =  "7~" # sep,
  year     =  2022,
  url      = "http://arxiv.org/abs/2209.03434",
  file     = "All Papers/My Library/Chen et al. 2022 - Sporthesia - Augmenting Sports Videos Using Natural Language.pdf",
  annote   = "Comment: 10 pages, IEEE VIS conference"
}

@ARTICLE{Sarcar2013-kq,
  title     = "{{EyeK}: an efficient dwell-free eye gaze-based text entry
               system}",
  author    = "Sarcar, S and Panwar, P and Chakraborty, T",
  journal   = "Proceedings of the 11th asia pacific",
  publisher = "dl.acm.org",
  abstract  = "Over the last three decades, eye gaze has become an important
               modality of text entry in large and small display digital devices
               covering people with disabilities beside the able …",
  year      =  2013,
  keywords  = "prj-gaze-shorthand"
}

@ARTICLE{Sarcar2013-uf,
  title    = "{EyeK: an efficient dwell-free eye gaze-based text entry system}",
  author   = "{Sarcar} and {Panwar} and {Chakraborty}",
  journal  = "Proceedings of the 11th asia pacific",
  abstract = "Over the last three decades, eye gaze has become an important
              modality of text entry in large and small display digital devices
              covering people with disabilities beside the able …",
  year     =  2013
}

@INPROCEEDINGS{Hansen2006-po,
  title     = "{Gaze communication systems for people with {ALS}}",
  author    = "Hansen, John Paulin and Lund, Hákon and Aoki, Hirotaka and Itoh,
               Kenji",
  booktitle = "{{ALS} Workshop, in conjunction with the 17th International
               Symposium on {ALS/MND}, Yokohama, Japan}",
  publisher = "wiki.cogain.org",
  abstract  = "… of research and development, because gaze cannot be reliably
               controlled in a … of ALS The main recommendation that we would
               like to give people with ALS is to start preparing for gaze …",
  year      =  2006,
  keywords  = "prj-gaze-shorthand"
}

@INPROCEEDINGS{Hansen2006-uy,
  title     = "{Gaze communication systems for people with {ALS}}",
  author    = "Hansen, John Paulin and Lund, Hákon and Aoki, Hirotaka and Itoh,
               Kenji",
  booktitle = "{ALS workshop, in conjunction with the 17th international
               symposium on ALS/MND, yokohama, japan}",
  publisher = "wiki.cogain.org",
  abstract  = "… of research and development, because gaze cannot be reliably
               controlled in a … of ALS The main recommendation that we would
               like to give people with ALS is to start preparing for gaze …",
  year      =  2006
}

@BOOK{Kaur2021-dj,
  title    = "{Subject Guided Eye Image Synthesis with Application to Gaze
              Redirection}",
  author   = "Kaur, Harsimran and Manduchi, Roberto",
  abstract = "Author(s): Kaur, Harsimran; Manduchi, Roberto | Abstract: We
              propose a method for synthesizing eye images from segmentation
              masks with a desired style. The style encompasses attributes such
              as skin color, texture, iris color, and personal identity. Our
              approach generates an eye image that is consistent with a given
              segmentation mask and has the attributes of the input style image.
              We apply our method to data augmentation as well as to gaze
              redirection. The previous techniques of synthesizing real eye
              images from synthetic eye images for data augmentation lacked
              control over the generated attributes. We demonstrate the
              effectiveness of the proposed method in synthesizing realistic eye
              images with given characteristics corresponding to the synthetic
              labels for data augmentation, which is further useful for various
              tasks such as gaze estimation, eye image segmentation, pupil
              detection, etc. We also show how our approach can be applied to
              gaze redirection using only synthetic gaze labels, improving the
              previous state of the art results. The main contributions of our
              paper are i) a novel approach for Style-Based eye image generation
              from segmentation mask; ii) the use of this approach for
              gaze-redirection without the need for gaze annotated real eye
              images.",
  year     =  2021,
  keywords = "eye contact;telepresence;prj-gaze-design;uist2022-gaze-design",
  isbn     =  9781665404778
}

@PHDTHESIS{Hietanen2020-si,
  title     = "{All Eyes on Eye Contact: Studies on cognitive, affective and
               behavioral effects of eye contact}",
  author    = "Hietanen, Jonne",
  publisher = "Tampere University",
  year      =  2020,
  isbn      =  9789520317133,
  language  = "en"
}

@PHDTHESIS{Hietanen2020-fl,
  title    = "{All Eyes on Eye Contact: Studies on cognitive, affective and
              behavioral effects of eye contact}",
  author   = "Hietanen, Jonne",
  year     =  2020,
  school   = "Tampere University",
  language = "en"
}

@ARTICLE{Isomoto_undated-bx,
  title    = "{Gaze-based Command Activation Technique Robust Against
              Unintentional Activation using Dwell-then-Gesture}",
  author   = "Isomoto, Toshiya and Yamanaka, Shota and Shizuki, Buntarou",
  abstract = "We demonstrate a gaze-based command activation technique that is
              robust against unintentional command activations using a series of
              dwelling on a target and performing a speciﬁc gesture
              (dwell-thengesture manipulation). The gesture adopted is a simple
              two-level stroke, which consists of a sequence of two orthogonal
              strokes. To achieve robustness against unintentional command
              activations, we designed and ﬁne-tuned a gesture detection system
              based on how users move their gaze, as revealed through three
              experiments. Although our technique seems to simply combine
              well-known dwelland gesture-based manipulations, implying a low
              rate of success, our technique is actually the ﬁrst technique that
              consists of a short time dwelling for target selection and a
              simple gesture for command activation. In addition, our technique
              will be the ﬁrst technique adopting a marking menu, which is a
              traditional menu for command activation used in mouse- or
              pen-based interactions to gaze-based interactions.",
  language = "en"
}

@ARTICLE{He2019-vf,
  title    = "{Exploring Configurations for Multi-user Communication in Virtual
              Reality}",
  author   = "He, Zhenyi and Rosenberg, Karl and Perlin, K",
  journal  = "ArXiv",
  volume   = "abs/1911.06877",
  abstract = "This work proposes CollabVR, a distributed multi-user
              collaboration environment, to explore how digital content improves
              expression and understanding of ideas among groups, and designed
              and examined three possible configurations for participants and
              shared manipulable objects. Virtual Reality (VR) enables users to
              collaborate while exploring scenarios not realizable in the
              physical world. We propose CollabVR, a distributed multi-user
              collaboration environment, to explore how digital content improves
              expression and understanding of ideas among groups. To achieve
              this, we designed and examined three possible configurations for
              participants and shared manipulable objects. In configuration (1),
              participants stand side-by-side. In (2), participants are
              positioned across from each other, mirrored face-to-face. In (3),
              called ``eyes-free,'' participants stand side-by-side looking at a
              shared display, and draw upon a horizontal surface. We also
              explored a ``telepathy'' mode, in which participants could see
              from each other's point of view. We implemented ``3DSketch''
              visual objects for participants to manipulate and move between
              virtual content boards in the environment. To evaluate the system,
              we conducted a study in which four people at a time used each of
              the three configurations to cooperate and communicate ideas with
              each other. We have provided experimental results and interview
              responses.",
  year     =  2019,
  language = "en"
}

@ARTICLE{He2019-ww,
  title    = "{Exploring configurations for multi-user communication in virtual
              reality}",
  author   = "{He} and {Rosenberg} and {Perlin}",
  journal  = "ArXiv",
  volume   = "abs/1911.06877",
  abstract = "This work proposes CollabVR, a distributed multi-user
              collaboration environment, to explore how digital content improves
              expression and understanding of ideas among groups, and designed
              and examined three possible configurations for participants and
              shared manipulable objects. Virtual Reality (VR) enables users to
              collaborate while exploring scenarios not realizable in the
              physical world. We propose CollabVR, a distributed multi-user
              collaboration environment, to explore how digital content improves
              expression and understanding of ideas among groups. To achieve
              this, we designed and examined three possible configurations for
              participants and shared manipulable objects. In configuration (1),
              participants stand side-by-side. In (2), participants are
              positioned across from each other, mirrored face-to-face. In (3),
              called “eyes-free,” participants stand side-by-side looking at a
              shared display, and draw upon a horizontal surface. We also
              explored a “telepathy” mode, in which participants could see from
              each other's point of view. We implemented “3DSketch” visual
              objects for participants to manipulate and move between virtual
              content boards in the environment. To evaluate the system, we
              conducted a study in which four people at a time used each of the
              three configurations to cooperate and communicate ideas with each
              other. We have provided experimental results and interview
              responses.",
  year     =  2019,
  file     = "All Papers/My Library/He et al. 2019 - Exploring configurations for multi-user communication in virtual reality.pdf",
  language = "en"
}

@ARTICLE{2011-vq,
  title    = "{視線の向きを表現可能な2画面積層表示を用いたテレビ会議システムの提案}",
  author   = "和之, 磯 and 宗和, 伊達 and 英明, 高田 and 康子, 安藤 and 宣彦, 松浦",
  journal  = "情報処理学会論文誌",
  volume   =  52,
  number   =  3,
  pages    = "1224--1233",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  mar,
  year     =  2011,
  keywords = "特集：マルチメディア、分散、協調とモバイルシステム;eye contact;telepresence",
  issn     = "1882-7764"
}

@ARTICLE{2011-sb,
  title    = "{視線の向きを表現可能な2画面積層表示を用いたテレビ会議システムの提案}",
  author   = "{和之} and {宗和} and {英明} and {康子} and {宣彦}",
  journal  = "情報処理学会論文誌",
  volume   =  52,
  number   =  3,
  pages    = "1224–1233",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  mar,
  year     =  2011,
  issn     = "1882-7764"
}

@ARTICLE{Bente2007-ph,
  title     = "{Effects of simulated gaze on social presence, person perception
               and personality attribution in avatar-mediated communication}",
  author    = "Bente, G and Eschenburg, Felix and Aelker, Lisa",
  publisher = "matthewlombard.com",
  abstract  = "The paper introduces a platform for experimental analysis of
               social gaze in avatar-mediated communications and reports on two
               studies demonstrating the person perception effects of varying
               durations of directed gaze. The avatar platform allows to
               transmit nonverbal behaviour in real-time and to replace
               particular components by simulated data. Study 1, conducted with
               gender homogeneous female dyads, used two variations of directed
               gaze (2 vs. 4 seconds). Consistent with the literature the longer
               gaze duration was found to cause significantly better evaluations
               of the interaction partner and higher levels of co-presence.
               Study 2, conducted with mixed-sex dyads, included two more
               variations of gaze duration (8 and 16 seconds). Prior effects in
               evaluation and co-presence could not be reproduced. However
               significant effects occurred with respect to the attribution of
               personality traits. These effects were nonlinear. Consistent with
               the literature female observers responded more sensitively to the
               gaze variations of the male partners.",
  year      =  2007,
  url       = "http://matthewlombard.com/ISPR/Proceedings/2007/Bente,%20Eschenburg,%20and%20Aelker.pdf",
  file      = "All Papers/Other/Bente et al. 2007 - Effects of simulated gaze on social presence, person perception and personality attribution in avatar-mediated communication.pdf"
}

@ARTICLE{Van_Der_Kamp2010-mh,
  title     = "{Gaze-based paint program with voice recognition}",
  author    = "Van Der Kamp, J",
  publisher = "Citeseer",
  abstract  = "… possibility of using gaze to control a cursor in a paint
               program. … investigates using gaze to control a cursor in a paint
               program … allow easy selection with gaze is implemented with
               buttons …",
  year      =  2010,
  keywords  = "prj-gaze-shorthand"
}

@ARTICLE{Van_Der_Kamp2010-mi,
  title    = "{Gaze-based paint program with voice recognition}",
  author   = "Van Der Kamp, J",
  abstract = "Eye trackers work by measuring where a person's gaze is focused on
              a computer monitor in real-time. This allows for certain
              applications to be controlled by the eyes which benefits disabled
              users for whom keyboard and mouse are not an option as input. This
              dissertation presents an application used for drawing on screen
              using an eye tracker for control. Voice recognition is also used
              which offers a method of confirming actions which is separated
              from eye movements.",
  year     =  2010,
  url      = "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.464.4531&rep=rep1&type=pdf"
}

@ARTICLE{Pumarola2020-pz,
  title    = "{D-NeRF: Neural radiance fields for dynamic scenes}",
  author   = "Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and
              Moreno-Noguer, Francesc",
  journal  = "arXiv [cs.CV]",
  abstract = "Neural rendering techniques combining machine learning with
              geometric reasoning have arisen as one of the most promising
              approaches for synthesizing novel views of a scene from a sparse
              set of images. Among these, stands out the Neural radiance fields
              (NeRF), which trains a deep network to map 5D input coordinates
              (representing spatial location and viewing direction) into a
              volume density and view-dependent emitted radiance. However,
              despite achieving an unprecedented level of photorealism on the
              generated images, NeRF is only applicable to static scenes, where
              the same spatial location can be queried from different images. In
              this paper we introduce D-NeRF, a method that extends neural
              radiance fields to a dynamic domain, allowing to reconstruct and
              render novel images of objects under rigid and non-rigid motions
              from a {single} camera moving around the scene. For this purpose
              we consider time as an additional input to the system, and split
              the learning process in two main stages: one that encodes the
              scene into a canonical space and another that maps this canonical
              representation into the deformed scene at a particular time. Both
              mappings are simultaneously learned using fully-connected
              networks. Once the networks are trained, D-NeRF can render novel
              images, controlling both the camera view and the time variable,
              and thus, the object movement. We demonstrate the effectiveness of
              our approach on scenes with objects under rigid, articulated and
              non-rigid motions. Code, model weights and the dynamic scenes
              dataset will be released.",
  month    =  nov,
  year     =  2020,
  url      = "http://arxiv.org/abs/2011.13961",
  file     = "All Papers/My Library/Pumarola et al. 2020 - D-NeRF - Neural radiance fields for dynamic scenes.pdf"
}

@ARTICLE{2011-qg,
  title   = "{可動式カメラによる社会的テレプレゼンスの強化}",
  author  = "{村上友樹} and {中西英之} and {Others}",
  journal = "情報処理学会論文誌",
  volume  =  52,
  number  =  4,
  pages   = "1635–1643",
  year    =  2011
}

@ARTICLE{Kacete_undated-vl,
  title  = "{Head pose free 3D gaze estimation using RGB-D camera}",
  author = "Kacete, Amine and Séguier, Renaud and Collobert, Michel and Royan,
            Jérôme",
  url    = "https://hal.archives-ouvertes.fr/hal-01393594/file/Head_Pose_Free_3D_Gaze_Estimation_Using_RGB-D_Camera_postprint.pdf",
  file   = "All Papers/My Library/Kacete et al. - Head pose free 3D gaze estimation using RGB-D camera.pdf"
}

@ARTICLE{Ryskeldiev2018-rb,
  title     = "{Distributed metaverse: creating decentralized blockchain-based
               model for peer-to-peer sharing of virtual spaces for mixed
               reality applications}",
  author    = "Ryskeldiev, B and Ochiai, Y and Cohen, M and Herder, J",
  journal   = "Proceedings of the 9th",
  publisher = "dl.acm.org",
  abstract  = "Mixed reality telepresence is becoming an increasingly popular
               form of interaction in social and collaborative applications. We
               are interested in how created virtual spaces can be …",
  year      =  2018,
  keywords  = "spatial computing"
}

@ARTICLE{Ryskeldiev2018-tk,
  title    = "{Distributed metaverse: creating decentralized blockchain-based
              model for peer-to-peer sharing of virtual spaces for mixed reality
              applications}",
  author   = "{Ryskeldiev} and {Ochiai} and {Cohen} and {Herder}",
  journal  = "Proceedings of the 9th",
  abstract = "Mixed reality telepresence is becoming an increasingly popular
              form of interaction in social and collaborative applications. We
              are interested in how created virtual spaces can be …",
  year     =  2018
}

@ARTICLE{Fidalgo2020-yh,
  title    = "{{MAGIC}: Manipulating Avatars and Gestures to Improve Remote
              Collaboration}",
  author   = "Fidalgo, Catarina Gonçalves",
  abstract = "Remote work plays an essential role in projects in which
              geographically separated people need to perform joint tasks. Apart
              from saving time and resources, collaborating remotely is helpful
              in situations when confinement is required. Life-sized
              face-to-face telepresence promotes the sense of ''being there''
              and improves collaboration by allowing an immediate understanding
              of nonverbal cues. However, when discussing shared 3D content in a
              face-to-face setting, having different points-of-view of the model
              paired with occlusions raises ambiguities in analysis, decreasing
              workspace awareness. In this dissertation, we introduce MAGIC, a
              novel telepresence approach that improves remote collaboration in
              shared 3D workspaces by allowing participants to communicate
              through nonverbal cues while sharing the same workspace
              perspective, integrating task-, person-, and referencespace
              seamlessly. To enable a face-to-face setting with shared
              perspective, we manipulate the remote participant's
              representations and gestures so that they correctly apply to the
              local person's reference space. To evaluate our approach, we
              developed a Virtual Reality prototype that combines the virtual
              spaces of two remote collaborators so that they can work together
              in the same worktable. We capture the collaborators' hands and
              head positions in real-time and use them to animate fully rigged
              avatars through inverse kinematics. Results from a user evaluation
              suggest that MAGIC is effective in improving task performance and
              increase workspace awareness. Furthermore, manipulations improved
              the sense of presence between remote collaborators despite being
              unnoticed.",
  year     =  2020,
  language = "en"
}

@ARTICLE{Fidalgo2020-ym,
  title    = "{MAGIC: Manipulating avatars and gestures to improve remote
              collaboration}",
  author   = "Fidalgo, Catarina Gonçalves",
  abstract = "Remote work plays an essential role in projects in which
              geographically separated people need to perform joint tasks. Apart
              from saving time and resources, collaborating remotely is helpful
              in situations when confinement is required. Life-sized
              face-to-face telepresence promotes the sense of ”being there” and
              improves collaboration by allowing an immediate understanding of
              nonverbal cues. However, when discussing shared 3D content in a
              face-to-face setting, having different points-of-view of the model
              paired with occlusions raises ambiguities in analysis, decreasing
              workspace awareness. In this dissertation, we introduce MAGIC, a
              novel telepresence approach that improves remote collaboration in
              shared 3D workspaces by allowing participants to communicate
              through nonverbal cues while sharing the same workspace
              perspective, integrating task-, person-, and referencespace
              seamlessly. To enable a face-to-face setting with shared
              perspective, we manipulate the remote participant's
              representations and gestures so that they correctly apply to the
              local person's reference space. To evaluate our approach, we
              developed a Virtual Reality prototype that combines the virtual
              spaces of two remote collaborators so that they can work together
              in the same worktable. We capture the collaborators' hands and
              head positions in real-time and use them to animate fully rigged
              avatars through inverse kinematics. Results from a user evaluation
              suggest that MAGIC is effective in improving task performance and
              increase workspace awareness. Furthermore, manipulations improved
              the sense of presence between remote collaborators despite being
              unnoticed.",
  year     =  2020,
  file     = "All Papers/My Library/Fidalgo 2020 - MAGIC - Manipulating avatars and gestures to improve remote collaboration.pdf",
  language = "en"
}

@ARTICLE{Ghosh2021-jo,
  title    = "{Automatic gaze analysis: A survey of deep learning based
              approaches}",
  author   = "Ghosh, Shreya and Dhall, Abhinav and Hayat, Munawar and Knibbe,
              Jarrod and Ji, Qiang",
  abstract = "Eye gaze analysis is an important research problem in the field of
              Computer Vision and Human-Computer Interaction. Even with notable
              progress in the last 10 years, automatic gaze analysis still
              remains challenging due to the uniqueness of eye appearance,
              eye-head interplay, occlusion, image quality, and illumination
              conditions. There are several open questions, including what are
              the important cues to interpret gaze direction in an unconstrained
              environment without prior knowledge and how to encode them in
              real-time. We review the progress across a range of gaze analysis
              tasks and applications to elucidate these fundamental questions,
              identify effective methods in gaze analysis, and provide possible
              future directions. We analyze recent gaze estimation and
              segmentation methods, especially in the unsupervised and weakly
              supervised domain, based on their advantages and reported
              evaluation metrics. Our analysis shows that the development of a
              robust and generic gaze analysis method still needs to address
              real-world challenges such as unconstrained setup and learning
              with less supervision. We conclude by discussing future research
              directions for designing a real-world gaze analysis system that
              can propagate to other domains including Computer Vision,
              Augmented Reality (AR), Virtual Reality (VR), and Human Computer
              Interaction (HCI). Project Page:
              https://github.com/i-am-shreya/EyeGazeSurvey",
  month    =  "8~" # dec,
  year     =  2021,
  url      = "http://arxiv.org/abs/2108.05479",
  file     = "All Papers/My Library/Ghosh et al. 2021 - Automatic gaze analysis - A survey of deep learning based approaches.pdf"
}

@ARTICLE{Milgram1994-cm,
  title    = "{A Taxonomy of Mixed Reality Visual Displays}",
  author   = "Milgram, P and Kishino, F",
  journal  = "IEICE transactions on information and systems",
  abstract = "Paul Milgram received the B.A.Sc. degree from the University of
              Toronto in 1970, the M.S.E.E. degree from the Technion (Israel) in
              1973 and the Ph.D. degree from the University of Toronto in 1980.
              From 1980 to 1982 he was a ZWO Visiting Scientist and a NATO
              Postdoctoral in the Netherlands, researching automobile driving
              behaviour. From 1982 to 1984 he was a Senior Research Engineer in
              Human Engineering at the National Aerospace Laboratory (NLR) in
              Amsterdam, where his work involved the modelling of aircraft
              flight crew activity, advanced display concepts and control loops
              with human operators in space teleoperation. Since 1986 he has
              worked at the Industrial Engineering Department of the University
              of Toronto, where he is currently an Associate Professor and
              Coordinator of the Human Factors Engineering group. He is also
              cross appointed to the Department of Psychology. In 1993-94 he was
              an invited researcher at the ATR Communication Systems Research
              Laboratories, in Kyoto, Japan. His research interests include
              display and control issues in telerobotics and virtual
              environments, stereoscopic video and computer graphics, cognitive
              engineering, and human factors issues in medicine. He is also
              President of Translucent Technologies, a company which produces
              ``Plato'' liquid crystal visual occlusion spectacles (of which he
              is the inventor), for visual and psychomotor research.",
  month    =  "25~" # dec,
  year     =  1994,
  url      = "https://www.semanticscholar.org/paper/A-Taxonomy-of-Mixed-Reality-Visual-Displays-Milgram-Kishino/f78a31be8874eda176a5244c645289be9f1d4317",
  file     = "All Papers/My Library/Milgram and Kishino 1994 - A Taxonomy of Mixed Reality Visual Displays.pdf",
  issn     = "0916-8532",
  annote   = "[TLDR] Paul Milgram's research interests include display and
              control issues in telerobotics and virtual environments,
              stereoscopic video and computer graphics, cognitive engineering,
              and human factors issues in medicine."
}

@ARTICLE{undated-im,
  title    = "{Visual effects to highlight a remote person's approach enhance
              telepresence}",
  author   = "英之, 中西",
  keywords = "eye contact;telepresence"
}

@ARTICLE{undated-yp,
  title  = "{Visual effects to highlight a remote person's approach enhance
            telepresence}",
  author = "英之, 中西"
}

@ARTICLE{Wu2019-mn,
  title    = "{Imagine that! Leveraging emergent affordances for 3D tool
              synthesis}",
  author   = "Wu, Yizhe and Kasewa, Sudhanshu and Groth, Oliver and Salter,
              Sasha and Sun, Li and Jones, Oiwi Parker and Posner, Ingmar",
  journal  = "arXiv [cs.LG]",
  abstract = "In this paper we explore the richness of information captured by
              the latent space of a vision-based generative model. The model
              combines unsupervised generative learning with a task-based
              performance predictor to learn and to exploit task-relevant object
              affordances given visual observations from a reaching task,
              involving a scenario and a stick-like tool. While the learned
              embedding of the generative model captures factors of variation in
              3D tool geometry (e.g. length, width, and shape), the performance
              predictor identifies sub-manifolds of the embedding that correlate
              with task success. Within a variety of scenarios, we demonstrate
              that traversing the latent space via backpropagation from the
              performance predictor allows us to imagine tools appropriate for
              the task at hand. Our results indicate that affordances-like the
              utility for reaching-are encoded along smooth trajectories in
              latent space. Accessing these emergent affordances by considering
              only high-level performance criteria (such as task success)
              enables an agent to manipulate tool geometries in a targeted and
              deliberate way.",
  month    =  sep,
  year     =  2019,
  url      = "http://arxiv.org/abs/1909.13561",
  file     = "All Papers/My Library/Wu et al. 2019 - Imagine that! Leveraging emergent affordances for 3D tool synthesis.pdf"
}

@ARTICLE{Liu2022-xf,
  title    = "{StructDiffusion: Object-centric diffusion for semantic
              rearrangement of novel objects}",
  author   = "Liu, Weiyu and Hermans, Tucker and Chernova, Sonia and Paxton,
              Chris",
  abstract = "Robots operating in human environments must be able to rearrange
              objects into semantically-meaningful configurations, even if these
              objects are previously unseen. In this work, we focus on the
              problem of building physically-valid structures without
              step-by-step instructions. We propose StructDiffusion, which
              combines a diffusion model and an object-centric transformer to
              construct structures out of a single RGB-D image based on
              high-level language goals, such as “set the table.” Our method
              shows how diffusion models can be used for complex multi-step 3D
              planning tasks. StructDiffusion improves success rate on
              assembling physically-valid structures out of unseen objects by on
              average 16\% over an existing multi-modal transformer model, while
              allowing us to use one multi-task model to produce a wider range
              of different structures. We show experiments on held-out objects
              in both simulation and on real-world rearrangement tasks. For
              videos and additional results, check out our website:
              http://weiyuliu.com/StructDiffusion/.",
  month    =  "11~" # aug,
  year     =  2022,
  url      = "http://arxiv.org/abs/2211.04604",
  file     = "All Papers/My Library/Liu et al. 2022 - StructDiffusion - Object-centric diffusion for semantic rearrangement of novel objects.pdf"
}

@ARTICLE{Watson2022-hz,
  title    = "{Novel view synthesis with diffusion models}",
  author   = "Watson, Daniel and Chan, William and Martin-Brualla, Ricardo and
              Ho, Jonathan and Tagliasacchi, Andrea and Norouzi, Mohammad",
  abstract = "We present 3DiM, a diffusion model for 3D novel view synthesis,
              which is able to translate a single input view into consistent and
              sharp completions across many views. The core component of 3DiM is
              a pose-conditional image-to-image diffusion model, which takes a
              source view and its pose as inputs, and generates a novel view for
              a target pose as output. 3DiM can generate multiple views that are
              3D consistent using a novel technique called stochastic
              conditioning. The output views are generated autoregressively, and
              during the generation of each novel view, one selects a random
              conditioning view from the set of available views at each
              denoising step. We demonstrate that stochastic conditioning
              significantly improves the 3D consistency of a naive sampler for
              an image-to-image diffusion model, which involves conditioning on
              a single fixed view. We compare 3DiM to prior work on the SRN
              ShapeNet dataset, demonstrating that 3DiM's generated completions
              from a single view achieve much higher fidelity, while being
              approximately 3D consistent. We also introduce a new evaluation
              methodology, 3D consistency scoring, to measure the 3D consistency
              of a generated object by training a neural field on the model's
              output views. 3DiM is geometry free, does not rely on
              hyper-networks or test-time optimization for novel view synthesis,
              and allows a single model to easily scale to a large number of
              scenes.",
  month    =  "10~" # jun,
  year     =  2022,
  url      = "http://arxiv.org/abs/2210.04628",
  file     = "All Papers/My Library/Watson et al. 2022 - Novel view synthesis with diffusion models.pdf"
}

@ARTICLE{1997-jb,
  title     = "{映像文法に基づいた遠隔リアルタイム会議システムのインタフェース制御法}",
  author    = "{橋本昌嗣}",
  publisher = "dspace.jaist.ac.jp",
  abstract  = "… ポジションのこと をマスターショットと呼び，映像に意味を与える重要な要素の一つとして利用されて いる．
               本論文で提供する制御方法では，まず， 会議 や 会議 中の目的に応じた 会議 室の種類，例 えば…",
  year      =  1997
}

@ARTICLE{1997-be,
  title    = "{映像文法に基づいた遠隔リアルタイム会議システムのインタフェース制御法}",
  author   = "{橋本昌嗣}",
  abstract = "… ポジションのこと をマスターショットと呼び，映像に意味を与える重要な要素の一つとして利用されて いる．
              本論文で提供する制御方法では，まず， 会議 や 会議 中の目的に応じた 会議 室の種類，例 えば…",
  year     =  1997
}

@ARTICLE{Panwar2012-zd,
  title     = "{{EyeBoard}: A fast and accurate eye gaze-based text entry
               system}",
  author    = "Panwar, P and Sarcar, S and Samanta, D",
  journal   = "2012 4th International",
  publisher = "ieeexplore.ieee.org",
  abstract  = "Over the last three decades, eye gaze has become an important
               modality of text entry in large and small display digital devices
               covering people with disabilities beside the able …",
  year      =  2012,
  keywords  = "prj-gaze-shorthand"
}

@ARTICLE{Panwar2012-jw,
  title    = "{EyeBoard: A fast and accurate eye gaze-based text entry system}",
  author   = "{Panwar} and {Sarcar} and {Samanta}",
  journal  = "2012 4th International",
  abstract = "Over the last three decades, eye gaze has become an important
              modality of text entry in large and small display digital devices
              covering people with disabilities beside the able …",
  year     =  2012
}

@ARTICLE{Ryskeldiev2018-jq,
  title     = "{Spatial Social Media: Towards Collaborative Mixed Reality
               Telepresence ``On The Go''}",
  author    = "Ryskeldiev, B",
  journal   = "Extended Abstracts of the 2018 CHI Conference on",
  publisher = "dl.acm.org",
  abstract  = "Live video streaming is becoming an increasingly popular form of
               interaction in social media, with mobile devices being used for
               sharing of remote situation`` on the go.'' We are …",
  year      =  2018,
  keywords  = "spatial computing"
}

@ARTICLE{Lim2022-we,
  title    = "{Webcam eye tracking: Study conduction and acceptance of remote
              tests with gaze analysis}",
  author   = "Lim, Sezen and Walber, Tina and Schaefer, Christoph and Riehl,
              Lena",
  abstract = "Webcam eye tracking for the collection of gaze data in the context
              of user studies is convenient - it can be used in remote tests
              where participants do not need special hardware. The approach has
              strong limitations, especially regarding the motion-free nature of
              the test persons during data recording and the quality of the gaze
              data obtained. Our study with 52 participants shows that usable
              eye tracking data can be obtained with commercially available
              webcams in a remote setting. However, a high drop off rate must be
              considered, which is why we recommend a high over-recruitment of
              150\%. We also show that the acceptance of the approach by the
              study participants is high despite the given limitations.",
  month    =  "28~" # jul,
  year     =  2022,
  url      = "http://arxiv.org/abs/2207.14380",
  file     = "All Papers/My Library/Lim et al. 2022 - Webcam eye tracking - Study conduction and acceptance of remote tests with gaze analysis.pdf"
}

@ARTICLE{2003-we,
  title   = "{自己像追加による視線理解の変化}",
  author  = "{森川治} and {橋本亮一} and {山下樹里}",
  journal = "インタラクション",
  volume  =  2003,
  pages   = "279–286",
  year    =  2003
}

@ARTICLE{1992-hv,
  title     = "{{TV会議のための多視線一致方式}}",
  author    = "{小松忠彦} and 志和, 新一",
  journal   = "情報処理学会研究報告",
  publisher = "一般社団法人情報処理学会",
  volume    =  93,
  pages     = "77--84",
  abstract  = "
               多対多のTV会議に於て全てのメンバー間の視線一致が成り立つ多視線一致方式について,その原理とシステム構成,更にTV会議用以外の応用分野の展望について述べた.TV会議以外の応用分野としては,一体感を持ちながら画面を観察する各人が専用画像を持てる特徴を利用したもの,他人に自分の画像を覗かれない特性を利用したものが考えられる.前者の応用として,多言語字幕表示装置,多画面TVゲーム用ディスプレイ後者の応用として,情報を専有できる多視線プレゼンテーションシステム,多視線公衆電話端末について述べた.
               This paper describes on principle and applications of the
               multiple eye-contact method. For the applications of the method
               except TV conferences, the multilingual subtitles display system,
               multi-image presentation system in which a presentator can see
               his private information on the displayed image, multi-image TV
               game systems, and private public TV phone are described.",
  year      =  1992,
  keywords  = "eye contact;telepresence"
}

@ARTICLE{1992-vq,
  title    = "{TV会議のための多視線一致方式}",
  author   = "{小松忠彦} and {志和}",
  journal  = "情報処理学会研究報告",
  volume   =  93,
  pages    = "77–84",
  abstract = "
              多対多のTV会議に於て全てのメンバー間の視線一致が成り立つ多視線一致方式について,その原理とシステム構成,更にTV会議用以外の応用分野の展望について述べた.TV会議以外の応用分野としては,一体感を持ちながら画面を観察する各人が専用画像を持てる特徴を利用したもの,他人に自分の画像を覗かれない特性を利用したものが考えられる.前者の応用として,多言語字幕表示装置,多画面TVゲーム用ディスプレイ後者の応用として,情報を専有できる多視線プレゼンテーションシステム,多視線公衆電話端末について述べた.
              This paper describes on principle and applications of the multiple
              eye-contact method. For the applications of the method except TV
              conferences, the multilingual subtitles display system,
              multi-image presentation system in which a presentator can see his
              private information on the displayed image, multi-image TV game
              systems, and private public TV phone are described.",
  year     =  1992
}

@ARTICLE{Tanveer2023-aa,
  title    = "{DS-Fusion: Artistic typography via discriminated and stylized
              diffusion}",
  author   = "Tanveer, Maham and Wang, Yizhi and Mahdavi-Amiri, Ali and Zhang,
              Hao",
  abstract = "We introduce a novel method to automatically generate an artistic
              typography by stylizing one or more letter fonts to visually
              convey the semantics of an input word, while ensuring that the
              output remains readable. To address an assortment of challenges
              with our task at hand including conflicting goals (artistic
              stylization vs. legibility), lack of ground truth, and immense
              search space, our approach utilizes large language models to
              bridge texts and visual images for stylization and build an
              unsupervised generative model with a diffusion model backbone.
              Specifically, we employ the denoising generator in Latent
              Diffusion Model (LDM), with the key addition of a CNN-based
              discriminator to adapt the input style onto the input text. The
              discriminator uses rasterized images of a given letter/word font
              as real samples and output of the denoising generator as fake
              samples. Our model is coined DS-Fusion for discriminated and
              stylized diffusion. We showcase the quality and versatility of our
              method through numerous examples, qualitative and quantitative
              evaluation, as well as ablation studies. User studies comparing to
              strong baselines including CLIPDraw and DALL-E 2, as well as
              artist-crafted typographies, demonstrate strong performance of
              DS-Fusion.",
  month    =  "16~" # mar,
  year     =  2023,
  url      = "https://ds-fusion.github.io/",
  file     = "All Papers/My Library/Tanveer et al. 2023 - DS-Fusion - Artistic typography via discriminated and stylized diffusion.pdf"
}

@ARTICLE{He2021-xq,
  title     = "{Are You Looking at Me? Eye Gazing in Web Video Conferences}",
  author    = "He, Muchen and Xiong, Beibei and Xia, Kaseya",
  journal   = "Methods",
  publisher = "courses.ece.ubc.ca",
  volume    =  27,
  pages     =  28,
  abstract  = "… by the lack of eye contact due to the disparity between the
               position of the camera and the … This paper introduces a new way
               to achieve eye contact for multi-person teleconferencing. … With
               the gaze correction , it will create the illusion that everyone
               in this meeting is looking out …",
  year      =  2021,
  keywords  = "prj-gaze-design;uist2022-gaze-design",
  issn      = "1046-2023"
}

@ARTICLE{He2021-mp,
  title    = "{Are you looking at me? Eye gazing in web video conferences}",
  author   = "{He} and {Xiong} and {Xia}",
  journal  = "Methods",
  volume   =  27,
  pages    =  28,
  abstract = "… by the lack of eye contact due to the disparity between the
              position of the camera and the … This paper introduces a new way
              to achieve eye contact for multi-person teleconferencing. … With
              the gaze correction , it will create the illusion that everyone in
              this meeting is looking out …",
  year     =  2021,
  issn     = "1046-2023"
}

@ARTICLE{2020-qi,
  title    = "{Bubble Gaze Cursor：バブルカーソル法の視線操作への適用}",
  author   = "明根, 崔 and 大介, 坂本 and 哲雄, 小野",
  journal  = "情報処理学会論文誌",
  volume   =  61,
  number   =  2,
  pages    = "221--232",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  "15~" # feb,
  year     =  2020,
  url      = "https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=203141&item_no=1",
  file     = "All Papers/My Library/明根 et al. 2020 - Bubble Gaze Cursor：バブルカーソル法の視線操作への適用.pdf",
  issn     = "1882-7764",
  language = "ja"
}

@ARTICLE{Schneier2011-jd,
  title     = "{Fear and avoidance of eye contact in social anxiety disorder}",
  author    = "Schneier, F R and Rodebaugh, T L and Blanco, C and Lewin, H and
               {others}",
  journal   = "Comprehensive",
  publisher = "Elsevier",
  abstract  = "… Eye contact may trigger feelings of being scrutinized, and
               although eye contact is commonly feared in persons with social
               anxiety disorder, it … The purpose of this study was to
               characterize fear and avoidance of eye contact in patients with
               social anxiety disorder and in nonpatient …",
  year      =  2011,
  keywords  = "eye contact;telepresence"
}

@ARTICLE{Schneier2011-ev,
  title    = "{Fear and avoidance of eye contact in social anxiety disorder}",
  author   = "Schneier, F R and Rodebaugh, T L and Blanco, C and Lewin, H and
              {others}",
  journal  = "Comprehensive",
  abstract = "… Eye contact may trigger feelings of being scrutinized, and
              although eye contact is commonly feared in persons with social
              anxiety disorder, it … The purpose of this study was to
              characterize fear and avoidance of eye contact in patients with
              social anxiety disorder and in nonpatient …",
  year     =  2011,
  file     = "All Papers/My Library/Schneier et al. 2011 - Fear and avoidance of eye contact in social anxiety disorder.pdf"
}

@ARTICLE{Weiser2004-ao,
  title    = "{Designing calm technology}",
  author   = "Weiser, M",
  abstract = "Bits flowing through the wires of a computer network are
              ordinarily invisible. But a radically new tool shows those bits
              through motion, sound, and even touch. It communicates both light
              and heavy network traffic. Its output is so beautifully integrated
              with human information processing that one does not even need to
              be looking at it or near it to take advantage of its peripheral
              clues. It takes no space on your existing computer screen, and in
              fact does not use or contain a computer at all. It uses no
              software, only a few dollars in hardware, and can be shared by
              many people at the same time. It is called the ``Dangling
              String''.",
  year     =  2004,
  url      = "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fdc2e87fcb4575bdf5154840ebd19c2dd490165c",
  file     = "All Papers/Other/Weiser 2004 - Designing calm technology.pdf"
}

@ARTICLE{Qin2022-mq,
  title    = "{Learning-by-novel-view-synthesis for full-face appearance-based
              3D gaze estimation}",
  author   = "Qin, Jiawei and Shimoyama, Takuru and Sugano, Yusuke",
  abstract = "Despite recent advances in appearance-based gaze estimation
              techniques, the need for training data that covers the target head
              pose and gaze distribution remains a crucial challenge for
              practical deployment. This work examines a novel approach for
              synthesizing gaze estimation training data based on monocular 3D
              face reconstruction. Unlike prior works using multi-view
              reconstruction, photo-realistic CG models, or generative neural
              networks, our approach can manipulate and extend the head pose
              range of existing training data without any additional
              requirements. We introduce a projective matching procedure to
              align the reconstructed 3D facial mesh with the camera coordinate
              system and synthesize face images with accurate gaze labels. We
              also propose a mask-guided gaze estimation model and data
              augmentation strategies to further improve the estimation accuracy
              by taking advantage of synthetic training data. Experiments using
              multiple public datasets show that our approach significantly
              improves the estimation performance on challenging cross-dataset
              settings with non-overlapping gaze distributions.",
  month    =  "20~" # jan,
  year     =  2022,
  url      = "http://arxiv.org/abs/2201.07927",
  file     = "All Papers/My Library/Qin et al. 2022 - Learning-by-novel-view-synthesis for full-face appearance-based 3D gaze estimation.pdf"
}

@BOOK{Majaranta2012-lm,
  title     = "{Gaze Interaction and Applications of Eye Tracking: Advances in
               Assistive Technologies}",
  author    = "Majaranta, Päivi and Aoki, Hirotaka and Donegan, Mick and Hansen,
               Dan Witzner and Hansen, John Paulin and Hyrskykari, Aulikki and
               Räihä, Kari-Jouko",
  publisher = "IGI Global",
  year      =  2012,
  url       = "http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-61350-098-9",
  isbn      = "9781613500989,9781613500996"
}

@MISC{Kei_undated-fg,
  title        = "{Effects of highlighting a person's approach on social
                  telepresence}",
  author       = "Kei, Kat O and Nakanishi, Hideyuki and Ishiguro, Hiroshi",
  howpublished = "\url{http://www.interaction-ipsj.org/archives/paper2011/oral/0028/0028.pdf}",
  note         = "Accessed: 2021-10-13",
  file         = "All Papers/Other/Kei et al. - Effects of highlighting a person's approach on social telepresence.pdf",
  keywords     = "eye contact;telepresence"
}

@MISC{Kei_undated-cl,
  title  = "{Effects of highlighting a person's approach on social telepresence}",
  author = "Kei, Kat O and Nakanishi, Hideyuki and Ishiguro, Hiroshi",
  url    = "http://www.interaction-ipsj.org/archives/paper2011/oral/0028/0028.pdf",
  file   = "All Papers/My Library/Kei et al. - Effects of highlighting a person's approach on social telepresence.pdf",
  annote = "Accessed: 2021-10-13"
}

@ARTICLE{1999-ni,
  title     = "{{TVを介した「にらめっこ」} : アイコンタクト成立のための条件}",
  author    = "伊藤, 昭 and 松田, 健治 and 石垣, 誠 and 小嶋, 秀樹 and 矢野, 博之",
  journal   = "電子情報通信学会技術研究報告. HCS, ヒューマンコミュニケーション基礎",
  publisher = "一般社団法人電子情報通信学会",
  volume    =  98,
  number    =  503,
  pages     = "15--21",
  abstract  = "自然な対話においては, アイ・コンタクトは不可欠な要素である.しかしながら,
               これまでその役割はあまり良く調べられてこなかった.そこで我々は, アイ・コンタクトのとれる遠隔対話(TV電話)装置を開発し,
               人がどのくらいの精度で視線方向を検出できるのかを本装置を用いて調べてみた.その結果, 人は約4度の精度で視線方向を検出可能であり,
               これまでのTV電話(会議)システムでは視線が一致しないという印象を裏付た.次に, アイ・コンタクトの効果を調べるため,
               様々な条件下で睨めっこ実験を行なってみた.その結果は, 人は視線方向は検出できても,
               アイ・コンタクトが成立しているかどうかは判断できない, という意外なものであった. ``Eye-contact''seems to
               play an essential role for realizing a natural dialogue.Few
               researches have, however, investigated the real dialogue under
               eye-contact conditions.Hence, we developed a kind of TV
               conference system which an``eye-contact''possible between the
               dialoguer.First, the accuracy of the eye-direction detection is
               investigated, and found that only the mismatch of 4 degree is
               enough to be perceived.Next, Niramekko-Japanese outstaring game
               is conducted under various conditions, partly to investigate the
               effect of eye-contact on the dialoguer.The result is
               surprising-most people cannot detect the eye-contact, only they
               can detect the eye direction.",
  month     =  jan,
  year      =  1999,
  keywords  = "アイ・コンタクト;TV電話;睨めっこ;eye contact;telepresence"
}

@ARTICLE{1999-rw,
  title    = "{TVを介した「にらめっこ」 : アイコンタクト成立のための条件}",
  author   = "{伊藤} and {松田} and {石垣} and {小嶋} and {矢野}",
  journal  = "電子情報通信学会技術研究報告. HCS, ヒューマンコミュニケーション基礎",
  volume   =  98,
  number   =  503,
  pages    = "15–21",
  abstract = "自然な対話においては, アイ・コンタクトは不可欠な要素である.しかしながら,
              これまでその役割はあまり良く調べられてこなかった.そこで我々は, アイ・コンタクトのとれる遠隔対話(TV電話)装置を開発し,
              人がどのくらいの精度で視線方向を検出できるのかを本装置を用いて調べてみた.その結果, 人は約4度の精度で視線方向を検出可能であり,
              これまでのTV電話(会議)システムでは視線が一致しないという印象を裏付た.次に, アイ・コンタクトの効果を調べるため,
              様々な条件下で睨めっこ実験を行なってみた.その結果は, 人は視線方向は検出できても,
              アイ・コンタクトが成立しているかどうかは判断できない, という意外なものであった. “Eye-contact”seems to
              play an essential role for realizing a natural dialogue.Few
              researches have, however, investigated the real dialogue under
              eye-contact conditions.Hence, we developed a kind of TV conference
              system which an“eye-contact”possible between the dialoguer.First,
              the accuracy of the eye-direction detection is investigated, and
              found that only the mismatch of 4 degree is enough to be
              perceived.Next, Niramekko-Japanese outstaring game is conducted
              under various conditions, partly to investigate the effect of
              eye-contact on the dialoguer.The result is surprising-most people
              cannot detect the eye-contact, only they can detect the eye
              direction.",
  month    =  jan,
  year     =  1999
}

@ARTICLE{Blascheck_undated-ig,
  title    = "{State-of-the-Art of Visualization for Eye Tracking Data}",
  author   = "Blascheck, T and Kurzhals, K and Raschke, M and Burch, M and
              Weiskopf, D and Ertl, T",
  abstract = "Eye tracking technology is becoming easier and cheaper to use,
              resulting in its increasing application to numerous ﬁelds of
              research. The data collected during an eye tracking experiment can
              be analyzed by statistical methods and/or with visualization
              techniques. Visualizations can reveal characteristics of ﬁxations,
              saccades, and scanpath structures. In this survey, we present an
              overview of visualization techniques for eye tracking data and
              describe their functionality. We classify the visualization
              techniques using nine categories. The categories are based on
              properties of eye tracking data, including aspects of the stimuli
              and the viewer, and on properties of the visualization techniques.
              The classiﬁcation of about 90 publications including technical as
              well as application papers with modiﬁcations of common
              visualization techniques are described in more detail. We ﬁnally
              present possible directions for further research in the ﬁeld of
              eye tracking data visualization.",
  language = "en"
}

@ARTICLE{Weyl2022-vj,
  title    = "{Decentralized Society: Finding Web3's Soul}",
  author   = "Weyl, E Glen and Ohlhaver, Puja and Buterin, Vitalik",
  abstract = "Web3 today centers around expressing transferable, financialized
              assets, rather than encoding social relationships of trust. Yet
              many core economic activities---such as uncollateralized lending
              and building personal brands---are built on persistent,
              non-transferable relationships. In this paper, we illustrate how
              non-transferable ``soulbound'' tokens (SBTs) representing the
              commitments, credentials, and affiliations of ``Souls'' can encode
              the trust networks of the real economy to establish provenance and
              reputation. More importantly, SBTs enable other applications of
              increasing ambition, such as community wallet recovery,
              sybil-resistant governance, mechanisms for decentralization, and
              novel markets with decomposable, shared rights. We call this
              richer, pluralistic ecosystem ``Decentralized Society''
              (DeSoc)---a co-determined sociality, where Souls and communities
              come together bottom-up, as emergent properties of each other to
              co-create plural network goods and intelligences, at a range of
              scales. Key to this sociality is decomposable property rights and
              enhanced governance mechanisms---such as quadratic funding
              discounted by correlation scores---that reward trust and
              cooperation while protecting networks from capture, extraction,
              and domination. With such augmented sociality, web3 can eschew
              today's hyper-financialization in favor of a more transformative,
              pluralist future of increasing returns across social distance.",
  month    =  may,
  year     =  2022,
  file     = "All Papers/Other/Weyl et al. 2022 - Decentralized Society - Finding Web3's Soul.pdf",
  keywords = "Web3, pluralism, blockchains, soulbound tokens, NFTs"
}

@ARTICLE{Weyl2022-be,
  title    = "{Decentralized society: Finding web3's soul}",
  author   = "Weyl, E Glen and Ohlhaver, Puja and Buterin, Vitalik",
  abstract = "Web3 today centers around expressing transferable, financialized
              assets, rather than encoding social relationships of trust. Yet
              many core economic activities—such as uncollateralized lending and
              building personal brands—are built on persistent, non-transferable
              relationships. In this paper, we illustrate how non-transferable
              “soulbound” tokens (SBTs) representing the commitments,
              credentials, and affiliations of “Souls” can encode the trust
              networks of the real economy to establish provenance and
              reputation. More importantly, SBTs enable other applications of
              increasing ambition, such as community wallet recovery,
              sybil-resistant governance, mechanisms for decentralization, and
              novel markets with decomposable, shared rights. We call this
              richer, pluralistic ecosystem “Decentralized Society” (DeSoc)—a
              co-determined sociality, where Souls and communities come together
              bottom-up, as emergent properties of each other to co-create
              plural network goods and intelligences, at a range of scales. Key
              to this sociality is decomposable property rights and enhanced
              governance mechanisms—such as quadratic funding discounted by
              correlation scores—that reward trust and cooperation while
              protecting networks from capture, extraction, and domination. With
              such augmented sociality, web3 can eschew today's
              hyper-financialization in favor of a more transformative,
              pluralist future of increasing returns across social distance.",
  month    =  may,
  year     =  2022,
  file     = "All Papers/My Library/Weyl et al. 2022 - Decentralized society - Finding web3's soul.pdf"
}

@ARTICLE{2012-vs,
  title    = "{「実体」は存在感を強化するか:ロボット会議か：ビデオ会議を代替する可能性}",
  author   = "一晶, 田中 and 健太, 山本 and 聡, 尾上 and 英之, 中西 and 浩, 石黒",
  journal  = "研究報告 ヒューマンコンピュータインタラクション（HCI）",
  volume   = "2012-HCI-148",
  number   =  7,
  pages    = "1--8",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  may,
  year     =  2012,
  keywords = "人・生命との融合;eye contact;telepresence"
}

@ARTICLE{2012-ci,
  title    = "{「実体」は存在感を強化するか:ロボット会議か：ビデオ会議を代替する可能性}",
  author   = "一晶, 田中 and 健太, 山本 and 聡, 尾上 and 英之, 中西 and 浩, 石黒",
  journal  = "研究報告 ヒューマンコンピュータインタラクション（HCI）",
  volume   = "2012-HCI-148",
  number   =  7,
  pages    = "1–8",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  may,
  year     =  2012
}

@TECHREPORT{2016-wh,
  title     = "{テレビ会議話者間の視線一致知覚範囲を考慮した目領域画像合成型視線補正法}",
  author    = "井上, 卓弥 and 高橋, 友和 and 平山, 高嗣 and 川西, 康友 and 出口, 大輔 and 井手, 一郎 and
               村瀬, 洋 and 黒住, 隆行 and 柏野, 邦夫",
  publisher = "電子情報通信学会",
  volume    =  115,
  pages     = "53--58",
  month     =  jan,
  year      =  2016,
  keywords  = "テレビ会議; 視線一致; 注視点分類; video conferencing; eye-contact; gaze
               classification;eye contact;telepresence",
  language  = "ja"
}

@TECHREPORT{2016-ou,
  title       = "{テレビ会議話者間の視線一致知覚範囲を考慮した目領域画像合成型視線補正法}",
  author      = "井上, 卓弥 and 高橋, 友和 and 平山, 高嗣 and 川西, 康友 and 出口, 大輔 and 井手, 一郎
                 and 村瀬, 洋 and 黒住, 隆行 and 柏野, 邦夫",
  institution = "電子情報通信学会",
  pages       = "53–58",
  month       =  jan,
  year        =  2016,
  language    = "ja"
}

@ARTICLE{ThanyaditSantawat2018-en,
  title    = "{Efficient Information Sharing Techniques between Workers of
              Heterogeneous Tasks in {3D} {CVE}}",
  author   = "{ThanyaditSantawat} and {PunpongsanonParinya} and {PongTing-Chuen}",
  journal  = "undefined",
  abstract = "This research presents a novel approach called ``SmartGlass'',
              which automates the very labor-intensive and therefore time-heavy
              and expensive process of manually cataloging and cataloging
              individual components of a 3D model. Collaboration between a
              helper and a worker in a 3D collaborative virtual environment
              usually requires real-time information sharing, since the worker
              relies on the timely assistance from the helpe...",
  year     =  2018,
  language = "en"
}

@ARTICLE{Argyle1976-bt,
  title    = "{Gaze and mutual gaze}",
  author   = "Argyle, Michael and Cook, Mark",
  volume   =  210,
  abstract = "Reviews and interprets research findings in the use of the eyes in
              social behavior. Among the topics discussed are gaze patterns in
              animals and their development in children, the link between gaze
              and speech, deviant patterns of gaze in psychiatric cases,
              cross-cultural differences, and gaze as a signal for interpersonal
              attitudes and emotions. (21 p ref) (PsycINFO Database Record (c)
              2016 APA, all rights reserved)",
  year     =  1976,
  url      = "https://psycnet.apa.org/fulltext/1976-11825-000.pdf"
}

@ARTICLE{Komine_undated-wq,
  title    = "{Proposal and evaluation for the interactive video
              teleconferencing system with the eye contact}",
  author   = "Komine, Takahiro and Katsumoto, Michiaki and Tan, Yasuo",
  keywords = "eye contact;telepresence"
}

@ARTICLE{Komine_undated-zp,
  title  = "{Proposal and evaluation for the interactive video teleconferencing
            system with the eye contact}",
  author = "Komine, Takahiro and Katsumoto, Michiaki and Tan, Yasuo"
}

@TECHREPORT{Aung2018-uh,
  title       = "{Who Are They Looking At? Automatic Eye Gaze Following for
                 Classroom Observation Video Analysis}",
  author      = "Aung, Arkar Min and Ramakrishnan, Anand and Whitehill, Jacob R",
  institution = "International Educational Data Mining Society",
  abstract    = "We develop an end-to-end neural network-based computer vision
                 system to automatically identify ``where'' each person within a
                 2-D image of a school classroom is looking (``gaze
                 following''), as well as ``who'' she/he is looking at.
                 Automatic gaze following could help facilitate data-mining of
                 large datasets of ``classroom observation'' videos that are
                 collected routinely in schools around the world in order to
                 understand social interactions between teachers and students.
                 Our network is based on the architecture by [27] but is
                 extended to predict whether each person is looking at a target
                 inside or outside the image; and to predict not only where, but
                 who the person is looking at. Moreover, since our focus is on
                 classroom observation videos, we collected a dataset from
                 scratch of publicly available classroom sessions from 70
                 YouTube videos and collected labels from 408 labelers who
                 annotated a total of 17,758 gazes in 2, 263 unique image
                 frames. Results of our experiments indicate that the proposed
                 neural network can estimate the gaze target -- either the
                 spatial location or the face of a person -- with substantially
                 higher accuracy compared to several baselines. [For the full
                 proceedings, see ED593090.]",
  month       =  jul,
  year        =  2018,
  url         = "https://eric.ed.gov/?id=ED593195",
  file        = "All Papers/My Library/Aung et al. 2018 - Who Are They Looking At - Automatic Eye Gaze Following for Classroom Observation Video Analysis.pdf",
  language    = "en"
}

@ARTICLE{Isbister1999-aa,
  title    = "{ソーシャルインタラクション：サイバー空間での社会的インタラクションのための設計}",
  author   = "{Isbister}",
  journal  = "情報処理",
  volume   =  40,
  number   =  6,
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jun,
  year     =  1999
}

@ARTICLE{2009-ih,
  title    = "{テレビ会議において視線の伝達が話者交替に及ぼす影響の分析}",
  author   = "圭亮, 西村 and 晃嗣, 上野 and 創吾, 坪井 and 信宏, 下郡",
  journal  = "研究報告グループウェアとネットワークサービス（GN）",
  volume   =  2009,
  number   = "33(2009-GN-71)",
  pages    = "163--168",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  mar,
  year     =  2009,
  keywords = "READ;eye contact;telepresence"
}

@ARTICLE{2009-ji,
  title    = "{テレビ会議において視線の伝達が話者交替に及ぼす影響の分析}",
  author   = "{圭亮} and {晃嗣} and {創吾} and {信宏}",
  journal  = "研究報告グループウェアとネットワークサービス（GN）",
  volume   =  2009,
  number   = "33(2009-GN-71)",
  pages    = "163–168",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  mar,
  year     =  2009
}

@BOOK{Kaur2021-ir,
  title     = "{2021 IEEE winter conference on applications of computer vision
               (WACV)}",
  author    = "Kaur, Harsimran and Manduchi, Roberto",
  publisher = "IEEE",
  abstract  = "Author(s): Kaur, Harsimran; Manduchi, Roberto | Abstract: We
               propose a method for synthesizing eye images from segmentation
               masks with a desired style. The style encompasses attributes such
               as skin color, texture, iris color, and personal identity. Our
               approach generates an eye image that is consistent with a given
               segmentation mask and has the attributes of the input style
               image. We apply our method to data augmentation as well as to
               gaze redirection. The previous techniques of synthesizing real
               eye images from synthetic eye images for data augmentation lacked
               control over the generated attributes. We demonstrate the
               effectiveness of the proposed method in synthesizing realistic
               eye images with given characteristics corresponding to the
               synthetic labels for data augmentation, which is further useful
               for various tasks such as gaze estimation, eye image
               segmentation, pupil detection, etc. We also show how our approach
               can be applied to gaze redirection using only synthetic gaze
               labels, improving the previous state of the art results. The main
               contributions of our paper are i) a novel approach for
               Style-Based eye image generation from segmentation mask; ii) the
               use of this approach for gaze-redirection without the need for
               gaze annotated real eye images.",
  year      =  2021,
  url       = "https://play.google.com/store/books/details?id=NAmJzgEACAAJ",
  file      = "All Papers/My Library/Kaur and Manduchi 2021 - 2021 IEEE winter conference on applications of computer vision (WACV).pdf",
  isbn      =  9781665404778,
  language  = "en"
}

@ARTICLE{1999-tw,
  title     = "{文殊の知恵システム : 意志決定可能な視線一致型テレビ会議システム}",
  author    = "青木, 輝勝 and ウィドヨ, クスタルト and 坂本, 信樹 and 鈴木, 一徳 and 佐分, 淑樹 and 安田, 浩",
  journal   = "電子情報通信学会技術研究報告. IE, 画像工学",
  publisher = "一般社団法人電子情報通信学会",
  volume    =  98,
  number    =  552,
  pages     = "9--14",
  abstract  = "With the recent evolutionary progress in network and terminal
               technologies, current videoconference systems are in the level of
               practical use. The existing systems, however, would not be fully
               useful for decision making process since eyes of users can not
               always be in contact that makes it difficult to read the
               expression of partipants.In this paper, we discuss the principle
               and the impact of ``MONJU no CHIE System'', a videoconference
               system with eye contact using a special display device called
               ``Glass Vision''.",
  month     =  jan,
  year      =  1999,
  keywords  = "eye contact;telepresence"
}

@ARTICLE{1999-zq,
  title    = "{文殊の知恵システム : 意志決定可能な視線一致型テレビ会議システム}",
  author   = "{青木} and {ウィドヨ} and {坂本} and {鈴木} and {佐分} and {安田}",
  journal  = "電子情報通信学会技術研究報告. IE, 画像工学",
  volume   =  98,
  number   =  552,
  pages    = "9–14",
  abstract = "With the recent evolutionary progress in network and terminal
              technologies, current videoconference systems are in the level of
              practical use. The existing systems, however, would not be fully
              useful for decision making process since eyes of users can not
              always be in contact that makes it difficult to read the
              expression of partipants.In this paper, we discuss the principle
              and the impact of “MONJU no CHIE System”, a videoconference system
              with eye contact using a special display device called “Glass
              Vision”.",
  month    =  jan,
  year     =  1999
}

@ARTICLE{2006-yc,
  title    = "{[ポスター講演] テレビ電話におけるアイコンタクト効果の検討}",
  author   = "千紗, 吉富 and 信生, 中嶋",
  journal  = "情報処理学会研究報告オーディオビジュアル複合情報処理（AVM）",
  volume   =  2006,
  number   = "79(2006-AVM-053)",
  pages    = "53--56",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jul,
  year     =  2006,
  keywords = "eye contact;telepresence"
}

@ARTICLE{2006-du,
  title    = "{[ポスター講演] テレビ電話におけるアイコンタクト効果の検討}",
  author   = "{千紗} and {信生}",
  journal  = "情報処理学会研究報告オーディオビジュアル複合情報処理（AVM）",
  volume   =  2006,
  number   = "79(2006-AVM-053)",
  pages    = "53–56",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jul,
  year     =  2006
}

@ARTICLE{Cohen2017-fg,
  title    = "{“Twhirleds”: Spun and whirled affordances controlling multimodal
              mobile-ambient environments with reality distortion and
              synchronized lighting to preserve …}",
  author   = "{Cohen} and {Ranaweera} and {Ryskeldiev} and {others}",
  journal  = "Scientific phone apps and mobile devices",
  abstract = "The popularity of the contemporary smartphone makes it an
              attractive platform for new applications. We are exploring the
              potential of such personal devices to control networked …",
  year     =  2017,
  file     = "All Papers/My Library/Cohen et al. 2017 - “Twhirleds” - Spun and whirled affordances controlli ... ments with reality distortion and synchronized lighting to preserve ….pdf",
  issn     = "2364-4958"
}

@ARTICLE{2016-fe,
  title     = "{会話場面における視線行動と満足度および印象評価の検討}",
  author    = "土屋, 裕希乃 and Yukino, Tsuchiya",
  journal   = "国際経営・文化研究 = Cross-cultural business and cultural studies :
               国際コミュニケーション学会誌",
  publisher = "国際コミュニケーション学会",
  volume    =  21,
  number    =  1,
  pages     = "153--162",
  abstract  = "The purpose of this study is to examine the relation between gaze
               behavior and impression in the conversation by using an eye mark
               recorder. 71 undergraduates were randomly selected in two groups
               including blankness and smile. Both group participants watched
               two videos. And they were asked to talk with a target person
               appeared on a monitor. The target person's direction of eyes and
               facial expression were manipulated. The results indicated that
               blankness group more gazed at target person's eye when the target
               person gazed participant. Moreover, both groups significantly
               increased satisfaction with the conversation and interpersonal
               impressions. And, Smile group were indicated higher interpersonal
               impressions than blankness group when the target person's gaze at
               participants. The results show that gaze behavior and facial
               expression makes positive effects in face to face interaction.",
  year      =  2016,
  keywords  = "視線行動;アイコンタクト;スマイル;印象評価;アイマークレコーダー;eye contact",
  issn      = "1343-1412"
}

@ARTICLE{2016-io,
  title    = "{会話場面における視線行動と満足度および印象評価の検討}",
  author   = "{土屋} and {Yukino}",
  journal  = "国際経営・文化研究 = Cross-cultural business and cultural studies :
              国際コミュニケーション学会誌",
  volume   =  21,
  number   =  1,
  pages    = "153–162",
  abstract = "The purpose of this study is to examine the relation between gaze
              behavior and impression in the conversation by using an eye mark
              recorder. 71 undergraduates were randomly selected in two groups
              including blankness and smile. Both group participants watched two
              videos. And they were asked to talk with a target person appeared
              on a monitor. The target person's direction of eyes and facial
              expression were manipulated. The results indicated that blankness
              group more gazed at target person's eye when the target person
              gazed participant. Moreover, both groups significantly increased
              satisfaction with the conversation and interpersonal impressions.
              And, Smile group were indicated higher interpersonal impressions
              than blankness group when the target person's gaze at
              participants. The results show that gaze behavior and facial
              expression makes positive effects in face to face interaction.",
  year     =  2016,
  issn     = "1343-1412"
}

@ARTICLE{2007-ou,
  title    = "{タイルドディスプレイを用いた多地点遠隔コミュニケーションシステムに関する研究}",
  author   = "{尚久} and {康生} and {小山田耕二}",
  journal  = "情報処理学会研究報告コンピュータビジョンとイメージメディア（CVIM）",
  volume   =  2007,
  number   = "87(2007-CVIM-160)",
  pages    = "1–6",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  sep,
  year     =  2007
}

@ARTICLE{Jiang2022-gz,
  title    = "{Ditto: Building digital twins of articulated objects from
              interaction}",
  author   = "Jiang, Zhenyu and Hsu, Cheng-Chun and Zhu, Yuke",
  journal  = "arXiv [cs.CV]",
  abstract = "Digitizing physical objects into the virtual world has the
              potential to unlock new research and applications in embodied AI
              and mixed reality. This work focuses on recreating interactive
              digital twins of real-world articulated objects, which can be
              directly imported into virtual environments. We introduce Ditto to
              learn articulation model estimation and 3D geometry reconstruction
              of an articulated object through interactive perception. Given a
              pair of visual observations of an articulated object before and
              after interaction, Ditto reconstructs part-level geometry and
              estimates the articulation model of the object. We employ implicit
              neural representations for joint geometry and articulation
              modeling. Our experiments show that Ditto effectively builds
              digital twins of articulated objects in a category-agnostic way.
              We also apply Ditto to real-world objects and deploy the recreated
              digital twins in physical simulation. Code and additional results
              are available at https://ut-austin-rpl.github.io/Ditto",
  month    =  feb,
  year     =  2022,
  url      = "http://arxiv.org/abs/2202.08227",
  file     = "All Papers/My Library/Jiang et al. 2022 - Ditto - Building digital twins of articulated objects from interaction.pdf"
}

@ARTICLE{2001-ka,
  title    = "{視線インタフェースから視線コミュニケーションへ －視線のある環境を目指して－}",
  author   = "健彦, 大野",
  journal  = "情報処理学会研究報告コンピュータビジョンとイメージメディア（CVIM）",
  volume   =  2001,
  number   = "87(2001-CVIM-129)",
  pages    = "171--178",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  sep,
  year     =  2001
}

@ARTICLE{2001-hz,
  title    = "{視線インタフェースから視線コミュニケーションへ －視線のある環境を目指して－}",
  author   = "{健彦}",
  journal  = "情報処理学会研究報告コンピュータビジョンとイメージメディア（CVIM）",
  volume   =  2001,
  number   = "87(2001-CVIM-129)",
  pages    = "171–178",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  sep,
  year     =  2001
}

@ARTICLE{Ryskeldiev2018-xe,
  title    = "{Spatial social media: Towards collaborative mixed reality
              telepresence “On the go”}",
  author   = "{Ryskeldiev}",
  journal  = "Extended Abstracts of the 2018 CHI Conference on",
  abstract = "Live video streaming is becoming an increasingly popular form of
              interaction in social media, with mobile devices being used for
              sharing of remote situation“ on the go.” We are …",
  year     =  2018
}

@ARTICLE{2010-pk,
  title    = "{距離感と抵抗感の減少を目指したフープ型ビデオチャットシステム}",
  author   = "{真吾}",
  journal  = "研究報告グループウェアとネットワークサービス（GN）",
  volume   = "2010-GN-74",
  number   =  13,
  pages    = "1–6",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jan,
  year     =  2010
}

@ARTICLE{2015-rv,
  title    = "{多地点マルチメディア会議システムにおける視線共有方法の検討}",
  author   = "{雅文} and {範高}",
  journal  = "第77回全国大会講演論文集",
  volume   =  2015,
  number   =  1,
  pages    = "171–172",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  mar,
  year     =  2015
}

@ARTICLE{Hessels2018-jk,
  title    = "{Eye contact takes two–autistic and social anxiety traits predict
              gaze behavior in dyadic interaction}",
  author   = "{Hessels} and {Holleman} and {others}",
  journal  = "Journal of physics. B, Atomic, molecular, and optical physics: an
              Institute of Physics journal",
  abstract = "… While two-way gaze is sometimes referred to as ` eye contact ',
              and no eye gaze as `averted gaze', we refrain from using these
              terms, as it may imply something special or intentional about
              these gaze states. For all three gaze states, the frequency of
              occurrences, the mean …",
  year     =  2018,
  file     = "All Papers/My Library/Hessels et al. 2018 - Eye contact takes two–autistic and social anxiety traits predict gaze behavior in dyadic interaction.pdf",
  issn     = "0953-4075"
}

@ARTICLE{2010-uy,
  title    = "{距離感と抵抗感の減少を目指したフープ型ビデオチャットシステム}",
  author   = "真吾, 藤田 and 孝, 吉野",
  journal  = "研究報告グループウェアとネットワークサービス（GN）",
  volume   = "2010-GN-74",
  number   =  13,
  pages    = "1--6",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jan,
  year     =  2010,
  keywords = "eye contact;telepresence"
}

@ARTICLE{Hafner2023-wg,
  title    = "{Mastering Diverse Domains through World Models}",
  author   = "Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap,
              Timothy",
  abstract = "General intelligence requires solving tasks across many domains.
              Current reinforcement learning algorithms carry this potential but
              are held back by the resources and knowledge required to tune them
              for new tasks. We present DreamerV3, a general and scalable
              algorithm based on world models that outperforms previous
              approaches across a wide range of domains with fixed
              hyperparameters. These domains include continuous and discrete
              actions, visual and low-dimensional inputs, 2D and 3D worlds,
              different data budgets, reward frequencies, and reward scales. We
              observe favorable scaling properties of DreamerV3, with larger
              models directly translating to higher data-efficiency and final
              performance. Applied out of the box, DreamerV3 is the first
              algorithm to collect diamonds in Minecraft from scratch without
              human data or curricula, a long-standing challenge in artificial
              intelligence. Our general algorithm makes reinforcement learning
              broadly applicable and allows scaling to hard decision-making
              problems.",
  month    =  "10~" # jan,
  year     =  2023,
  url      = "http://arxiv.org/abs/2301.04104",
  file     = "All Papers/My Library/Hafner et al. 2023 - Mastering Diverse Domains through World Models.pdf",
  annote   = "Comment: Website: https://danijar.com/dreamerv3"
}

@ARTICLE{2005-ns,
  title     = "{複数人の映像会話における空間の整合性と人の補償行動 : ノンバーバル情報のコミュニケーションにおける役割}",
  author    = "新井, 久美子 and 武川, 直樹 and 田口, 晶洋 and 湯浅, 将英",
  journal   = "電子情報通信学会技術研究報告. HCS, ヒューマンコミュニケーション基礎",
  publisher = "一般社団法人電子情報通信学会",
  volume    =  105,
  number    =  385,
  pages     = "75--80",
  abstract  = "対面のコミュニケーション(Face-to-face Communication)と同様,
               映像を介した会話システム(Video-mediated Communication :
               VMC)においても視線や顔向きなどのノンバーバル情報は重要な役割を果たす.しかし,
               現状普及しているテレビ会議システムなどにおいては,
               装置構成上の制限からこれらの情報が正確に伝えられない.特に多人数が参加する映像会議システムにおいては,
               (1)だれがだれに対して発言しているのか, (2)相手と相互に見つめあっている状態(アイコンタクト)か,
               の情報がわかりにくいためコミュニケーションに問題を生じることがある.本研究は映像対話におけるこのような空間の整合性の問題の重要性を明らかにし,
               システムの設計基準の根拠を与えることを目標として, ユーザの行動分析を行う.そのため,
               多人数の最小構成である3人用のVMC模擬システムを構成し, 相手との視線一致, 相手と視線不一致,
               話したい相手とは異なる人物との視線一致, の3種の空間整合・不整合状態を作り出し, 被験者による会話実験を通じて,
               ユーザの行動分析, 比較を行った.実験の結果, 空間不整合のときのユーザの確認行動, 補償行動が確認された. Nonverbal
               cues such as gaze and head orientation play an important role in
               video-mediated Communication as well as face-to-face
               communication. Existing video conferencing system available in a
               marketplace, however, cannot convey such visual cues correctly.
               Above all, multiparty communication systems, where visual cues :
               (1) ``Who speaks to whom?'' (2) ``Do any of them gaze each
               other?'' are not properly recognized by participants, may produce
               misunderstanding in communication. This study aims to clarify the
               importance of spatial consistency in visual communication space,
               and to give a design basis for the systems by analyzing users'
               behaviors. For that purpose we develop a VMC simulation system
               for three persons as a minimum configuration of multiparty VMC
               systems in which three spatial consistency/inconsistency
               situations can be chosen : (1) Eye-contact system, where each
               participant can be mutually gazed, (2) Non-eye-contact system,
               where any of the participants cannot be mutually gazed, and (3)
               Wrong-eye-contact, where person A's gaze to B is conveyed to
               person C. These three spatial configurations are compared and
               evaluated through the users' behaviors in the experiments. The
               experimental results show that typical user's behaviors which can
               be classified into confirmation-and compensation behaviors.",
  month     =  nov,
  year      =  2005,
  keywords  = "映像会話;空間の整合性;ノンバーバル情報;ユーザの行動;視線;eye contact;telepresence",
  issn      = "0913-5685"
}

@ARTICLE{2005-of,
  title    = "{複数人の映像会話における空間の整合性と人の補償行動 : ノンバーバル情報のコミュニケーションにおける役割}",
  author   = "{新井} and {武川} and {田口} and {湯浅}",
  journal  = "電子情報通信学会技術研究報告. HCS, ヒューマンコミュニケーション基礎",
  volume   =  105,
  number   =  385,
  pages    = "75–80",
  abstract = "対面のコミュニケーション(Face-to-face Communication)と同様,
              映像を介した会話システム(Video-mediated Communication :
              VMC)においても視線や顔向きなどのノンバーバル情報は重要な役割を果たす.しかし,
              現状普及しているテレビ会議システムなどにおいては,
              装置構成上の制限からこれらの情報が正確に伝えられない.特に多人数が参加する映像会議システムにおいては,
              (1)だれがだれに対して発言しているのか, (2)相手と相互に見つめあっている状態(アイコンタクト)か,
              の情報がわかりにくいためコミュニケーションに問題を生じることがある.本研究は映像対話におけるこのような空間の整合性の問題の重要性を明らかにし,
              システムの設計基準の根拠を与えることを目標として, ユーザの行動分析を行う.そのため,
              多人数の最小構成である3人用のVMC模擬システムを構成し, 相手との視線一致, 相手と視線不一致,
              話したい相手とは異なる人物との視線一致, の3種の空間整合・不整合状態を作り出し, 被験者による会話実験を通じて,
              ユーザの行動分析, 比較を行った.実験の結果, 空間不整合のときのユーザの確認行動, 補償行動が確認された. Nonverbal
              cues such as gaze and head orientation play an important role in
              video-mediated Communication as well as face-to-face
              communication. Existing video conferencing system available in a
              marketplace, however, cannot convey such visual cues correctly.
              Above all, multiparty communication systems, where visual cues :
              (1) “Who speaks to whom?” (2) “Do any of them gaze each other?”
              are not properly recognized by participants, may produce
              misunderstanding in communication. This study aims to clarify the
              importance of spatial consistency in visual communication space,
              and to give a design basis for the systems by analyzing users'
              behaviors. For that purpose we develop a VMC simulation system for
              three persons as a minimum configuration of multiparty VMC systems
              in which three spatial consistency/inconsistency situations can be
              chosen : (1) Eye-contact system, where each participant can be
              mutually gazed, (2) Non-eye-contact system, where any of the
              participants cannot be mutually gazed, and (3) Wrong-eye-contact,
              where person A's gaze to B is conveyed to person C. These three
              spatial configurations are compared and evaluated through the
              users' behaviors in the experiments. The experimental results show
              that typical user's behaviors which can be classified into
              confirmation-and compensation behaviors.",
  month    =  nov,
  year     =  2005,
  issn     = "0913-5685"
}

@ARTICLE{Intille_undated-js,
  title    = "{Concept and Partial Prototype Video: Ubiquitous Video
              Communication with the Perception of Eye Contact}",
  author   = "Intille, Emmanuel Munguia Tapia Stephen S and Stoddard, John
              Rebula Steve",
  journal  = "ccs. neu. edu",
  abstract = "This concept and partial prototype video introduces a strategy for
              creating a video conferencing system for future ubiquitous
              computing environments that can guarantee two remote conversants
              the ability to establish eye contact. Unlike prior work, eye
              contact can be achieved even as people move about their respective
              environments engaging in everyday tasks."
}

@ARTICLE{Intille_undated-bo,
  title    = "{Concept and partial prototype video: Ubiquitous video
              communication with the perception of eye contact}",
  author   = "Intille, Emmanuel Munguia Tapia Stephen S and Stoddard, John
              Rebula Steve",
  journal  = "ccs. neu. edu",
  abstract = "This concept and partial prototype video introduces a strategy for
              creating a video conferencing system for future ubiquitous
              computing environments that can guarantee two remote conversants
              the ability to establish eye contact. Unlike prior work, eye
              contact can be achieved even as people move about their respective
              environments engaging in everyday tasks."
}

@ARTICLE{1999-gn,
  title     = "{Hough 変換を用いた瞳認識とアイコンタクトする顔映像生成について}",
  author    = "山口, 剛 and 冨永, 将史 and 輿水, 大和 and 村上, 和人",
  journal   = "電子情報通信学会技術研究報告. PRMU, パターン認識・メディア理解",
  publisher = "一般社団法人電子情報通信学会",
  volume    =  99,
  number    =  449,
  pages     = "21--28",
  abstract  = "
               本論文では、TV会議における自然なヒューマンインターフェイスを実現する新しい手法について述べる。TV会議において人は、TVカメラではなく、TVモニタを見てしまいがちである。そのため、筆者らはそれを実現するために、まず初めに入力された画像からHough変換を用いて瞳を抽出する方法、そしてTV会議における視線を変化させる方法について提案する。また、若干の予備的実験によりその有効性を示す。
               This paper proposed a new methodology to realize a natural human
               interface of face-to-face communication on the TV conference
               environment. Since human is likely to look at the face of his
               partner of his partner on TV monitor not at TV camera, he will
               usually fail to send his own eye-contact facial images to him.
               The basic idea to improve this fatal communication situation is
               to regenerate facial image by chaning the direction of the irises
               in the same original facial image, We proposed firstly a method
               to recognize the irises from the input image by using Hough
               transform of circle detection and then proposed a method to
               change the direction of irises in such a way that his face could
               be eye-contacted to his partner on the TV conference system. The
               proposed methods and some experimental results are presented to
               demonstrate the effectivity and the coming inherent subjects.",
  month     =  nov,
  year      =  1999,
  keywords  = "円の Hough 変換;瞳認識;TV会議;eye contact;telepresence",
  issn      = "0913-5685"
}

@ARTICLE{1999-ol,
  title    = "{Hough 変換を用いた瞳認識とアイコンタクトする顔映像生成について}",
  author   = "{山口} and {冨永} and {輿水} and {村上}",
  journal  = "電子情報通信学会技術研究報告. PRMU, パターン認識・メディア理解",
  volume   =  99,
  number   =  449,
  pages    = "21–28",
  abstract = "
              本論文では、TV会議における自然なヒューマンインターフェイスを実現する新しい手法について述べる。TV会議において人は、TVカメラではなく、TVモニタを見てしまいがちである。そのため、筆者らはそれを実現するために、まず初めに入力された画像からHough変換を用いて瞳を抽出する方法、そしてTV会議における視線を変化させる方法について提案する。また、若干の予備的実験によりその有効性を示す。
              This paper proposed a new methodology to realize a natural human
              interface of face-to-face communication on the TV conference
              environment. Since human is likely to look at the face of his
              partner of his partner on TV monitor not at TV camera, he will
              usually fail to send his own eye-contact facial images to him. The
              basic idea to improve this fatal communication situation is to
              regenerate facial image by chaning the direction of the irises in
              the same original facial image, We proposed firstly a method to
              recognize the irises from the input image by using Hough transform
              of circle detection and then proposed a method to change the
              direction of irises in such a way that his face could be
              eye-contacted to his partner on the TV conference system. The
              proposed methods and some experimental results are presented to
              demonstrate the effectivity and the coming inherent subjects.",
  month    =  nov,
  year     =  1999,
  issn     = "0913-5685"
}

@ARTICLE{Tevet2022-oj,
  title    = "{Human motion diffusion model}",
  author   = "Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan
              and Bermano, Amit H and Cohen-Or, Daniel",
  abstract = "Natural and expressive human motion generation is the holy grail
              of computer animation. It is a challenging task, due to the
              diversity of possible motion, human perceptual sensitivity to it,
              and the difficulty of accurately describing it. Therefore, current
              generative solutions are either low-quality or limited in
              expressiveness. Diffusion models, which have already shown
              remarkable generative capabilities in other domains, are promising
              candidates for human motion due to their many-to-many nature, but
              they tend to be resource hungry and hard to control. In this
              paper, we introduce Motion Diffusion Model (MDM), a carefully
              adapted classifier-free diffusion-based generative model for the
              human motion domain. MDM is transformer-based, combining insights
              from motion generation literature. A notable design-choice is the
              prediction of the sample, rather than the noise, in each diffusion
              step. This facilitates the use of established geometric losses on
              the locations and velocities of the motion, such as the foot
              contact loss. As we demonstrate, MDM is a generic approach,
              enabling different modes of conditioning, and different generation
              tasks. We show that our model is trained with lightweight
              resources and yet achieves state-of-the-art results on leading
              benchmarks for text-to-motion and action-to-motion.
              https://guytevet.github.io/mdm-page/ .",
  month    =  "29~" # sep,
  year     =  2022,
  url      = "http://arxiv.org/abs/2209.14916",
  file     = "All Papers/My Library/Tevet et al. 2022 - Human motion diffusion model.pdf"
}

@ARTICLE{Zhang2021-sb,
  title    = "{Onfocus detection: Identifying Individual-Camera eye contact from
              unconstrained images}",
  author   = "Zhang, Dingwen and Wang, Bo and Wang, Gerong and Zhang, Qiang and
              Zhang, Jiajia and Han, Jungong and You, Zheng",
  journal  = "arXiv [cs.CV]",
  abstract = "Onfocus detection aims at identifying whether the focus of the
              individual captured by a camera is on the camera or not. Based on
              the behavioral research, the focus of an individual during
              face-to-camera communication leads to a special type of eye
              contact, i.e., the individual-camera eye contact, which is a
              powerful signal in social communication and plays a crucial role
              in recognizing irregular individual status (e.g., lying or
              suffering mental disease) and special purposes (e.g., seeking help
              or attracting fans). Thus, developing effective onfocus detection
              algorithms is of significance for assisting the criminal
              investigation, disease discovery, and social behavior analysis.
              However, the review of the literature shows that very few efforts
              have been made toward the development of onfocus detector due to
              the lack of large-scale public available datasets as well as the
              challenging nature of this task. To this end, this paper engages
              in the onfocus detection research by addressing the above two
              issues. Firstly, we build a large-scale onfocus detection dataset,
              named as the OnFocus Detection In the Wild (OFDIW). It consists of
              20,623 images in unconstrained capture conditions (thus called “in
              the wild”) and contains individuals with diverse emotions, ages,
              facial characteristics, and rich interactions with surrounding
              objects and background scenes. On top of that, we propose a novel
              end-to-end deep model, i.e., the eye-context interaction inferring
              network (ECIIN), for onfocus detection, which explores eye-context
              interaction via dynamic capsule routing. Finally, comprehensive
              experiments are conducted on the proposed OFDIW dataset to
              benchmark the existing learning models and demonstrate the
              effectiveness of the proposed ECIIN. The project (containing both
              datasets and codes) is at https://github.com/wintercho/focus.",
  month    =  mar,
  year     =  2021,
  url      = "http://arxiv.org/abs/2103.15307",
  file     = "All Papers/My Library/Zhang et al. 2021 - Onfocus detection - Identifying Individual-Camera eye contact from unconstrained images.pdf"
}

@ARTICLE{Thanyadit2018-mp,
  title    = "{Efficient information sharing techniques between workers of
              heterogeneous tasks in 3d cve}",
  author   = "Thanyadit, S and Punpongsanon, P and Pong, T C",
  journal  = "Proceedings of the ACM on",
  abstract = "Collaboration between a helper and a worker in a 3D collaborative
              virtual environment usually requires real-time information
              sharing, since the worker relies on the timely assistance from the
              helper. In contrast, collaboration between workers requires them
              to shift their attention between independent tasks and dependent
              tasks. In worker-worker collaborations, a real-time updating
              technique could create excess information, which may be a
              distraction. In this paper, we compare different information
              sharing techniques and …",
  year     =  2018,
  url      = "https://dl.acm.org/doi/abs/10.1145/3274441",
  language = "en"
}

@ARTICLE{2007-re,
  title    = "{タイルドディスプレイを用いた多地点遠隔コミュニケーションシステムに関する研究}",
  author   = "誠, 澤藤 and 尚久, 坂本 and 康生, 江原 and {小山田耕二}",
  journal  = "情報処理学会研究報告コンピュータビジョンとイメージメディア（CVIM）",
  volume   =  2007,
  number   = "87(2007-CVIM-160)",
  pages    = "1--6",
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  sep,
  year     =  2007,
  keywords = "eye contact;telepresence"
}

@ARTICLE{Carbune2019-pw,
  title    = "{Fast multi-language LSTM-based online handwriting recognition}",
  author   = "Carbune, Victor and Gonnet, Pedro and Deselaers, Thomas and
              Rowley, Henry A and Daryin, Alexander and Calvo, Marcos and Wang,
              Li-Lun and Keysers, Daniel and Feuz, Sandro and Gervais, Philippe",
  abstract = "We describe an online handwriting system that is able to support
              102 languages using a deep neural network architecture. This new
              system has completely replaced our previous
              Segment-and-Decode-based system and reduced the error rate by
              20\%-40\% relative for most languages. Further, we report new
              state-of-the-art results on IAM-OnDB for both the open and closed
              dataset setting. The system combines methods from sequence
              recognition with a new input encoding using B\'ezier curves. This
              leads to up to 10x faster recognition times compared to our
              previous system. Through a series of experiments we determine the
              optimal configuration of our models and report the results of our
              setup on a number of additional public datasets.",
  month    =  "22~" # feb,
  year     =  2019,
  url      = "http://arxiv.org/abs/1902.10525",
  file     = "All Papers/My Library/Carbune et al. 2019 - Fast multi-language LSTM-based online handwriting recognition.pdf"
}

@ARTICLE{Cohen2017-ba,
  title     = "{``Twhirleds'': Spun and whirled affordances controlling
               multimodal mobile-ambient environments with reality distortion
               and synchronized lighting to preserve …}",
  author    = "Cohen, M and Ranaweera, R and Ryskeldiev, B and {others}",
  journal   = "Scientific Phone Apps",
  publisher = "Springer",
  abstract  = "The popularity of the contemporary smartphone makes it an
               attractive platform for new applications. We are exploring the
               potential of such personal devices to control networked …",
  year      =  2017,
  keywords  = "spatial computing"
}

@MISC{Inoue_undated-fa,
  title        = "{A study on image transformation of eye areas for synthesizing
                  eye-contacts in video conferencing}",
  author       = "Inoue, Takuya and Takahashi, Tomokazu and Hirayama, Takatsugu
                  and Deguchi, Daisuke and Ichiro, I D E and Murase, Hiroshi and
                  Kurozumi, Takayuki and Kashino, Kunio",
  howpublished = "https://www.murase.m.is.nagoya-u.ac.jp/
                  ide/res/paper/J14-kenkyukai-inouet-1pub.pdf",
  keywords     = "eye contact;telepresence"
}

@MISC{Inoue_undated-xx,
  title  = "{A study on image transformation of eye areas for synthesizing
            eye-contacts in video conferencing}",
  author = "Inoue, Takuya and Takahashi, Tomokazu and Hirayama, Takatsugu and
            Deguchi, Daisuke and Ichiro, I D E and Murase, Hiroshi and Kurozumi,
            Takayuki and Kashino, Kunio",
  url    = "https://www.murase.m.is.nagoya-u.ac.jp/
            ide/res/paper/J14-kenkyukai-inouet-1pub.pdf"
}

@ARTICLE{Wang2020-ie,
  title    = "{One-Shot Free-View neural Talking-Head synthesis for video
              conferencing}",
  author   = "Wang, Ting-Chun and Mallya, Arun and Liu, Ming-Yu",
  journal  = "arXiv [cs.CV]",
  abstract = "We propose a neural talking-head video synthesis model and
              demonstrate its application to video conferencing. Our model
              learns to synthesize a talking-head video using a source image
              containing the target person's appearance and a driving video that
              dictates the motion in the output. Our motion is encoded based on
              a novel keypoint representation, where the identity-specific and
              motion-related information is decomposed unsupervisedly. Extensive
              experimental validation shows that our model outperforms competing
              methods on benchmark datasets. Moreover, our compact keypoint
              representation enables a video conferencing system that achieves
              the same visual quality as the commercial H.264 standard while
              only using one-tenth of the bandwidth. Besides, we show our
              keypoint representation allows the user to rotate the head during
              synthesis, which is useful for simulating face-to-face video
              conferencing experiences.",
  month    =  nov,
  year     =  2020,
  url      = "http://arxiv.org/abs/2011.15126",
  file     = "All Papers/My Library/Wang et al. 2020 - One-Shot Free-View neural Talking-Head synthesis for video conferencing.pdf"
}

@ARTICLE{2011-el,
  title    = "{可動式カメラによる社会的テレプレゼンスの強化}",
  author   = "{加藤慶} and {村上友樹} and {中西英之} and {Others}",
  journal  = "情報処理学会論文誌",
  volume   =  52,
  number   =  4,
  pages    = "1635--1643",
  year     =  2011,
  keywords = "eye contact;telepresence"
}

@ARTICLE{Isbister1999-an,
  title    = "{ソーシャルインタラクション：サイバー空間での社会的インタラクションのための設計}",
  author   = "Isbister, Katherine and 亨, 石田",
  journal  = "情報処理",
  volume   =  40,
  number   =  6,
  abstract = "情報学広場 情報処理学会電子図書館",
  month    =  jun,
  year     =  1999,
  keywords = "社会的; ソーシャルインタラクション; 特集;eye contact;telepresence"
}

@INCOLLECTION{Bialowas2019-sy,
  title     = "{Eye-tracking in marketing research}",
  author    = "Białowąs, Sylwester Andrzej and Szyszka, Adrianna",
  booktitle = "{Managing economic innovations – methods and instruments}",
  publisher = "unknown",
  pages     = "91–104",
  abstract  = "PDF | On Jan 1, 2019, Sylwester Białowąs and others published
               Eye-tracking in Marketing Research | Find, read and cite all the
               research you need on ResearchGate",
  month     =  "1~" # jan,
  year      =  2019,
  url       = "https://www.researchgate.net/publication/338705061_Eye-tracking_in_Marketing_Research",
  file      = "All Papers/My Library/Białowąs and Szyszka 2019 - Eye-tracking in marketing research.pdf",
  isbn      =  9788379862771
}

@ARTICLE{Andersson1995-um,
  title    = "{Video conference system and method for providing parallax
              correction and sense of presence}",
  author   = "Andersson, R L and Chen, Tsuhan and Haskell, B and ツハン, チェン and
              バリン, ジェフリー ハスケル and ラッセル, レンナート アンダーソン",
  journal  = "undefined",
  abstract = "A television(TV) conference system capable of producing a sense of
              presence based on the actual direction of a head and an eye
              contact in remotely sited images is provided. PROBLEM TO BE
              SOLVED: To provide a television(TV) conference system capable of
              producing a sense of presence based on the actual direction of a
              head and an eye contact in remotely sited images. SOLUTION: In a
              TV conference system, video cameras 30, 32 for generating a video
              signal representing the image frames of respective conference
              participants A, B, and image receivers 22, 24 for displaying the
              image frames of remote conference perticipants A, B are arranged
              in respective conference rooms 28, 26. A parallax angle
              $\vartheta$ is defined by the image receiver, the video camera and
              participant's eyes in the room. The system includes also a frame
              generating system 34 for analyzing the image frame of each
              conference participant in each room and generating a
              parallax-compensated frame. A signal representing each
              parallax-compensated frame is transmitted to its corresponding
              image receiver. When three participants or more are present,
              respective input image frames for the positions of heads are
              analyzed to correct the directions of the heads.",
  year     =  1995,
  language = "en"
}

@ARTICLE{Andersson1995-oy,
  title    = "{Video conference system and method for providing parallax
              correction and sense of presence}",
  author   = "{Andersson} and {Chen} and {Haskell} and {ツハン} and {バリン} and
              {ラッセル}",
  journal  = "undefined",
  abstract = "A television(TV) conference system capable of producing a sense of
              presence based on the actual direction of a head and an eye
              contact in remotely sited images is provided. PROBLEM TO BE
              SOLVED: To provide a television(TV) conference system capable of
              producing a sense of presence based on the actual direction of a
              head and an eye contact in remotely sited images. SOLUTION: In a
              TV conference system, video cameras 30, 32 for generating a video
              signal representing the image frames of respective conference
              participants A, B, and image receivers 22, 24 for displaying the
              image frames of remote conference perticipants A, B are arranged
              in respective conference rooms 28, 26. A parallax angle ϑ is
              defined by the image receiver, the video camera and participant's
              eyes in the room. The system includes also a frame generating
              system 34 for analyzing the image frame of each conference
              participant in each room and generating a parallax-compensated
              frame. A signal representing each parallax-compensated frame is
              transmitted to its corresponding image receiver. When three
              participants or more are present, respective input image frames
              for the positions of heads are analyzed to correct the directions
              of the heads.",
  year     =  1995,
  language = "en"
}

@INCOLLECTION{Jacob2003-wk,
  title     = "{Eye Tracking in Human-Computer Interaction and Usability
               Research: Ready to Deliver the Promises}",
  author    = "Jacob, Robert J K and Karn, Keith S",
  editor    = "Hyönä, J and Radach, R and Deubel, H",
  booktitle = "{The Mind's Eye}",
  publisher = "North-Holland",
  address   = "Amsterdam",
  pages     = "573--605",
  abstract  = "This chapter discusses the application of eye movements to user
               interfaces, both for analyzing interfaces (measuring usability)
               and as an actual control medium within a human–computer dialogue.
               For usability analysis, the user's eye movements are recorded
               during system use and later analyzed retrospectively; however,
               the eye movements do not affect the interface in real time. As a
               direct control medium, the eye movements are obtained and used in
               real time as an input to the user–computer dialogue. The eye
               movements might be the sole input, typically for disabled users
               or hands-busy applications, or might be used as one of several
               inputs, combining with mouse, keyboard, sensors, or other
               devices. From the perspective of mainstream eye-movement
               research, human–computer interaction, together with related work
               in the broader field of communications and media research,
               appears as a new and very promising area of applied work. Both
               basic and applied work can profit from integration within a
               unified field of eye­-movement research. Application of eye
               tracking in human–computer interaction remains a very promising
               approach; its technological and market barriers are finally being
               reduced.",
  month     =  "1~" # jan,
  year      =  2003,
  url       = "https://www.sciencedirect.com/science/article/pii/B9780444510204500311",
  isbn      =  9780444510204,
  language  = "en"
}

@ARTICLE{Kumar_undated-qm,
  title    = "{EyeExposé: Switching Applications with Your Eyes}",
  author   = "Kumar, Manu and Paepcke, Andreas and Winograd, Terry",
  abstract = "We present a technique for switching between active applications
              by using a combination of keyboard (or any other trigger) and eye
              gaze. In particular, our approach combines the use of a
              two-dimensional layout visualization for showing the user all open
              applications and the use of eye gaze tracking for selecting the
              desired window. Our studies show that this combination of gaze and
              the visual representation of active tasks allows users to switch
              between applications quickly and naturally. Users strongly
              preferred this technique of switching between applications
              compared to other alternatives.",
  language = "en"
}

@MISC{noauthor_undated-md,
  title        = "{Dwell Selection with ML-based Intent Prediction Using Only
                  Gaze Data | Proceedings of the ACM on Interactive, Mobile,
                  Wearable and Ubiquitous Technologies}",
  howpublished = "\url{https://dl.acm.org/doi/10.1145/3550301}",
  note         = "Accessed: 2023-8-10"
}

@MISC{noauthor_undated-vu,
  title        = "{ChameleonMask | 第 33 回 ACM 年次会議の議事録 コンピューティング システムにおけるヒューマン
                  ファクターに関する拡張概要}",
  howpublished = "\url{https://dl.acm.org/doi/abs/10.1145/2702613.2732506}",
  note         = "Accessed: 2024-6-11"
}

@MISC{noauthor_undated-lb,
  title  = "{遠隔視覚対話における人間特性の分析とその応用 - 国立国会図書館デジタルコレクション}",
  url    = "https://dl.ndl.go.jp/info:ndljp/pid/3188017",
  annote = "Accessed: 2021-10-20"
}

@MISC{noauthor_undated-rz,
  title        = "{{JAIST} Repository: 多地点遠隔会議における自然なコミュニケーションの実現に関する研究}",
  howpublished = "\url{https://dspace.jaist.ac.jp/dspace/handle/10119/965}",
  note         = "Accessed: 2021-10-15",
  keywords     = "eye contact;telepresence"
}

@MISC{noauthor_undated-al,
  title    = "{JAIST Repository: 多地点遠隔会議における自然なコミュニケーションの実現に関する研究}",
  url      = "https://dspace.jaist.ac.jp/dspace/handle/10119/965",
  language = "ja",
  annote   = "Accessed: 2022-10-11"
}

@MISC{noauthor_undated-oo,
  title        = "{Visualization of eye gaze data using heat maps}",
  howpublished = "\url{https://etalpykla.vilniustech.lt/handle/123456789/68329}",
  note         = "Accessed: 2023-7-20"
}

@ARTICLE{noauthor_undated-xy,
  title = "{aoyama\_ipsj2017.pdf}",
  url   = "https://www.iplab.cs.tsukuba.ac.jp/paper/convention/aoyama_ipsj2017.pdf",
  file  = "All Papers/My Library/aoyama_ipsj2017.pdf - aoyama_ipsj2017.pdf"
}
